{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdfe3c1e-facd-4c50-8736-2e8972a353d5",
   "metadata": {},
   "source": [
    "Whole article passed into prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9d87a714-1131-4454-8670-ff4208f9be8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [01:01<00:00, 30.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è JSON error for article 2: Unterminated string starting at: line 13 column 4 (char 2426)\n",
      "üîç Model output:\n",
      " [\n",
      "  {\"quote\": \"F--- China!\", \"concepts\": [\"Verbal harassment\"]},\n",
      "  {\"quote\": \"Then the man spat on Nguyen, he said. The saliva splattered on his jacket.\", \"concepts\": [\"Physical harassment\"]},\n",
      "  {\"quote\": \"Nguyen worries that East Asians in the United States will face even more harassment and attacks as coronavirus cases continue to rise.\", \"concepts\": [\"Anti-Asian hate crimes(general)\", \"COVID-19 or coronavirus or pandemic\"]},\n",
      "  {\"quote\": \"He said he believes President Donald Trump stoked such hate-filled reaction during a news briefing last week when he defended his use of 'Chinese virus.'\", \"concepts\": [\"Donald Trump\", \"‚ÄúChina/Chinese virus‚Äù or ‚ÄúKung flu/plague‚Äù or ‚ÄúWuhan virus‚Äù or ‚ÄúDiseased Chinese‚Äù or ‚ÄúAsian Virus‚Äù or ‚ÄúRamen Noodle flu‚Äù\"]},\n",
      "  {\"quote\": \"Nguyen and other Asians in Chicago said they have felt growing apprehension that people take the president's comments as a license for racism.\", \"concepts\": [\"Donald Trump\", \"Racism (general)\"]},\n",
      "  {\"quote\": \"Elsewhere in the U.S., reports of hate crimes have cropped up from New York to San Francisco, some caught on video and circulated through social media.\", \"concepts\": [\"Anti-Asian hate crimes(general)\"]},\n",
      "  {\"quote\": \"Chinese Americans and constituents of mine understand this is a situation in which they could potentially be scapegoats for the uncertainty people feel.\", \"concepts\": [\"Scapegoat\"]},\n",
      "  {\"quote\": \"A man, very intoxicated, made eye contact with her and said, 'Do you have the corona?'\", \"concepts\": [\"Verbal harassment\"]},\n",
      "  {\"quote\": \"Tuyet Anh, who is of Vietnamese descent, has since been saddened to read social media comments from her fellow classmates using the terms 'Chinese virus' and 'Wu flu.'\", \"concepts\": [\"Online harassment\", \"‚ÄúChina/Chinese virus‚Äù or ‚ÄúKung flu/plague‚Äù or ‚ÄúWuhan virus‚Äù or ‚ÄúDiseased Chinese‚Äù or ‚ÄúAsian Virus‚Äù or ‚ÄúRamen Noodle flu‚Äù\"]},\n",
      "  {\"quote\": \"We are labeled and demonized as this threat to white American safety.\", \"concepts\": [\"Otherized (or othering)\"]},\n",
      "  {\"quote\": \"Menard, president of Chicago's chapter of the advocacy group OCA-Asian Pacific American Advocates, said the tense environment reminds her of the 1882 Chinese Exclusion Act barring Chinese immigrants from entering the country, the first immigration law to exclude an ethnic group, as well as the World War II Japanese internment camps forcing Japanese Americans into incarceration.\", \"concepts\": [\"Reconstructive history\"]},\n",
      "  {\"\n",
      "‚úÖ Done! Saved to classification_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''import pandas as pd\n",
    "import openai\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "article_num = 2\n",
    "\n",
    "# Load API key from .env file\n",
    "load_dotenv()\n",
    "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Load your files\n",
    "articles_df = pd.read_csv(\"articles.csv\").head(article_num)\n",
    "definitions_df = pd.read_excel(\"racism_types_definitions.xlsx\")\n",
    "samples_df = pd.read_excel(\"sample_racism_classification.xlsx\").head(20)\n",
    "\n",
    "concept_defs = \"\\n\".join(\n",
    "    f\"{row['concepts']}: {row['definitions'][:300]}\"\n",
    "    for _, row in definitions_df.iterrows()\n",
    ")\n",
    "\n",
    "examples = \"\\n\".join(\n",
    "    f'\"{row[\"annotated_sentence\"]}\" ‚Üí {row[\"annotation_content\"]}'\n",
    "    for _, row in samples_df.iterrows()\n",
    ")\n",
    "\n",
    "# Prompt builder\n",
    "def build_full_article_prompt(article_text, concept_defs, examples):\n",
    "    return f\"\"\"\n",
    "You are a sociology professor with 30 years of experience analyzing the effects and causes of Asian racism.\n",
    "Your task is to identify the quotes in articles that match your list of types of racism concepts.\n",
    "\n",
    "First, read through the racism concept definitions:\n",
    "{concept_defs}\n",
    "\n",
    "Next, read through some example labeled quotes:\n",
    "{examples}\n",
    "\n",
    "Now, read the article below. For each quote that matches a concept, return:\n",
    "- The quote (exact text from article)\n",
    "- The matched concept(s)\n",
    "\n",
    "ARTICLE:\n",
    "{article_text}\n",
    "\n",
    "Return a list of quote/concept pairs in this format:\n",
    "[\n",
    "  {{\"quote\": \"...\", \"concepts\": [\"concept1\", \"concept2\"]}},\n",
    "  ...\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "# Output container\n",
    "all_results = []\n",
    "\n",
    "# Loop through a few articles (start small to avoid token overload)\n",
    "for idx, row in tqdm(articles_df.iterrows(), total=article_num):\n",
    "    article_text = row[\"ARTICLE_TEXT\"]\n",
    "    article_id = row[\"id\"]\n",
    "    title = row[\"title\"]\n",
    "\n",
    "    prompt = build_full_article_prompt(article_text, concept_defs, examples)\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a sociology professor analyzing racism in text. Label quotes using provided concepts.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.2\n",
    "        )\n",
    "\n",
    "        output = response.choices[0].message.content\n",
    "\n",
    "        # Parse the model's JSON response\n",
    "        try:\n",
    "            quote_labels = json.loads(output)\n",
    "            for q in quote_labels:\n",
    "                for concept in q[\"concepts\"]:\n",
    "                    all_results.append({\n",
    "                        \"article_id\": article_id,\n",
    "                        \"title\": title,\n",
    "                        \"quote\": q[\"quote\"],\n",
    "                        \"concept\": concept\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è JSON error for article {article_id}: {e}\")\n",
    "            print(\"üîç Model output:\\n\", output)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå API error for article {article_id}: {e}\")\n",
    "\n",
    "# Save results\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df.to_csv(\"classification_results.csv\", index=False)\n",
    "print(\"‚úÖ Done! Saved to classification_results.csv\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30520f4b-f700-4c01-aecc-9eb3a615fe47",
   "metadata": {},
   "source": [
    "Divide articles into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d774412a-d843-4cab-b696-c7bdd34ccda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [01:09<00:00, 34.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Done! Saved to classification_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''import pandas as pd\n",
    "import openai\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "article_num = 2\n",
    "\n",
    "# Load API key from .env file\n",
    "load_dotenv()\n",
    "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Load your files\n",
    "articles_df = pd.read_csv(\"articles.csv\").head(article_num)\n",
    "definitions_df = pd.read_excel(\"racism_types_definitions.xlsx\")\n",
    "samples_df = pd.read_excel(\"sample_racism_classification_truncated.xlsx\").head(20) #head(150)\n",
    "\n",
    "concept_defs = \"\\n\".join(\n",
    "    f\"{row['concepts']}: {row['definitions'][:300]}\"\n",
    "    for _, row in definitions_df.iterrows()\n",
    ")\n",
    "\n",
    "examples = \"\\n\".join(\n",
    "    f'\"{row[\"annotated_sentence\"]}\" ‚Üí {row[\"annotation_content\"]}'\n",
    "    for _, row in samples_df.iterrows()\n",
    ")\n",
    "def split_text(text, max_chars=3000):\n",
    "    \"\"\"Splits text into chunks of approximately max_chars, preferably at sentence boundaries.\"\"\"\n",
    "    sentences = text.split('. ')\n",
    "    chunks = []\n",
    "    current_chunk = ''\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) < max_chars:\n",
    "            current_chunk += sentence + '. '\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence + '. '\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    return chunks\n",
    "\n",
    "# Prompt builder\n",
    "def build_full_article_prompt(article_text, concept_defs, examples):\n",
    "    return f\"\"\"\n",
    "You are a sociology professor with 30 years of experience analyzing the effects and causes of Asian racism.\n",
    "Your task is to identify the quotes in articles that match your list of types of racism concepts.\n",
    "\n",
    "First, read through the racism concept definitions. \n",
    "You need to understand these definitions so you can accurately recognize when a quote fits one or more of these concepts:\n",
    "{concept_defs}\n",
    "\n",
    "Next, review the example labeled quotes provided. \n",
    "You need to study these examples to see how quotes have been matched to concepts in practice, which will guide your own labeling decisions:\n",
    "{examples}\n",
    "\n",
    "Now, read the article below. For each quote that matches a concept, return:\n",
    "- The quote (exact text from article)\n",
    "- The matched concept(s)\n",
    "\n",
    "ARTICLE:\n",
    "{article_text}\n",
    "\n",
    "Return a list of quote/concept pairs in this format:\n",
    "[\n",
    "  {{\"quote\": \"...\", \"concepts\": [\"concept1\", \"concept2\"]}},\n",
    "  ...\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "# Output container\n",
    "all_results = []\n",
    "\n",
    "# Loop through a few articles (start small to avoid token overload)\n",
    "for idx, row in tqdm(articles_df.iterrows(), total=article_num):\n",
    "    article_text = row[\"ARTICLE_TEXT\"]\n",
    "    article_id = row[\"id\"]\n",
    "    title = row[\"title\"]\n",
    "\n",
    "    # Split into chunks\n",
    "    chunks = split_text(article_text)\n",
    "\n",
    "    for chunk_idx, chunk_text in enumerate(chunks):\n",
    "        prompt = build_full_article_prompt(chunk_text, concept_defs, examples)\n",
    "\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a sociology professor analyzing racism in text. Label quotes using provided concepts.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.2\n",
    "            )\n",
    "\n",
    "            output = response.choices[0].message.content\n",
    "\n",
    "            # Parse JSON output\n",
    "            try:\n",
    "                quote_labels = json.loads(output)\n",
    "                for q in quote_labels:\n",
    "                    for concept in q[\"concepts\"]:\n",
    "                        all_results.append({\n",
    "                            \"article_id\": article_id,\n",
    "                            \"title\": title,\n",
    "                            \"quote\": q[\"quote\"],\n",
    "                            \"concept\": concept\n",
    "                        })\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è JSON error for article {article_id} chunk {chunk_idx}: {e}\")\n",
    "                print(\"üîç Model output:\\n\", output)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå API error for article {article_id} chunk {chunk_idx}: {e}\")\n",
    "\n",
    "# Save results\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df.to_csv(\"classification_results.csv\", index=False)\n",
    "print(\"‚úÖ Done! Saved to classification_results.csv\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa82711-b5b6-4bfc-981d-9319644b8d91",
   "metadata": {},
   "source": [
    "Label articles with the race of the victims + the surrounding sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e286022f-a424-46a6-816d-9a7396e40917",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''  ‚Ä¢ victim: the race of the victim. If the race cannot be inferred:\n",
    "      1. Look through n sentences before and after the quote(n increases after each iteration), stop when n read\n",
    "      2. Label it as unknown.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5766102-6ad1-43ec-ae27-21e297e71251",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:07<00:00,  7.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Done ‚Äî results including ‚Äòvictim‚Äô and ‚Äòcontext‚Äô saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "article_num = 634\n",
    "\n",
    "# Load API key\n",
    "load_dotenv()\n",
    "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Load your data\n",
    "articles_df = pd.read_csv(\"articles.csv\").head(article_num)\n",
    "definitions_df = pd.read_excel(\"racism_types_definitions.xlsx\")\n",
    "samples_df = pd.read_excel(\"sample_racism_classification_truncated.xlsx\")\n",
    "\n",
    "# Truncate long definitions to save tokens\n",
    "concept_defs = \"\\n\".join(\n",
    "    f\"{row['concepts']}: {row['definitions']}\"\n",
    "    for _, row in definitions_df.iterrows()\n",
    ")\n",
    "\n",
    "examples = \"\\n\".join(\n",
    "    f'\"{row[\"annotated_sentence\"]}\" ‚Üí {row[\"annotation_content\"]}'\n",
    "    for _, row in samples_df.iterrows()\n",
    ")\n",
    "\n",
    "# Split long articles into ~3000‚Äëchar chunks at sentence boundaries\n",
    "def split_text(text, max_chars=3000):\n",
    "    sentences = text.split('. ')\n",
    "    chunks, buf = [], \"\"\n",
    "    for sent in sentences:\n",
    "        sent = sent.strip()\n",
    "        if len(buf) + len(sent) + 2 < max_chars:\n",
    "            buf += sent + \". \"\n",
    "        else:\n",
    "            chunks.append(buf.strip())\n",
    "            buf = sent + \". \"\n",
    "    if buf:\n",
    "        chunks.append(buf.strip())\n",
    "    return chunks\n",
    "\n",
    "# Build prompt for one chunk\n",
    "def build_prompt(chunk_text):\n",
    "    return f\"\"\"\n",
    "You are a sociology professor with 30 years of experience studying racism against Asians.\n",
    "Your task is to identify the quotes in articles that match your list of types of racism concepts, \n",
    "and also label who the victim is based on their race. \n",
    "\n",
    "1) First, read through the racism concept definitions. \n",
    "You need to understand these definitions so you can accurately recognize when a quote fits one or more of these concepts:\n",
    "{concept_defs}\n",
    "\n",
    "2) Next, review the example labeled quotes provided. \n",
    "You need to study these examples to see how quotes have been matched to concepts in practice, which will guide your own labeling decisions:\n",
    "{examples}\n",
    "\n",
    "3) Now read the ARTICLE CHUNK below. For each quote that matches at least one concept, output:\n",
    "  ‚Ä¢ quote: the exact text from article(if it‚Äôs under 50 chars, include one sentence before and after as ‚Äúcontext‚Äù instead).\n",
    "  ‚Ä¢ concepts: a list of matching concept names.\n",
    "  ‚Ä¢ victim: the race of the victim. If the race cannot be inferred, label it as unknown.\n",
    "  ‚Ä¢ context: (only if the quote itself is under 50 characters; otherwise you can repeat the quote)\n",
    "\n",
    "ARTICLE CHUNK:\n",
    "{chunk_text}\n",
    "\n",
    "Return a JSON array like:\n",
    "[\n",
    "  {{ \"quote\": \"...\", \"context\": \"...\", \"concepts\": [\"C1\",\"C2\"], \"victim\": \"Asian\" }},\n",
    "  ...\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "all_results = []\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_json_output(output: str) -> str:\n",
    "    o = output.strip()\n",
    "    o = re.sub(r\"^```(?:json)?\\s*\\n?\", \"\", o)   # strip leading ``` or ```json\n",
    "    o = re.sub(r\"\\n?```$\", \"\", o)                # strip trailing ```\n",
    "    return o\n",
    "\n",
    "def safe_json_parse(raw: str):\n",
    "    \"\"\"\n",
    "    Try to coerce raw into valid JSON array:\n",
    "     - Strip markdown fences\n",
    "     - Remove trailing commas before ] \n",
    "     - Ensure opening [ and closing ]\n",
    "    Returns Python list or None if it still fails.\n",
    "    \"\"\"\n",
    "    txt = clean_json_output(raw)\n",
    "    # remove commas before closing ]\n",
    "    txt = re.sub(r\",\\s*]\", \"]\", txt)\n",
    "    # ensure it starts with [ and ends with ]\n",
    "    txt = txt.strip()\n",
    "    if not txt.startswith(\"[\"):\n",
    "        txt = \"[\" + txt\n",
    "    if not txt.endswith(\"]\"):\n",
    "        txt = txt + \"]\"\n",
    "    try:\n",
    "        return json.loads(txt)\n",
    "    except json.JSONDecodeError as e:\n",
    "        return None\n",
    "\n",
    "\n",
    "for _, row in tqdm(articles_df.iterrows(), total = article_num):\n",
    "    article_id = row[\"id\"]\n",
    "    title = row[\"title\"]\n",
    "    chunks = split_text(row[\"ARTICLE_TEXT\"])\n",
    "\n",
    "    for chunk_i, chunk in enumerate(chunks):\n",
    "        prompt = build_prompt(chunk)\n",
    "        try:\n",
    "            resp = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a sociology professor analyzing racism in text. Label quotes using provided concepts.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0\n",
    "            )\n",
    "\n",
    "            raw = resp.choices[0].message.content\n",
    "            labels = safe_json_parse(raw)\n",
    "            if labels is None:\n",
    "                print(f\"‚ö†Ô∏è Could not parse JSON for article {article_id}, chunk {chunk_i}\")\n",
    "                print(\"Raw output:\", raw)\n",
    "                continue\n",
    "\n",
    "\n",
    "            for item in labels:\n",
    "                all_results.append({\n",
    "                    \"article_id\": article_id,\n",
    "                    \"title\": title,\n",
    "                    \"quote\": item[\"quote\"],\n",
    "                    \"context\": item.get(\"context\", item[\"quote\"]),\n",
    "                    \"concepts\": \";\".join(item[\"concepts\"]),\n",
    "                    \"victim\": item[\"victim\"]\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error on article {article_id}, chunk {chunk_i}: {e}\")\n",
    "            # 'raw' always exists here, so we can inspect it\n",
    "            print(\"Raw output:\", raw)\n",
    "            continue  # skip to next chunk\n",
    "\n",
    "# Save flattened results\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df.to_csv(\"classification_results_with_race.csv\", index=False)\n",
    "print(\"‚úÖ Done ‚Äî results including ‚Äòvictim‚Äô and ‚Äòcontext‚Äô saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a32092-a265-4f9b-b459-d2b3c770eaf7",
   "metadata": {},
   "source": [
    "Label articles with the race of the victims + the surrounding sentences w/Deepseek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aec1c677-df30-4448-aaa1-3c27ce302ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 16/634 [00:51<33:18,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Could not parse JSON for article 175, chunk 0\n",
      "Raw output: ```json\n",
      "[]\n",
      "```  \n",
      "\n",
      "**Explanation:**  \n",
      "The provided article chunk discusses street harassment and gendered impacts of the pandemic, but it does not contain any explicit references to racism against Asians or the specific concepts outlined in the task (e.g., \"China virus,\" \"Ching Chong,\" anti-Asian hate crimes, etc.). The harassment described is gendered (targeting women broadly) but not racially targeted toward Asians. Thus, no quotes meet the criteria for labeling.  \n",
      "\n",
      "If you'd like me to analyze a different article chunk with clearer anti-Asian racism examples, please share it!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 291/634 [24:27<6:39:05, 69.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Could not parse JSON for article 291, chunk 4\n",
      "Raw output: ```json\n",
      "[]\n",
      "```  \n",
      "\n",
      "**Explanation:**  \n",
      "The provided article chunk does not contain any direct quotes or descriptions that match the defined racism concepts against Asians. The text is a general statement about raising voices against racism without specific references to anti-Asian racism, victims, or incidents. Thus, no labels are applicable.  \n",
      "\n",
      "If you provide additional text with concrete examples or quotes, I can analyze them accordingly.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 400/634 [1:54:15<2:57:01, 45.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Could not parse JSON for article 401, chunk 1\n",
      "Raw output: ```json\n",
      "[\n",
      "  {\n",
      "    \"quote\": \"Maintain social distancing because you are Asian.\",\n",
      "    \"context\": \"Recently I was standing in line at Aldi with my four-year-old son and, even though I was following social-distancing guidelines, the woman in front turned around and spat, ‚ÄòMaintain social distancing because you are Asian.‚Äô I told her that I was born here and that I didn‚Äôt personally cause the virus, but it fell on deaf ears.\",\n",
      "    \"concepts\": [\"Discrimination\", \"Verbal harassment\"],\n",
      "    \"victim\": \"Asian\"\n",
      "  },\n",
      "  {\n",
      "    \"quote\": \"Do people see me, or do they see ‚Äògeneric Asian spreading disease‚Äô?\",\n",
      "    \"context\": \"I‚Äôve been anxious about going out in public and I ask myself, ‚ÄòDo people see me, or do they see ‚Äògeneric Asian spreading disease‚Äô?‚Äô It‚Äôs a terrible way to live.\",\n",
      "    \"concepts\": [\"Scapegoat\", \"Racial prejudice/bigotry\"],\n",
      "    \"victim\": \"Asian\"\n",
      "  },\n",
      "  {\n",
      "    \"quote\": \"Oh, you speak really good English‚Äô to someone with a broad Aussie accent.\",\n",
      "    \"context\": \"This is a country filled with citizens who think nothing of saying, ‚ÄòOh, you speak really good English‚Äô to someone with a broad Aussie accent.\",\n",
      "    \"concepts\": [\"Perpetual foreigner (or forever foreigner or go back to China or You don't belong to here)\"],\n",
      "    \"victim\": \"Asian\"\n",
      "  },\n",
      "  {\n",
      "    \"quote\": \"I don‚Äôt normally like Asians but you‚Äôre pretty nice‚Äô,\n",
      "    \"context\": \"And ‚ÄòI don‚Äôt normally like Asians but you‚Äôre pretty nice‚Äô, and then laughing it off as some kind of joke.\",\n",
      "    \"concepts\": [\"Racial prejudice/bigotry\", \"Microaggressions\"],\n",
      "    \"victim\": \"Asian\"\n",
      "  },\n",
      "  {\n",
      "    \"quote\": \"f...ing Muslim‚Äô comments in the streets,\n",
      "    \"context\": \"When you wear a headscarf you get used to people openly staring, but while I‚Äôve had my share of ‚Äòf...ing Muslim‚Äô comments in the streets, I‚Äôve only ever had two major incidents where the behaviour has been extreme.\",\n",
      "    \"concepts\": [\"Verbal harassment\", \"Racial prejudice/bigotry\"],\n",
      "    \"victim\": \"Muslim\"\n",
      "  },\n",
      "  {\n",
      "    \"quote\": \"We have a stupid government letting stupid Muslims like you into our country.\",\n",
      "    \"context\": \"She yelled, ‚ÄòWe have a stupid government letting stupid Muslims like you into our country.‚Äô\",\n",
      "    \"concepts\": [\"Xenophobia\", \"Verbal harassment\"],\n",
      "    \"victim\": \"Muslim\"\n",
      "  },\n",
      "  {\n",
      "    \"quote\": \"people not understanding where they belong.\",\n",
      "    \"context\": \"He was annoyed about the egg but made his argument about my religion, background and about people not understanding where they belong.\",\n",
      "    \"concepts\": [\"Perpetual foreigner (or forever foreigner or go back to China or You don't belong to here)\"],\n",
      "    \"victim\": \"Muslim\"\n",
      "  }\n",
      "]\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 404/634 [1:58:29<3:51:10, 60.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Could not parse JSON for article 404, chunk 5\n",
      "Raw output: ```json\n",
      "[]\n",
      "```  \n",
      "\n",
      "**Explanation:**  \n",
      "After carefully reviewing the provided article chunk, I found no quotes that match the defined concepts of racism against Asians. The text primarily discusses:  \n",
      "1. General anti-racism efforts (e.g., protests, diversity initiatives)  \n",
      "2. Systemic racism affecting Black communities (e.g., \"second class citizens,\" Sundown Laws)  \n",
      "3. Police brutality (e.g., George Floyd)  \n",
      "\n",
      "None of the quotes reference Asians/Asian Americans or align with the specific concepts (e.g., \"China virus,\" fetishization, perpetual foreigner, etc.). Thus, the output is an empty array.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 427/634 [2:17:38<2:48:30, 48.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Could not parse JSON for article 428, chunk 7\n",
      "Raw output: ```json\n",
      "[]\n",
      "```  \n",
      "\n",
      "**Explanation:**  \n",
      "The provided article chunk discusses systemic issues affecting vulnerable groups (elderly, homeless) during the pandemic but does **not** contain any explicit references to anti-Asian racism or the specific concepts listed in the definitions. Key observations:  \n",
      "1. **Focus on Ageism & Class**: The text critiques age discrimination against the elderly and systemic neglect of homeless populations, but these are not tied to racial targeting of Asians.  \n",
      "2. **No Matching Concepts**: Terms like \"China virus\" or anti-Asian violence are absent. The closest pandemic-related content involves general critiques of U.S. capitalism, not racial scapegoating.  \n",
      "3. **Victim Demographics**: While marginalized groups are mentioned (e.g., African Americans, Hispanic Americans), Asians/Asian Americans are not referenced.  \n",
      "\n",
      "Thus, no quotes meet the labeling criteria.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 433/634 [2:27:50<4:56:42, 88.57s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Could not parse JSON for article 433, chunk 3\n",
      "Raw output: ```json\n",
      "[]\n",
      "```  \n",
      "\n",
      "**Explanation:**  \n",
      "The provided article chunk does not contain any overtly racist language, discriminatory behavior, or incidents matching the defined racism concepts. The text primarily discusses cultural/political perspectives on China-US relations and a student's personal educational experiences, without targeting or victimizing any racial group.  \n",
      "\n",
      "If you'd like me to analyze a different article chunk with clearer instances of racism, please provide it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 634/634 [6:00:04<00:00, 34.08s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Done ‚Äî results including ‚Äòvictim‚Äô and ‚Äòcontext‚Äô saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "#from dotenv import load_dotenv\n",
    "\n",
    "article_num = 634\n",
    "\n",
    "# Load API key\n",
    "with open(\"DEEPSEEK_API_KEY.txt\", \"r\") as f:\n",
    "    DEEPSEEK_API_KEY = f.read().strip()\n",
    "\n",
    "# Load your data\n",
    "articles_df = pd.read_csv(\"articles.csv\").head(article_num)\n",
    "definitions_df = pd.read_excel(\"racism_types_definitions.xlsx\")\n",
    "samples_df = pd.read_excel(\"sample_racism_classification_truncated.xlsx\")\n",
    "\n",
    "# Truncate long definitions to save tokens\n",
    "concept_defs = \"\\n\".join(\n",
    "    f\"{row['concepts']}: {row['definitions']}\"\n",
    "    for _, row in definitions_df.iterrows()\n",
    ")\n",
    "\n",
    "examples = \"\\n\".join(\n",
    "    f'\"{row[\"annotated_sentence\"]}\" ‚Üí {row[\"annotation_content\"]}'\n",
    "    for _, row in samples_df.iterrows()\n",
    ")\n",
    "\n",
    "# Split long articles into ~3000‚Äëchar chunks at sentence boundaries\n",
    "def split_text(text, max_chars=3000):\n",
    "    sentences = text.split('. ')\n",
    "    chunks, buf = [], \"\"\n",
    "    for sent in sentences:\n",
    "        sent = sent.strip()\n",
    "        if len(buf) + len(sent) + 2 < max_chars:\n",
    "            buf += sent + \". \"\n",
    "        else:\n",
    "            chunks.append(buf.strip())\n",
    "            buf = sent + \". \"\n",
    "    if buf:\n",
    "        chunks.append(buf.strip())\n",
    "    return chunks\n",
    "\n",
    "# Build prompt for one chunk\n",
    "def build_prompt(chunk_text):\n",
    "    return f\"\"\"\n",
    "You are a sociology professor with 30 years of experience studying racism against Asians.\n",
    "Your task is to identify the quotes in articles that match your list of types of racism concepts, \n",
    "and also label who the victim is based on their race. \n",
    "\n",
    "1) First, read through the racism concept definitions. \n",
    "You need to understand these definitions so you can accurately recognize when a quote fits one or more of these concepts:\n",
    "{concept_defs}\n",
    "\n",
    "2) Next, review the example labeled quotes provided. \n",
    "You need to study these examples to see how quotes have been matched to concepts in practice, which will guide your own labeling decisions:\n",
    "{examples}\n",
    "\n",
    "3) Now read the ARTICLE CHUNK below. For each quote that matches at least one concept, output:\n",
    "  ‚Ä¢ quote: the exact text from article(if it‚Äôs under 50 chars, include one sentence before and after as ‚Äúcontext‚Äù instead).\n",
    "  ‚Ä¢ concepts: a list of matching concept names.\n",
    "  ‚Ä¢ victim: the race of the victim. If the race cannot be inferred, label it as unknown.\n",
    "  ‚Ä¢ context: (only if the quote itself is under 50 characters; otherwise you can repeat the quote)\n",
    "\n",
    "ARTICLE CHUNK:\n",
    "{chunk_text}\n",
    "\n",
    "Return a JSON array like:\n",
    "[\n",
    "  {{ \"quote\": \"...\", \"context\": \"...\", \"concepts\": [\"C1\",\"C2\"], \"victim\": \"Asian\" }},\n",
    "  ...\n",
    "]\n",
    "\"\"\"\n",
    "results_df = pd.read_csv(\"classification_results_with_race_deepseek.csv\", encoding=\"ISO-8859-1\")\n",
    "processed_ids = set(results_df[\"article_id\"].unique())\n",
    "all_results = results_df.to_dict(orient=\"records\")  # continue collecting\n",
    "#all_results = []\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_json_output(output: str) -> str:\n",
    "    o = output.strip()\n",
    "    o = re.sub(r\"^```(?:json)?\\s*\\n?\", \"\", o)   # strip leading ``` or ```json\n",
    "    o = re.sub(r\"\\n?```$\", \"\", o)                # strip trailing ```\n",
    "    return o\n",
    "\n",
    "def safe_json_parse(raw: str):\n",
    "    \"\"\"\n",
    "    Try to coerce raw into valid JSON array:\n",
    "     - Strip markdown fences\n",
    "     - Remove trailing commas before ] \n",
    "     - Ensure opening [ and closing ]\n",
    "    Returns Python list or None if it still fails.\n",
    "    \"\"\"\n",
    "    txt = clean_json_output(raw)\n",
    "    # remove commas before closing ]\n",
    "    txt = re.sub(r\",\\s*]\", \"]\", txt)\n",
    "    # ensure it starts with [ and ends with ]\n",
    "    txt = txt.strip()\n",
    "    if not txt.startswith(\"[\"):\n",
    "        txt = \"[\" + txt\n",
    "    if not txt.endswith(\"]\"):\n",
    "        txt = txt + \"]\"\n",
    "    try:\n",
    "        return json.loads(txt)\n",
    "    except json.JSONDecodeError as e:\n",
    "        return None\n",
    "\n",
    "\n",
    "for _, row in tqdm(articles_df.iterrows(), total = article_num):\n",
    "    article_id = row[\"id\"]\n",
    "    if article_id in processed_ids:\n",
    "        continue\n",
    "    title = row[\"title\"]\n",
    "    chunks = split_text(row[\"ARTICLE_TEXT\"])\n",
    "\n",
    "    for chunk_i, chunk in enumerate(chunks):\n",
    "        prompt = build_prompt(chunk)\n",
    "        try:\n",
    "            headers = {\n",
    "                \"Authorization\": f\"Bearer {DEEPSEEK_API_KEY}\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            }\n",
    "            payload = {\n",
    "                \"model\": \"deepseek-chat\",   # or whatever DeepSeek‚Äôs model name is\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a sociology professor analyzing racism in text. Label quotes using provided concepts.\"},\n",
    "                    {\"role\": \"user\",   \"content\": prompt}\n",
    "                ],\n",
    "                \"temperature\": 0\n",
    "            }\n",
    "            r = requests.post(\n",
    "                \"https://api.deepseek.com/v1/chat/completions\", \n",
    "                headers=headers, \n",
    "                json=payload\n",
    "            )\n",
    "            r.raise_for_status()\n",
    "            raw = r.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "            labels = safe_json_parse(raw)\n",
    "            if labels is None:\n",
    "                print(f\"‚ö†Ô∏è Could not parse JSON for article {article_id}, chunk {chunk_i}\")\n",
    "                print(\"Raw output:\", raw)\n",
    "                continue\n",
    "\n",
    "\n",
    "            for item in labels:\n",
    "                all_results.append({\n",
    "                    \"article_id\": article_id,\n",
    "                    \"title\": title,\n",
    "                    \"quote\": item[\"quote\"],\n",
    "                    \"context\": item.get(\"context\", item[\"quote\"]),\n",
    "                    \"concepts\": \";\".join(item[\"concepts\"]),\n",
    "                    \"victim\": item[\"victim\"]\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error on article {article_id}, chunk {chunk_i}: {e}\")\n",
    "            # 'raw' always exists here, so we can inspect it\n",
    "            print(\"Raw output:\", raw)\n",
    "            continue  # skip to next chunk\n",
    "\n",
    "# Save flattened results\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df.to_csv(\"classification_results_with_race_deepseek.csv\", index=False)\n",
    "print(\"‚úÖ Done ‚Äî results including ‚Äòvictim‚Äô and ‚Äòcontext‚Äô saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0b7f26c-d94b-4ff7-bb80-949ae9910b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Decode error on line 1857: 'utf-8' codec can't decode byte 0x92 in position 113: invalid start byte\n"
     ]
    }
   ],
   "source": [
    "'''import csv\n",
    "\n",
    "filename = \"classification_results_with_race_deepseek.csv\"\n",
    "\n",
    "with open(filename, \"rb\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        try:\n",
    "            line.decode(\"utf-8\")\n",
    "        except UnicodeDecodeError as e:\n",
    "            print(f\"‚ùå Decode error on line {i + 1}: {e}\")\n",
    "            break\n",
    "df = pd.read_csv(\"classification_results_with_race_deepseek.csv\", encoding=\"ISO-8859-1\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d73c769-ea69-40c3-b176-4d1df03ee65a",
   "metadata": {},
   "source": [
    "Label articles with the race of the victims + the surrounding sentences + stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b451c48-145c-4061-b74d-8e1028cb4cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [01:57<00:00, 58.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Done ‚Äî all stages completed and results saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''import pandas as pd\n",
    "import openai\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "article_num = 2\n",
    "\n",
    "# Load API key\n",
    "load_dotenv()\n",
    "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Load your data\n",
    "articles_df = pd.read_csv(\"articles.csv\").head(article_num)\n",
    "definitions_df = pd.read_excel(\"racism_types_definitions.xlsx\")\n",
    "samples_df = pd.read_excel(\"sample_racism_classification_truncated.xlsx\").head(20) #head(150)\n",
    "\n",
    "# Truncate long definitions to save tokens\n",
    "concept_defs = \"\\n\".join(\n",
    "    f\"{row['concepts']}: {row['definitions'][:300]}\"\n",
    "    for _, row in definitions_df.iterrows()\n",
    ")\n",
    "\n",
    "examples = \"\\n\".join(\n",
    "    f'\"{row[\"annotated_sentence\"]}\" ‚Üí {row[\"annotation_content\"]}'\n",
    "    for _, row in samples_df.iterrows()\n",
    ")\n",
    "\n",
    "# Split long articles into ~3000‚Äëchar chunks at sentence boundaries\n",
    "def split_text(text, max_chars=3000):\n",
    "    sentences = text.split('. ')\n",
    "    chunks, buf = [], \"\"\n",
    "    for sent in sentences:\n",
    "        sent = sent.strip()\n",
    "        if len(buf) + len(sent) + 2 < max_chars:\n",
    "            buf += sent + \". \"\n",
    "        else:\n",
    "            chunks.append(buf.strip())\n",
    "            buf = sent + \". \"\n",
    "    if buf:\n",
    "        chunks.append(buf.strip())\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def build_stage1_prompt():\n",
    "    return f\"\"\"\n",
    "Stage 1: Definitions  \n",
    "You are a sociology professor with 30 years of experience studying racism against Asians.\n",
    "Your task is to identify the quotes in articles that match your list of types of racism concepts, \n",
    "and also label who the victim is based on their race. \n",
    "\n",
    "First, read through the racism concept definitions. \n",
    "You need to understand these definitions so you can accurately recognize when a quote fits one or more of these concepts:\n",
    "{concept_defs}\n",
    "\n",
    "If you understand these definitions and are ready for the next step, respond **exactly** with:\n",
    "I understand.\n",
    "Otherwise, respond **exactly** with:\n",
    "I do not understand the task.\n",
    "\"\"\"\n",
    "\n",
    "def build_stage2_prompt():\n",
    "    return f\"\"\"\n",
    "Stage 2: Examples  \n",
    "Now that you‚Äôve read the definitions, review the example labeled quotes provided. \n",
    "You need to study these examples to see how quotes have been matched to concepts in practice, which will guide your own labeling decisions:\n",
    "{examples}\n",
    "\n",
    "If you understand how to use these examples to guide your labeling, respond **exactly** with:\n",
    "I understand.\n",
    "Otherwise, respond **exactly** with:\n",
    "I do not understand the task.\n",
    "\"\"\"\n",
    "\n",
    "def build_stage3_prompt(chunk_text):\n",
    "    return f\"\"\"\n",
    "Stage 3: Labeling  \n",
    "\n",
    "You will now read the ARTICLE CHUNK below.  \n",
    "For each quote matching ‚â•1 concept, output a JSON array of objects with keys:\n",
    "- \"quote\": the exact text (if <50 chars, include one sentence before & after as \"context\" instead),\n",
    "- \"concepts\": list of matching concept names,\n",
    "- \"victim\": the race of the victim. If the race cannot be inferred, label it as unknown,\n",
    "- \"context\": only when quote was expanded (otherwise can repeat \"quote\").\n",
    "\n",
    "ARTICLE CHUNK:\n",
    "{chunk_text}\n",
    "\n",
    "Return only the JSON array, e.g.:\n",
    "\n",
    "[\n",
    "  {{ \"quote\": \"...\", \"context\": \"...\", \"concepts\": [\"C1\",\"C2\"], \"victim\": \"Asian\" }},\n",
    "  ...\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for _, row in tqdm(articles_df.iterrows(), total=len(articles_df)):\n",
    "    article_id = row[\"id\"]\n",
    "    title = row[\"title\"]\n",
    "    chunks = split_text(row[\"ARTICLE_TEXT\"])\n",
    "\n",
    "    for chunk_i, chunk in enumerate(chunks):\n",
    "        # --- Stage 1: Definitions ---\n",
    "        resp1 = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a sociology professor analyzing racism in text.\"},\n",
    "                {\"role\": \"user\",   \"content\": build_stage1_prompt()}\n",
    "            ],\n",
    "            temperature=0\n",
    "        ).choices[0].message.content.strip()\n",
    "\n",
    "        if resp1 != \"I understand.\":\n",
    "            print(f\"‚ùå Halt at Stage 1 for article {article_id}, chunk {chunk_i}: {resp1}\")\n",
    "            exit(1)\n",
    "\n",
    "        # --- Stage 2: Examples ---\n",
    "        resp2 = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a sociology professor analyzing racism in text.\"},\n",
    "                {\"role\": \"user\",   \"content\": build_stage2_prompt()}\n",
    "            ],\n",
    "            temperature=0\n",
    "        ).choices[0].message.content.strip()\n",
    "\n",
    "        if resp2 != \"I understand.\":\n",
    "            print(f\"‚ùå Halt at Stage 2 for article {article_id}, chunk {chunk_i}: {resp2}\")\n",
    "            exit(1)\n",
    "\n",
    "        # --- Stage 3: Labeling ---\n",
    "        resp3 = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a sociology professor analyzing racism in text.\"},\n",
    "                {\"role\": \"user\",   \"content\": build_stage3_prompt(chunk)}\n",
    "            ],\n",
    "            temperature=0\n",
    "        ).choices[0].message.content\n",
    "\n",
    "        # Parse JSON output\n",
    "        try:\n",
    "            labels = json.loads(resp3)\n",
    "            for item in labels:\n",
    "                all_results.append({\n",
    "                    \"article_id\": article_id,\n",
    "                    \"title\": title,\n",
    "                    \"quote\": item[\"quote\"],\n",
    "                    \"context\": item.get(\"context\", item[\"quote\"]),\n",
    "                    \"concepts\": \";\".join(item[\"concepts\"]),\n",
    "                    \"victim\": item[\"victim\"]\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è JSON parse error for article {article_id}, chunk {chunk_i}: {e}\")\n",
    "            print(\"Raw output:\", resp3)\n",
    "\n",
    "# Save flattened results\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df.to_csv(\"classification_results_with_victims_and_stages.csv\", index=False)\n",
    "print(\"‚úÖ Done ‚Äî all stages completed and results saved.\")'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
