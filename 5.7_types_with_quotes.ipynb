{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fa82711-b5b6-4bfc-981d-9319644b8d91",
   "metadata": {},
   "source": [
    "Label articles with the race of the victims + the surrounding sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5766102-6ad1-43ec-ae27-21e297e71251",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done — results including ‘victim’ and ‘context’ saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "article_num = 634\n",
    "\n",
    "# Load API key\n",
    "load_dotenv()\n",
    "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Load your data\n",
    "articles_df = pd.read_csv(\"articles.csv\").head(article_num)\n",
    "definitions_df = pd.read_excel(\"racism_types_definitions.xlsx\")\n",
    "samples_df = pd.read_excel(\"sample_racism_classification_truncated.xlsx\")\n",
    "\n",
    "# Truncate long definitions to save tokens\n",
    "concept_defs = \"\\n\".join(\n",
    "    f\"{row['concepts']}: {row['definitions']}\"\n",
    "    for _, row in definitions_df.iterrows()\n",
    ")\n",
    "\n",
    "examples = \"\\n\".join(\n",
    "    f'\"{row[\"annotated_sentence\"]}\" → {row[\"annotation_content\"]}'\n",
    "    for _, row in samples_df.iterrows()\n",
    ")\n",
    "\n",
    "# Split long articles into ~3000‑char chunks at sentence boundaries\n",
    "def split_text(text, max_chars=3000):\n",
    "    sentences = text.split('. ')\n",
    "    chunks, buf = [], \"\"\n",
    "    for sent in sentences:\n",
    "        sent = sent.strip()\n",
    "        if len(buf) + len(sent) + 2 < max_chars:\n",
    "            buf += sent + \". \"\n",
    "        else:\n",
    "            chunks.append(buf.strip())\n",
    "            buf = sent + \". \"\n",
    "    if buf:\n",
    "        chunks.append(buf.strip())\n",
    "    return chunks\n",
    "\n",
    "# Build prompt for one chunk\n",
    "def build_prompt(chunk_text):\n",
    "    return f\"\"\"\n",
    "You are a sociology professor with 30 years of experience studying racism against Asians.\n",
    "Your task is to identify the quotes in articles that match your list of types of racism concepts, \n",
    "and also label who the victim is based on their race. \n",
    "\n",
    "1) First, read through the racism concept definitions. \n",
    "You need to understand these definitions so you can accurately recognize when a quote fits one or more of these concepts:\n",
    "{concept_defs}\n",
    "\n",
    "2) Next, review the example labeled quotes provided. \n",
    "You need to study these examples to see how quotes have been matched to concepts in practice, which will guide your own labeling decisions:\n",
    "{examples}\n",
    "\n",
    "3) Now read the ARTICLE CHUNK below. For each quote that matches at least one concept, output:\n",
    "  • quote: the exact text from article(if it’s under 50 chars, include one sentence before and after as “context” instead).\n",
    "  • concepts: a list of matching concept names.\n",
    "  • victim: the race of the victim. If the race cannot be inferred, label it as unknown.\n",
    "  • context: (only if the quote itself is under 50 characters; otherwise you can repeat the quote)\n",
    "\n",
    "ARTICLE CHUNK:\n",
    "{chunk_text}\n",
    "\n",
    "Return a JSON array like:\n",
    "[\n",
    "  {{ \"quote\": \"...\", \"context\": \"...\", \"concepts\": [\"C1\",\"C2\"], \"victim\": \"Asian\" }},\n",
    "  ...\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "all_results = []\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_json_output(output: str) -> str:\n",
    "    o = output.strip()\n",
    "    o = re.sub(r\"^```(?:json)?\\s*\\n?\", \"\", o)   # strip leading ``` or ```json\n",
    "    o = re.sub(r\"\\n?```$\", \"\", o)                # strip trailing ```\n",
    "    return o\n",
    "\n",
    "def safe_json_parse(raw: str):\n",
    "    \"\"\"\n",
    "    Try to coerce raw into valid JSON array:\n",
    "     - Strip markdown fences\n",
    "     - Remove trailing commas before ] \n",
    "     - Ensure opening [ and closing ]\n",
    "    Returns Python list or None if it still fails.\n",
    "    \"\"\"\n",
    "    txt = clean_json_output(raw)\n",
    "    # remove commas before closing ]\n",
    "    txt = re.sub(r\",\\s*]\", \"]\", txt)\n",
    "    # ensure it starts with [ and ends with ]\n",
    "    txt = txt.strip()\n",
    "    if not txt.startswith(\"[\"):\n",
    "        txt = \"[\" + txt\n",
    "    if not txt.endswith(\"]\"):\n",
    "        txt = txt + \"]\"\n",
    "    try:\n",
    "        return json.loads(txt)\n",
    "    except json.JSONDecodeError as e:\n",
    "        return None\n",
    "\n",
    "\n",
    "for _, row in tqdm(articles_df.iterrows(), total = article_num):\n",
    "    article_id = row[\"id\"]\n",
    "    title = row[\"title\"]\n",
    "    chunks = split_text(row[\"ARTICLE_TEXT\"])\n",
    "\n",
    "    for chunk_i, chunk in enumerate(chunks):\n",
    "        prompt = build_prompt(chunk)\n",
    "        try:\n",
    "            resp = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a sociology professor analyzing racism in text. Label quotes using provided concepts.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0\n",
    "            )\n",
    "\n",
    "            raw = resp.choices[0].message.content\n",
    "            labels = safe_json_parse(raw)\n",
    "            if labels is None:\n",
    "                print(f\"⚠️ Could not parse JSON for article {article_id}, chunk {chunk_i}\")\n",
    "                print(\"Raw output:\", raw)\n",
    "                continue\n",
    "\n",
    "\n",
    "            for item in labels:\n",
    "                all_results.append({\n",
    "                    \"article_id\": article_id,\n",
    "                    \"title\": title,\n",
    "                    \"quote\": item[\"quote\"],\n",
    "                    \"context\": item.get(\"context\", item[\"quote\"]),\n",
    "                    \"concepts\": \";\".join(item[\"concepts\"]),\n",
    "                    \"victim\": item[\"victim\"]\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error on article {article_id}, chunk {chunk_i}: {e}\")\n",
    "            # 'raw' always exists here, so we can inspect it\n",
    "            print(\"Raw output:\", raw)\n",
    "            continue  # skip to next chunk\n",
    "\n",
    "# Save flattened results\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df.to_csv(\"classification_results_with_race.csv\", index=False)\n",
    "print(\"✅ Done — results including ‘victim’ and ‘context’ saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a32092-a265-4f9b-b459-d2b3c770eaf7",
   "metadata": {},
   "source": [
    "Label articles with the race of the victims + the surrounding sentences w/Deepseek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aec1c677-df30-4448-aaa1-3c27ce302ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 16/634 [00:51<33:18,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Could not parse JSON for article 175, chunk 0\n",
      "Raw output: ```json\n",
      "[]\n",
      "```  \n",
      "\n",
      "**Explanation:**  \n",
      "The provided article chunk discusses street harassment and gendered impacts of the pandemic, but it does not contain any explicit references to racism against Asians or the specific concepts outlined in the task (e.g., \"China virus,\" \"Ching Chong,\" anti-Asian hate crimes, etc.). The harassment described is gendered (targeting women broadly) but not racially targeted toward Asians. Thus, no quotes meet the criteria for labeling.  \n",
      "\n",
      "If you'd like me to analyze a different article chunk with clearer anti-Asian racism examples, please share it!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 291/634 [24:27<6:39:05, 69.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Could not parse JSON for article 291, chunk 4\n",
      "Raw output: ```json\n",
      "[]\n",
      "```  \n",
      "\n",
      "**Explanation:**  \n",
      "The provided article chunk does not contain any direct quotes or descriptions that match the defined racism concepts against Asians. The text is a general statement about raising voices against racism without specific references to anti-Asian racism, victims, or incidents. Thus, no labels are applicable.  \n",
      "\n",
      "If you provide additional text with concrete examples or quotes, I can analyze them accordingly.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 400/634 [1:54:15<2:57:01, 45.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Could not parse JSON for article 401, chunk 1\n",
      "Raw output: ```json\n",
      "[\n",
      "  {\n",
      "    \"quote\": \"Maintain social distancing because you are Asian.\",\n",
      "    \"context\": \"Recently I was standing in line at Aldi with my four-year-old son and, even though I was following social-distancing guidelines, the woman in front turned around and spat, ‘Maintain social distancing because you are Asian.’ I told her that I was born here and that I didn’t personally cause the virus, but it fell on deaf ears.\",\n",
      "    \"concepts\": [\"Discrimination\", \"Verbal harassment\"],\n",
      "    \"victim\": \"Asian\"\n",
      "  },\n",
      "  {\n",
      "    \"quote\": \"Do people see me, or do they see ‘generic Asian spreading disease’?\",\n",
      "    \"context\": \"I’ve been anxious about going out in public and I ask myself, ‘Do people see me, or do they see ‘generic Asian spreading disease’?’ It’s a terrible way to live.\",\n",
      "    \"concepts\": [\"Scapegoat\", \"Racial prejudice/bigotry\"],\n",
      "    \"victim\": \"Asian\"\n",
      "  },\n",
      "  {\n",
      "    \"quote\": \"Oh, you speak really good English’ to someone with a broad Aussie accent.\",\n",
      "    \"context\": \"This is a country filled with citizens who think nothing of saying, ‘Oh, you speak really good English’ to someone with a broad Aussie accent.\",\n",
      "    \"concepts\": [\"Perpetual foreigner (or forever foreigner or go back to China or You don't belong to here)\"],\n",
      "    \"victim\": \"Asian\"\n",
      "  },\n",
      "  {\n",
      "    \"quote\": \"I don’t normally like Asians but you’re pretty nice’,\n",
      "    \"context\": \"And ‘I don’t normally like Asians but you’re pretty nice’, and then laughing it off as some kind of joke.\",\n",
      "    \"concepts\": [\"Racial prejudice/bigotry\", \"Microaggressions\"],\n",
      "    \"victim\": \"Asian\"\n",
      "  },\n",
      "  {\n",
      "    \"quote\": \"f...ing Muslim’ comments in the streets,\n",
      "    \"context\": \"When you wear a headscarf you get used to people openly staring, but while I’ve had my share of ‘f...ing Muslim’ comments in the streets, I’ve only ever had two major incidents where the behaviour has been extreme.\",\n",
      "    \"concepts\": [\"Verbal harassment\", \"Racial prejudice/bigotry\"],\n",
      "    \"victim\": \"Muslim\"\n",
      "  },\n",
      "  {\n",
      "    \"quote\": \"We have a stupid government letting stupid Muslims like you into our country.\",\n",
      "    \"context\": \"She yelled, ‘We have a stupid government letting stupid Muslims like you into our country.’\",\n",
      "    \"concepts\": [\"Xenophobia\", \"Verbal harassment\"],\n",
      "    \"victim\": \"Muslim\"\n",
      "  },\n",
      "  {\n",
      "    \"quote\": \"people not understanding where they belong.\",\n",
      "    \"context\": \"He was annoyed about the egg but made his argument about my religion, background and about people not understanding where they belong.\",\n",
      "    \"concepts\": [\"Perpetual foreigner (or forever foreigner or go back to China or You don't belong to here)\"],\n",
      "    \"victim\": \"Muslim\"\n",
      "  }\n",
      "]\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 404/634 [1:58:29<3:51:10, 60.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Could not parse JSON for article 404, chunk 5\n",
      "Raw output: ```json\n",
      "[]\n",
      "```  \n",
      "\n",
      "**Explanation:**  \n",
      "After carefully reviewing the provided article chunk, I found no quotes that match the defined concepts of racism against Asians. The text primarily discusses:  \n",
      "1. General anti-racism efforts (e.g., protests, diversity initiatives)  \n",
      "2. Systemic racism affecting Black communities (e.g., \"second class citizens,\" Sundown Laws)  \n",
      "3. Police brutality (e.g., George Floyd)  \n",
      "\n",
      "None of the quotes reference Asians/Asian Americans or align with the specific concepts (e.g., \"China virus,\" fetishization, perpetual foreigner, etc.). Thus, the output is an empty array.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 427/634 [2:17:38<2:48:30, 48.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Could not parse JSON for article 428, chunk 7\n",
      "Raw output: ```json\n",
      "[]\n",
      "```  \n",
      "\n",
      "**Explanation:**  \n",
      "The provided article chunk discusses systemic issues affecting vulnerable groups (elderly, homeless) during the pandemic but does **not** contain any explicit references to anti-Asian racism or the specific concepts listed in the definitions. Key observations:  \n",
      "1. **Focus on Ageism & Class**: The text critiques age discrimination against the elderly and systemic neglect of homeless populations, but these are not tied to racial targeting of Asians.  \n",
      "2. **No Matching Concepts**: Terms like \"China virus\" or anti-Asian violence are absent. The closest pandemic-related content involves general critiques of U.S. capitalism, not racial scapegoating.  \n",
      "3. **Victim Demographics**: While marginalized groups are mentioned (e.g., African Americans, Hispanic Americans), Asians/Asian Americans are not referenced.  \n",
      "\n",
      "Thus, no quotes meet the labeling criteria.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 433/634 [2:27:50<4:56:42, 88.57s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Could not parse JSON for article 433, chunk 3\n",
      "Raw output: ```json\n",
      "[]\n",
      "```  \n",
      "\n",
      "**Explanation:**  \n",
      "The provided article chunk does not contain any overtly racist language, discriminatory behavior, or incidents matching the defined racism concepts. The text primarily discusses cultural/political perspectives on China-US relations and a student's personal educational experiences, without targeting or victimizing any racial group.  \n",
      "\n",
      "If you'd like me to analyze a different article chunk with clearer instances of racism, please provide it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 634/634 [6:00:04<00:00, 34.08s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done — results including ‘victim’ and ‘context’ saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "#from dotenv import load_dotenv\n",
    "\n",
    "article_num = 634\n",
    "\n",
    "# Load API key\n",
    "with open(\"DEEPSEEK_API_KEY.txt\", \"r\") as f:\n",
    "    DEEPSEEK_API_KEY = f.read().strip()\n",
    "\n",
    "# Load your data\n",
    "articles_df = pd.read_csv(\"articles.csv\").head(article_num)\n",
    "definitions_df = pd.read_excel(\"racism_types_definitions.xlsx\")\n",
    "samples_df = pd.read_excel(\"sample_racism_classification_truncated.xlsx\")\n",
    "\n",
    "# Truncate long definitions to save tokens\n",
    "concept_defs = \"\\n\".join(\n",
    "    f\"{row['concepts']}: {row['definitions']}\"\n",
    "    for _, row in definitions_df.iterrows()\n",
    ")\n",
    "\n",
    "examples = \"\\n\".join(\n",
    "    f'\"{row[\"annotated_sentence\"]}\" → {row[\"annotation_content\"]}'\n",
    "    for _, row in samples_df.iterrows()\n",
    ")\n",
    "\n",
    "# Split long articles into ~3000‑char chunks at sentence boundaries\n",
    "def split_text(text, max_chars=3000):\n",
    "    sentences = text.split('. ')\n",
    "    chunks, buf = [], \"\"\n",
    "    for sent in sentences:\n",
    "        sent = sent.strip()\n",
    "        if len(buf) + len(sent) + 2 < max_chars:\n",
    "            buf += sent + \". \"\n",
    "        else:\n",
    "            chunks.append(buf.strip())\n",
    "            buf = sent + \". \"\n",
    "    if buf:\n",
    "        chunks.append(buf.strip())\n",
    "    return chunks\n",
    "\n",
    "# Build prompt for one chunk\n",
    "def build_prompt(chunk_text):\n",
    "    return f\"\"\"\n",
    "You are a sociology professor with 30 years of experience studying racism against Asians.\n",
    "Your task is to identify the quotes in articles that match your list of types of racism concepts, \n",
    "and also label who the victim is based on their race. \n",
    "\n",
    "1) First, read through the racism concept definitions. \n",
    "You need to understand these definitions so you can accurately recognize when a quote fits one or more of these concepts:\n",
    "{concept_defs}\n",
    "\n",
    "2) Next, review the example labeled quotes provided. \n",
    "You need to study these examples to see how quotes have been matched to concepts in practice, which will guide your own labeling decisions:\n",
    "{examples}\n",
    "\n",
    "3) Now read the ARTICLE CHUNK below. For each quote that matches at least one concept, output:\n",
    "  • quote: the exact text from article(if it’s under 50 chars, include one sentence before and after as “context” instead).\n",
    "  • concepts: a list of matching concept names.\n",
    "  • victim: the race of the victim. If the race cannot be inferred, label it as unknown.\n",
    "  • context: (only if the quote itself is under 50 characters; otherwise you can repeat the quote)\n",
    "\n",
    "ARTICLE CHUNK:\n",
    "{chunk_text}\n",
    "\n",
    "Return a JSON array like:\n",
    "[\n",
    "  {{ \"quote\": \"...\", \"context\": \"...\", \"concepts\": [\"C1\",\"C2\"], \"victim\": \"Asian\" }},\n",
    "  ...\n",
    "]\n",
    "\"\"\"\n",
    "results_df = pd.read_csv(\"classification_results_with_race_deepseek.csv\", encoding=\"ISO-8859-1\")\n",
    "processed_ids = set(results_df[\"article_id\"].unique())\n",
    "all_results = results_df.to_dict(orient=\"records\")  # continue collecting\n",
    "#all_results = []\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_json_output(output: str) -> str:\n",
    "    o = output.strip()\n",
    "    o = re.sub(r\"^```(?:json)?\\s*\\n?\", \"\", o)   # strip leading ``` or ```json\n",
    "    o = re.sub(r\"\\n?```$\", \"\", o)                # strip trailing ```\n",
    "    return o\n",
    "\n",
    "def safe_json_parse(raw: str):\n",
    "    \"\"\"\n",
    "    Try to coerce raw into valid JSON array:\n",
    "     - Strip markdown fences\n",
    "     - Remove trailing commas before ] \n",
    "     - Ensure opening [ and closing ]\n",
    "    Returns Python list or None if it still fails.\n",
    "    \"\"\"\n",
    "    txt = clean_json_output(raw)\n",
    "    # remove commas before closing ]\n",
    "    txt = re.sub(r\",\\s*]\", \"]\", txt)\n",
    "    # ensure it starts with [ and ends with ]\n",
    "    txt = txt.strip()\n",
    "    if not txt.startswith(\"[\"):\n",
    "        txt = \"[\" + txt\n",
    "    if not txt.endswith(\"]\"):\n",
    "        txt = txt + \"]\"\n",
    "    try:\n",
    "        return json.loads(txt)\n",
    "    except json.JSONDecodeError as e:\n",
    "        return None\n",
    "\n",
    "\n",
    "for _, row in tqdm(articles_df.iterrows(), total = article_num):\n",
    "    article_id = row[\"id\"]\n",
    "    if article_id in processed_ids:\n",
    "        continue\n",
    "    title = row[\"title\"]\n",
    "    chunks = split_text(row[\"ARTICLE_TEXT\"])\n",
    "\n",
    "    for chunk_i, chunk in enumerate(chunks):\n",
    "        prompt = build_prompt(chunk)\n",
    "        try:\n",
    "            headers = {\n",
    "                \"Authorization\": f\"Bearer {DEEPSEEK_API_KEY}\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            }\n",
    "            payload = {\n",
    "                \"model\": \"deepseek-chat\",   # or whatever DeepSeek’s model name is\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a sociology professor analyzing racism in text. Label quotes using provided concepts.\"},\n",
    "                    {\"role\": \"user\",   \"content\": prompt}\n",
    "                ],\n",
    "                \"temperature\": 0\n",
    "            }\n",
    "            r = requests.post(\n",
    "                \"https://api.deepseek.com/v1/chat/completions\", \n",
    "                headers=headers, \n",
    "                json=payload\n",
    "            )\n",
    "            r.raise_for_status()\n",
    "            raw = r.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "            labels = safe_json_parse(raw)\n",
    "            if labels is None:\n",
    "                print(f\"⚠️ Could not parse JSON for article {article_id}, chunk {chunk_i}\")\n",
    "                print(\"Raw output:\", raw)\n",
    "                continue\n",
    "\n",
    "\n",
    "            for item in labels:\n",
    "                all_results.append({\n",
    "                    \"article_id\": article_id,\n",
    "                    \"title\": title,\n",
    "                    \"quote\": item[\"quote\"],\n",
    "                    \"context\": item.get(\"context\", item[\"quote\"]),\n",
    "                    \"concepts\": \";\".join(item[\"concepts\"]),\n",
    "                    \"victim\": item[\"victim\"]\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error on article {article_id}, chunk {chunk_i}: {e}\")\n",
    "            # 'raw' always exists here, so we can inspect it\n",
    "            print(\"Raw output:\", raw)\n",
    "            continue  # skip to next chunk\n",
    "\n",
    "# Save flattened results\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df.to_csv(\"classification_results_with_race_deepseek.csv\", index=False)\n",
    "print(\"✅ Done — results including ‘victim’ and ‘context’ saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
