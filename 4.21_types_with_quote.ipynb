{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50d5a594-e179-4800-b58a-e74ab2296407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [02:58<00:00, 89.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done! Saved to classification_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "article_num = 2\n",
    "\n",
    "# Load API key\n",
    "load_dotenv()\n",
    "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Load your data\n",
    "articles_df = pd.read_csv(\"articles.csv\").head(article_num)\n",
    "definitions_df = pd.read_excel(\"racism_types_definitions.xlsx\")\n",
    "samples_df = pd.read_excel(\"sample_racism_classification.xlsx\").head(150)\n",
    "\n",
    "# Truncate long definitions to save tokens\n",
    "concept_defs = \"\\n\".join(\n",
    "    f\"{row['concepts']}: {row['definitions'][:300]}\"\n",
    "    for _, row in definitions_df.iterrows()\n",
    ")\n",
    "\n",
    "examples = \"\\n\".join(\n",
    "    f'\"{row[\"annotated_sentence\"]}\" → {row[\"annotation_content\"]}'\n",
    "    for _, row in samples_df.iterrows()\n",
    ")\n",
    "\n",
    "# Split long articles into ~3000‑char chunks at sentence boundaries\n",
    "def split_text(text, max_chars=3000):\n",
    "    sentences = text.split('. ')\n",
    "    chunks, buf = [], \"\"\n",
    "    for sent in sentences:\n",
    "        sent = sent.strip()\n",
    "        if len(buf) + len(sent) + 2 < max_chars:\n",
    "            buf += sent + \". \"\n",
    "        else:\n",
    "            chunks.append(buf.strip())\n",
    "            buf = sent + \". \"\n",
    "    if buf:\n",
    "        chunks.append(buf.strip())\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def build_stage1_prompt():\n",
    "    return f\"\"\"\n",
    "Stage 1: Definitions  \n",
    "You are a sociology professor with 30 years of experience studying racism against Asians.\n",
    "Read these racism concept definitions carefully—you need to internalize them so you can spot matching quotes later.\n",
    "\n",
    "Definitions:\n",
    "{concept_defs}\n",
    "\n",
    "If you understand these definitions and are ready for the next step, respond **exactly** with:\n",
    "I understand.\n",
    "Otherwise, respond **exactly** with:\n",
    "I do not understand the task.\n",
    "\"\"\"\n",
    "\n",
    "def build_stage2_prompt():\n",
    "    return f\"\"\"\n",
    "Stage 2: Examples  \n",
    "Now that you’ve read the definitions, here are some example quotes labeled with their concepts.\n",
    "Study them so you see how quotes map to definitions in practice:\n",
    "\n",
    "{examples}\n",
    "\n",
    "If you understand how to use these examples to guide your labeling, respond **exactly** with:\n",
    "I understand.\n",
    "Otherwise, respond **exactly** with:\n",
    "I do not understand the task.\n",
    "\"\"\"\n",
    "\n",
    "def build_stage3_prompt(chunk_text):\n",
    "    return f\"\"\"\n",
    "Stage 3: Labeling  \n",
    "You will now read the ARTICLE CHUNK below.  \n",
    "For each quote matching ≥1 concept, output a JSON array of objects with keys:\n",
    "- \"quote\": the exact text (if <50 chars, include one sentence before & after as \"context\" instead),\n",
    "- \"concepts\": list of matching concept names,\n",
    "- \"victim\": the race of the victim. If the race cannot be inferred, label it as unknown,\n",
    "- \"context\": only when quote was expanded (otherwise can repeat \"quote\").\n",
    "\n",
    "ARTICLE CHUNK:\n",
    "{chunk_text}\n",
    "\n",
    "Return only the JSON array, e.g.:\n",
    "\n",
    "[\n",
    "  {{ \"quote\": \"...\", \"context\": \"...\", \"concepts\": [\"C1\",\"C2\"], \"victim\": \"Asian\" }},\n",
    "  ...\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for _, row in tqdm(articles_df.iterrows(), total=len(articles_df)):\n",
    "    article_id = row[\"id\"]\n",
    "    title = row[\"title\"]\n",
    "    chunks = split_text(row[\"ARTICLE_TEXT\"])\n",
    "\n",
    "    for chunk_i, chunk in enumerate(chunks):\n",
    "        # --- Stage 1: Definitions ---\n",
    "        resp1 = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a sociology professor analyzing racism in text.\"},\n",
    "                {\"role\": \"user\",   \"content\": build_stage1_prompt()}\n",
    "            ],\n",
    "            temperature=0\n",
    "        ).choices[0].message.content.strip()\n",
    "\n",
    "        if resp1 != \"I understand.\":\n",
    "            print(f\"❌ Halt at Stage 1 for article {article_id}, chunk {chunk_i}: {resp1}\")\n",
    "            exit(1)\n",
    "\n",
    "        # --- Stage 2: Examples ---\n",
    "        resp2 = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a sociology professor analyzing racism in text.\"},\n",
    "                {\"role\": \"user\",   \"content\": build_stage2_prompt()}\n",
    "            ],\n",
    "            temperature=0\n",
    "        ).choices[0].message.content.strip()\n",
    "\n",
    "        if resp2 != \"I understand.\":\n",
    "            print(f\"❌ Halt at Stage 2 for article {article_id}, chunk {chunk_i}: {resp2}\")\n",
    "            exit(1)\n",
    "\n",
    "        # --- Stage 3: Labeling ---\n",
    "        resp3 = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a sociology professor analyzing racism in text.\"},\n",
    "                {\"role\": \"user\",   \"content\": build_stage3_prompt(chunk)}\n",
    "            ],\n",
    "            temperature=0\n",
    "        ).choices[0].message.content\n",
    "\n",
    "        # Parse JSON output\n",
    "        try:\n",
    "            labels = json.loads(resp3)\n",
    "            for item in labels:\n",
    "                all_results.append({\n",
    "                    \"article_id\": article_id,\n",
    "                    \"title\": title,\n",
    "                    \"quote\": item[\"quote\"],\n",
    "                    \"context\": item.get(\"context\", item[\"quote\"]),\n",
    "                    \"concepts\": \";\".join(item[\"concepts\"]),\n",
    "                    \"victim\": item[\"victim\"]\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ JSON parse error for article {article_id}, chunk {chunk_i}: {e}\")\n",
    "            print(\"Raw output:\", resp3)\n",
    "\n",
    "# Save flattened results\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df.to_csv(\"classification_results_with_victims_and_stages.csv\", index=False)\n",
    "print(\"✅ Done — all stages completed and results saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
