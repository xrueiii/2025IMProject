{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmMN5ZjZM+o/HSh0vQNj/z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xrueiii/2025IMProject/blob/main/%E5%B0%88%E9%A1%8C_%E5%88%A4%E6%96%B7%E9%87%8D%E8%A4%87%E6%96%87%E7%AB%A0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qD7-9LI6eqt1",
        "outputId": "b03315a9-d887-44c7-f6bc-f089a5f7fec8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q pandas sentence-transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"基本文本預處理：去除特殊字符，轉換為小寫\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    # 去除引號、換行符等\n",
        "    text = re.sub(r'[\\\"\\'\\n\\r\\\\]', ' ', str(text))\n",
        "    # 轉換為小寫\n",
        "    return text.lower()\n",
        "\n",
        "def find_similar_articles(file_path, similarity_threshold=0.8, columns_to_check=None, output_file=None):\n",
        "    \"\"\"\n",
        "    找出CSV文件中相似度高的文章對\n",
        "\n",
        "    參數:\n",
        "    file_path: CSV文件路徑\n",
        "    similarity_threshold: 相似度閾值（0-1之間，越高表示越相似）\n",
        "    columns_to_check: 要檢查相似度的列名列表，None表示檢查文章內容相關的列\n",
        "    output_file: 輸出結果的文件路徑\n",
        "\n",
        "    返回:\n",
        "    相似文章對的DataFrame\n",
        "    \"\"\"\n",
        "    print(f\"正在讀取CSV文件: {file_path}\")\n",
        "    # 讀取CSV文件\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        print(f\"成功讀取CSV文件，共有{len(df)}行\")\n",
        "        print(f\"CSV文件的列名: {list(df.columns)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"讀取文件出錯: {e}\")\n",
        "        return None\n",
        "\n",
        "    # 確定要檢查的列\n",
        "    if columns_to_check is None:\n",
        "        # 嘗試自動識別可能包含文章內容的列\n",
        "        possible_content_columns = ['ARTICLE_TEXT', 'article_text', 'text', 'content', 'body', 'full_text']\n",
        "        columns_to_check = []\n",
        "        for col in possible_content_columns:\n",
        "            if col in df.columns:\n",
        "                columns_to_check.append(col)\n",
        "                break\n",
        "\n",
        "        # 如果找不到明顯的內容列，檢查是否有可能包含文章內容的列\n",
        "        if not columns_to_check:\n",
        "            for col in df.columns:\n",
        "                if 'text' in col.lower() or 'content' in col.lower() or 'article' in col.lower():\n",
        "                    columns_to_check.append(col)\n",
        "                    break\n",
        "\n",
        "        # 如果還是找不到，使用最後一列（假設文章內容通常是最後一列）\n",
        "        if not columns_to_check and len(df.columns) > 0:\n",
        "            text_col = df.columns[-1]\n",
        "            # 檢查最後一列的內容是否看起來像文章（平均長度超過100個字符）\n",
        "            avg_length = df[text_col].astype(str).str.len().mean()\n",
        "            if avg_length > 100:\n",
        "                columns_to_check = [text_col]\n",
        "                print(f\"沒有找到明確的文章內容列，將使用最後一列 '{text_col}' 作為內容列（平均長度: {avg_length:.1f}個字符）\")\n",
        "            else:\n",
        "                # 嘗試找出平均長度最長的列\n",
        "                lengths = {col: df[col].astype(str).str.len().mean() for col in df.columns}\n",
        "                longest_col = max(lengths.items(), key=lambda x: x[1])\n",
        "                if longest_col[1] > 100:\n",
        "                    columns_to_check = [longest_col[0]]\n",
        "                    print(f\"將使用平均長度最長的列 '{longest_col[0]}' 作為內容列（平均長度: {longest_col[1]:.1f}個字符）\")\n",
        "                else:\n",
        "                    print(\"無法找到適合的文章內容列，請手動指定columns_to_check參數\")\n",
        "                    return None\n",
        "    else:\n",
        "        # 確保所有指定的列都存在\n",
        "        missing_cols = [col for col in columns_to_check if col not in df.columns]\n",
        "        if missing_cols:\n",
        "            print(f\"警告：以下指定的列不存在於CSV中: {missing_cols}\")\n",
        "            columns_to_check = [col for col in columns_to_check if col in df.columns]\n",
        "\n",
        "        if not columns_to_check:\n",
        "            print(\"所有指定的列都不存在於CSV中，無法進行分析\")\n",
        "            return None\n",
        "\n",
        "    print(f\"將檢查以下列的相似度: {columns_to_check}\")\n",
        "\n",
        "    # 準備要分析的文本\n",
        "    combined_texts = []\n",
        "    for _, row in df.iterrows():\n",
        "        combined_text = \" \".join([preprocess_text(row[col]) for col in columns_to_check])\n",
        "        combined_texts.append(combined_text)\n",
        "\n",
        "    # 使用TF-IDF向量化文本\n",
        "    print(\"正在向量化文本...\")\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    try:\n",
        "        tfidf_matrix = vectorizer.fit_transform(combined_texts)\n",
        "    except Exception as e:\n",
        "        print(f\"向量化文本出錯: {e}\")\n",
        "        return None\n",
        "\n",
        "    # 計算餘弦相似度\n",
        "    print(\"正在計算文本相似度...\")\n",
        "    cosine_sim = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "    # 找出高相似度的文章對\n",
        "    print(f\"正在尋找相似度高於 {similarity_threshold} 的文章對...\")\n",
        "    similar_pairs = []\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        # 只檢查i之後的文章，避免重複比較和自身比較\n",
        "        for j in range(i+1, len(df)):\n",
        "            similarity = cosine_sim[i, j]\n",
        "            if similarity >= similarity_threshold:\n",
        "                pair_info = {\n",
        "                    'similarity': similarity\n",
        "                }\n",
        "\n",
        "                # 添加兩篇文章的識別信息\n",
        "                for article_idx, prefix in [(i, 'article1_'), (j, 'article2_')]:\n",
        "                    # 檢查是否有id列\n",
        "                    if 'id' in df.columns:\n",
        "                        pair_info[f'{prefix}id'] = df.iloc[article_idx]['id']\n",
        "                    else:\n",
        "                        pair_info[f'{prefix}id'] = article_idx\n",
        "\n",
        "                    # 檢查是否有title列\n",
        "                    if 'title' in df.columns:\n",
        "                        pair_info[f'{prefix}title'] = df.iloc[article_idx]['title']\n",
        "\n",
        "                    # 添加一小部分內容作為預覽\n",
        "                    for col in columns_to_check:\n",
        "                        preview_text = str(df.iloc[article_idx][col])\n",
        "                        if len(preview_text) > 100:\n",
        "                            preview_text = preview_text[:100] + \"...\"\n",
        "                        pair_info[f'{prefix}{col}_preview'] = preview_text\n",
        "\n",
        "                similar_pairs.append(pair_info)\n",
        "\n",
        "    # 轉換為DataFrame並按相似度降序排序\n",
        "    if similar_pairs:\n",
        "        result_df = pd.DataFrame(similar_pairs)\n",
        "        result_df = result_df.sort_values(by='similarity', ascending=False)\n",
        "\n",
        "        # 輸出結果\n",
        "        print(f\"找到 {len(result_df)} 對相似文章\")\n",
        "        if output_file:\n",
        "            result_df.to_csv(output_file, index=False)\n",
        "            print(f\"結果已保存到 {output_file}\")\n",
        "\n",
        "        return result_df\n",
        "    else:\n",
        "        print(\"未找到符合閾值的相似文章對\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# 使用示例\n",
        "if __name__ == \"__main__\":\n",
        "    # 替換為您的CSV文件路徑\n",
        "    file_path = \"articles.csv\"\n",
        "\n",
        "    # 自動識別合適的列 - 您也可以明確指定要檢查的列\n",
        "    # columns_to_check = ['ARTICLE_TEXT']\n",
        "\n",
        "    # 查找相似度大於0.8的文章\n",
        "    similar_articles = find_similar_articles(\n",
        "        file_path=file_path,\n",
        "        similarity_threshold=0.8,\n",
        "        # columns_to_check=columns_to_check,  # 不指定，讓程序自動識別\n",
        "        output_file=\"similar_articles.csv\"\n",
        "    )\n",
        "\n",
        "    # 顯示結果的前幾行\n",
        "    if similar_articles is not None and not similar_articles.empty:\n",
        "        print(\"\\n相似文章對的前5行:\")\n",
        "        print(similar_articles.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiS0ApKUwMG9",
        "outputId": "c0700360-af63-4da4-c51c-d31665d29c50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "正在讀取CSV文件: articles.csv\n",
            "成功讀取CSV文件，共有634行\n",
            "CSV文件的列名: ['id', 'title', 'author', 'publisher', 'date', 'summary', 'ARTICLE_TEXT', 'media_type', 'status']\n",
            "將檢查以下列的相似度: ['ARTICLE_TEXT']\n",
            "正在向量化文本...\n",
            "正在計算文本相似度...\n",
            "正在尋找相似度高於 0.8 的文章對...\n",
            "找到 62 對相似文章\n",
            "結果已保存到 similar_articles.csv\n",
            "\n",
            "相似文章對的前5行:\n",
            "    similarity  article1_id  \\\n",
            "3          1.0           23   \n",
            "45         1.0          431   \n",
            "40         1.0          362   \n",
            "44         1.0          408   \n",
            "2          1.0            9   \n",
            "\n",
            "                                       article1_title  \\\n",
            "3      Anti-Asian attacks must no longer be minimized   \n",
            "45  Epidemic of hate as attacks on Asian Americans...   \n",
            "40  'I feel like it's more blatant': Students afra...   \n",
            "44  Biases you didn't know existed in the healthca...   \n",
            "2   National group received 67 reports of anti-Asi...   \n",
            "\n",
            "                        article1_ARTICLE_TEXT_preview  article2_id  \\\n",
            "3   After Dylan Adler was screamed at, chased and ...           29   \n",
            "45  Four prominent Asian Americans says prejudice,...          432   \n",
            "40  Abby Bogart was called a virus at the start of...          367   \n",
            "44  Biases are forms of discrimination in which we...          425   \n",
            "2   A year ago, Lunar New Year celebrations unfold...           55   \n",
            "\n",
            "                                       article2_title  \\\n",
            "3      Anti-Asian attacks must no longer be minimized   \n",
            "45  Epidemic of hate as attacks on Asian Americans...   \n",
            "40  'I feel like it's more blatant': Students afra...   \n",
            "44  Biases you didn't know existed in the healthca...   \n",
            "2   National group received 67 reports of anti-Asi...   \n",
            "\n",
            "                        article2_ARTICLE_TEXT_preview  \n",
            "3   After Dylan Adler was screamed at, chased and ...  \n",
            "45  Four prominent Asian Americans says prejudice,...  \n",
            "40  Abby Bogart was called a virus at the start of...  \n",
            "44  Biases are forms of discrimination in which we...  \n",
            "2   A year ago, Lunar New Year celebrations unfold...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def remove_duplicate_articles(articles_path, similarity_path, threshold=0.89, date_col='date', id_col='id'):\n",
        "    # 讀取資料\n",
        "    articles_df = pd.read_csv(\"/content/articles.csv\")\n",
        "    sim_df = pd.read_csv(\"/content/similar_articles.csv\")\n",
        "\n",
        "    # 儲存要刪除的 ID\n",
        "    ids_to_remove = set()\n",
        "\n",
        "    for _, row in sim_df.iterrows():\n",
        "        if row['similarity'] > threshold:\n",
        "            id1, id2 = int(row['article1_id']), int(row['article2_id'])\n",
        "\n",
        "            # 根據日期判斷誰是比較新的\n",
        "            date1 = pd.to_datetime(articles_df.loc[articles_df[id_col] == id1, date_col].values[0])\n",
        "            date2 = pd.to_datetime(articles_df.loc[articles_df[id_col] == id2, date_col].values[0])\n",
        "\n",
        "            if date1 > date2:\n",
        "                ids_to_remove.add(id1)\n",
        "            else:\n",
        "                ids_to_remove.add(id2)\n",
        "\n",
        "    # 刪除重複文章\n",
        "    before_count = len(articles_df)\n",
        "    filtered_df = articles_df[~articles_df[id_col].isin(ids_to_remove)]\n",
        "    after_count = len(filtered_df)\n",
        "\n",
        "    print(f\"共刪除了 {before_count - after_count} 篇文章\")\n",
        "    print(f\"剩餘 {after_count} 篇文章\")\n",
        "    print(\"被刪除的文章 ID：\", sorted(ids_to_remove))\n",
        "\n",
        "    return filtered_df\n",
        "\n",
        "# 使用範例\n",
        "filtered_articles = remove_duplicate_articles(\n",
        "    articles_path=\"articles.csv\",\n",
        "    similarity_path=\"similarities.csv\",\n",
        "    threshold=0.89\n",
        ")\n",
        "\n",
        "# 可選：儲存結果\n",
        "filtered_articles.to_csv(\"articles_deduplicated.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YwTWspnVfng",
        "outputId": "15b03ec5-764e-419b-fabb-d4061f96081f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "共刪除了 47 篇文章\n",
            "剩餘 587 篇文章\n",
            "被刪除的文章 ID： [7, 29, 41, 51, 54, 55, 63, 64, 77, 78, 95, 106, 111, 135, 138, 146, 215, 229, 234, 254, 261, 271, 287, 289, 295, 353, 354, 362, 372, 378, 407, 408, 432, 444, 447, 462, 464, 472, 482, 491, 497, 498, 502, 509, 608, 612, 615]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"基本文本預處理：去除特殊字符，轉換為小寫\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    # 去除引號、換行符等\n",
        "    text = re.sub(r'[\\\"\\'\\n\\r\\\\]', ' ', str(text))\n",
        "    # 轉換為小寫\n",
        "    return text.lower()\n",
        "\n",
        "def find_similar_articles(file_path, similarity_threshold=0.8, columns_to_check=None, output_file=None):\n",
        "    \"\"\"\n",
        "    找出CSV文件中相似度高的文章對\n",
        "\n",
        "    參數:\n",
        "    file_path: CSV文件路徑\n",
        "    similarity_threshold: 相似度閾值（0-1之間，越高表示越相似）\n",
        "    columns_to_check: 要檢查相似度的列名列表，None表示檢查文章內容相關的列\n",
        "    output_file: 輸出結果的文件路徑\n",
        "\n",
        "    返回:\n",
        "    相似文章對的DataFrame\n",
        "    \"\"\"\n",
        "    print(f\"正在讀取CSV文件: {file_path}\")\n",
        "    # 讀取CSV文件\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        print(f\"成功讀取CSV文件，共有{len(df)}行\")\n",
        "        print(f\"CSV文件的列名: {list(df.columns)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"讀取文件出錯: {e}\")\n",
        "        return None\n",
        "\n",
        "    # 確定要檢查的列\n",
        "    if columns_to_check is None:\n",
        "        # 嘗試自動識別可能包含文章內容的列\n",
        "        possible_content_columns = ['ARTICLE_TEXT', 'article_text', 'text', 'content', 'body', 'full_text']\n",
        "        columns_to_check = []\n",
        "        for col in possible_content_columns:\n",
        "            if col in df.columns:\n",
        "                columns_to_check.append(col)\n",
        "                break\n",
        "\n",
        "        # 如果找不到明顯的內容列，檢查是否有可能包含文章內容的列\n",
        "        if not columns_to_check:\n",
        "            for col in df.columns:\n",
        "                if 'text' in col.lower() or 'content' in col.lower() or 'article' in col.lower():\n",
        "                    columns_to_check.append(col)\n",
        "                    break\n",
        "\n",
        "        # 如果還是找不到，使用最後一列（假設文章內容通常是最後一列）\n",
        "        if not columns_to_check and len(df.columns) > 0:\n",
        "            text_col = df.columns[-1]\n",
        "            # 檢查最後一列的內容是否看起來像文章（平均長度超過100個字符）\n",
        "            avg_length = df[text_col].astype(str).str.len().mean()\n",
        "            if avg_length > 100:\n",
        "                columns_to_check = [text_col]\n",
        "                print(f\"沒有找到明確的文章內容列，將使用最後一列 '{text_col}' 作為內容列（平均長度: {avg_length:.1f}個字符）\")\n",
        "            else:\n",
        "                # 嘗試找出平均長度最長的列\n",
        "                lengths = {col: df[col].astype(str).str.len().mean() for col in df.columns}\n",
        "                longest_col = max(lengths.items(), key=lambda x: x[1])\n",
        "                if longest_col[1] > 100:\n",
        "                    columns_to_check = [longest_col[0]]\n",
        "                    print(f\"將使用平均長度最長的列 '{longest_col[0]}' 作為內容列（平均長度: {longest_col[1]:.1f}個字符）\")\n",
        "                else:\n",
        "                    print(\"無法找到適合的文章內容列，請手動指定columns_to_check參數\")\n",
        "                    return None\n",
        "    else:\n",
        "        # 確保所有指定的列都存在\n",
        "        missing_cols = [col for col in columns_to_check if col not in df.columns]\n",
        "        if missing_cols:\n",
        "            print(f\"警告：以下指定的列不存在於CSV中: {missing_cols}\")\n",
        "            columns_to_check = [col for col in columns_to_check if col in df.columns]\n",
        "\n",
        "        if not columns_to_check:\n",
        "            print(\"所有指定的列都不存在於CSV中，無法進行分析\")\n",
        "            return None\n",
        "\n",
        "    print(f\"將檢查以下列的相似度: {columns_to_check}\")\n",
        "\n",
        "    # 準備要分析的文本\n",
        "    combined_texts = []\n",
        "    for _, row in df.iterrows():\n",
        "        combined_text = \" \".join([preprocess_text(row[col]) for col in columns_to_check])\n",
        "        combined_texts.append(combined_text)\n",
        "\n",
        "    # 使用TF-IDF向量化文本\n",
        "    print(\"正在向量化文本...\")\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    try:\n",
        "        tfidf_matrix = vectorizer.fit_transform(combined_texts)\n",
        "    except Exception as e:\n",
        "        print(f\"向量化文本出錯: {e}\")\n",
        "        return None\n",
        "\n",
        "    # 計算餘弦相似度\n",
        "    print(\"正在計算文本相似度...\")\n",
        "    cosine_sim = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "    # 找出高相似度的文章對\n",
        "    print(f\"正在尋找相似度高於 {similarity_threshold} 的文章對...\")\n",
        "    similar_pairs = []\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        # 只檢查i之後的文章，避免重複比較和自身比較\n",
        "        for j in range(i+1, len(df)):\n",
        "            similarity = cosine_sim[i, j]\n",
        "            if similarity >= similarity_threshold:\n",
        "                pair_info = {\n",
        "                    'similarity': similarity\n",
        "                }\n",
        "\n",
        "                # 添加兩篇文章的識別信息\n",
        "                for article_idx, prefix in [(i, 'article1_'), (j, 'article2_')]:\n",
        "                    # 檢查是否有id列\n",
        "                    if 'id' in df.columns:\n",
        "                        pair_info[f'{prefix}id'] = df.iloc[article_idx]['id']\n",
        "                    else:\n",
        "                        pair_info[f'{prefix}id'] = article_idx\n",
        "\n",
        "                    # 檢查是否有title列\n",
        "                    if 'title' in df.columns:\n",
        "                        pair_info[f'{prefix}title'] = df.iloc[article_idx]['title']\n",
        "\n",
        "                    # 添加一小部分內容作為預覽\n",
        "                    for col in columns_to_check:\n",
        "                        preview_text = str(df.iloc[article_idx][col])\n",
        "                        if len(preview_text) > 100:\n",
        "                            preview_text = preview_text[:100] + \"...\"\n",
        "                        pair_info[f'{prefix}{col}_preview'] = preview_text\n",
        "\n",
        "                similar_pairs.append(pair_info)\n",
        "\n",
        "    # 轉換為DataFrame並按相似度降序排序\n",
        "    if similar_pairs:\n",
        "        result_df = pd.DataFrame(similar_pairs)\n",
        "        result_df = result_df.sort_values(by='similarity', ascending=False)\n",
        "\n",
        "        # 輸出結果\n",
        "        print(f\"找到 {len(result_df)} 對相似文章\")\n",
        "        if output_file:\n",
        "            result_df.to_csv(output_file, index=False)\n",
        "            print(f\"結果已保存到 {output_file}\")\n",
        "\n",
        "        return result_df\n",
        "    else:\n",
        "        print(\"未找到符合閾值的相似文章對\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# 使用示例\n",
        "if __name__ == \"__main__\":\n",
        "    # 替換為您的CSV文件路徑\n",
        "    file_path = \"/content/articles_deduplicated.csv\"\n",
        "\n",
        "    # 自動識別合適的列 - 您也可以明確指定要檢查的列\n",
        "    # columns_to_check = ['ARTICLE_TEXT']\n",
        "\n",
        "    # 查找相似度大於0.8的文章\n",
        "    similar_articles = find_similar_articles(\n",
        "        file_path=file_path,\n",
        "        similarity_threshold=0.8,\n",
        "        # columns_to_check=columns_to_check,  # 不指定，讓程序自動識別\n",
        "        output_file=\"similar_articles_round2.csv\"\n",
        "    )\n",
        "\n",
        "    # 顯示結果的前幾行\n",
        "    if similar_articles is not None and not similar_articles.empty:\n",
        "        print(\"\\n相似文章對的前5行:\")\n",
        "        print(similar_articles.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnRKvM4DWcM2",
        "outputId": "5c08029d-58d5-4bb3-da42-c97e754d48ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "正在讀取CSV文件: /content/articles_deduplicated.csv\n",
            "成功讀取CSV文件，共有587行\n",
            "CSV文件的列名: ['id', 'title', 'author', 'publisher', 'date', 'summary', 'ARTICLE_TEXT', 'media_type', 'status']\n",
            "將檢查以下列的相似度: ['ARTICLE_TEXT']\n",
            "正在向量化文本...\n",
            "正在計算文本相似度...\n",
            "正在尋找相似度高於 0.8 的文章對...\n",
            "找到 6 對相似文章\n",
            "結果已保存到 similar_articles_round2.csv\n",
            "\n",
            "相似文章對的前5行:\n",
            "   similarity  article1_id                                     article1_title  \\\n",
            "1    0.887819           73  How Anti-Asian Activity Online Set the Stage f...   \n",
            "5    0.885127          590  Virus ravages San Francisco's Asian American c...   \n",
            "2    0.849716           81  A Tense Lunar New Year for the Bay Area After ...   \n",
            "4    0.848496          590  Virus ravages San Francisco's Asian American c...   \n",
            "3    0.840703          269  CANADIANS MUST ‘STAND UP' IN FIGHT AGAINST ANT...   \n",
            "\n",
            "                       article1_ARTICLE_TEXT_preview  article2_id  \\\n",
            "1  Protesters gather for a silent vigil in the Ch...          250   \n",
            "5  SAN FRANCISCO – Mandy Rong was terrified her 1...          595   \n",
            "2  The videos are graphic and shocking.In January...           82   \n",
            "4  SAN FRANCISCO – Mandy Rong was terrified her 1...          591   \n",
            "3  Amy Go says she was saddened by the shootings ...          316   \n",
            "\n",
            "                                      article2_title  \\\n",
            "1  Racist memes and posts about Asian-Americans o...   \n",
            "5                           The risk of invisibility   \n",
            "2  A Lunar New Year of Safety Whistles Instead of...   \n",
            "4  Asian Americans in San Francisco are dying at ...   \n",
            "3  Advocates call on Canadians to examine treatme...   \n",
            "\n",
            "                       article2_ARTICLE_TEXT_preview  \n",
            "1  In January, a new group popped up on the messa...  \n",
            "5  SAN FRANCISCO – Mandy Rong was terrified her 1...  \n",
            "2  The attacks have renewed fears over a wave of ...  \n",
            "4  SAN FRANCISCO – Mandy Rong was terrified her 1...  \n",
            "3  Amy Go says she was saddened by the shootings ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "根據 round_2 的similarity 計算之後，在下面的程式刪除了 [590, 595, 269] 這三篇文章，因此目前共刪除了 50 篇文章，剩餘 584 篇文章"
      ],
      "metadata": {
        "id": "krOToHr3lISo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 讀取文章\n",
        "articles_df = pd.read_csv(\"/content/articles_deduplicated.csv\")\n",
        "\n",
        "# 1. 刪除特定 ID\n",
        "ids_to_delete = [590, 595, 269]\n",
        "filtered_df = articles_df[~articles_df[\"id\"].isin(ids_to_delete)]\n",
        "\n",
        "# 轉換日期欄位為 datetime 格式\n",
        "filtered_df[\"date\"] = pd.to_datetime(filtered_df[\"date\"], format=\"%Y/%m/%d\", errors='coerce')\n",
        "\n",
        "# 2. 統計每個出版社總共有幾篇文章\n",
        "publisher_total_counts = filtered_df[\"publisher\"].value_counts().reset_index()\n",
        "publisher_total_counts.columns = [\"publisher\", \"total_articles\"]\n",
        "\n",
        "# 3. 統計每個出版社在 2021/3/16 之前的文章數\n",
        "cutoff_date = pd.to_datetime(\"2021/03/16\")\n",
        "filtered_before_cutoff = filtered_df[filtered_df[\"date\"] < cutoff_date]\n",
        "publisher_before_counts = filtered_before_cutoff[\"publisher\"].value_counts().reset_index()\n",
        "publisher_before_counts.columns = [\"publisher\", \"articles_before_2021_03_16\"]\n",
        "\n",
        "# 4. 合併兩個統計表格\n",
        "publisher_stats = pd.merge(\n",
        "    publisher_total_counts,\n",
        "    publisher_before_counts,\n",
        "    on=\"publisher\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "# 將 NaN 補 0 並轉成整數（某些出版社可能沒在 cutoff 前發表）\n",
        "publisher_stats[\"articles_before_2021_03_16\"] = publisher_stats[\"articles_before_2021_03_16\"].fillna(0).astype(int)\n",
        "\n",
        "# 儲存處理後的文章與統計結果\n",
        "filtered_df.to_csv(\"articles_after_manual_delete.csv\", index=False)\n",
        "publisher_stats.to_csv(\"publisher_article_stats.csv\", index=False)\n",
        "\n",
        "# 顯示統計結果\n",
        "print(publisher_stats)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOm5l4pgeQ8a",
        "outputId": "8b0d260a-7a95-4623-d60d-2cf8ef5bd6e0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-464968845cd2>:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_df[\"date\"] = pd.to_datetime(filtered_df[\"date\"], format=\"%Y/%m/%d\", errors='coerce')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         publisher  total_articles  articles_before_2021_03_16\n",
            "0           Others             429                           0\n",
            "1        USA Today              45                           0\n",
            "2         LA Times              31                           0\n",
            "3   New York Times              27                           0\n",
            "4  Washington Post              24                           0\n",
            "5     Boston Globe              17                           0\n",
            "6  Chicago Tribune               8                           0\n",
            "7     Star Tribune               3                           0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 讀入文章資料\n",
        "articles_df = pd.read_csv(\"/content/articles_deduplicated.csv\")\n",
        "\n",
        "# 1. 刪除指定的文章 ID\n",
        "ids_to_delete = [590, 595, 269]\n",
        "filtered_df = articles_df[~articles_df[\"id\"].isin(ids_to_delete)]\n",
        "\n",
        "# 2. 轉換 date 為 datetime 格式（格式為 yyyy-mm-dd）\n",
        "filtered_df[\"date\"] = pd.to_datetime(filtered_df[\"date\"], format=\"%Y-%m-%d\", errors='coerce')\n",
        "\n",
        "# 3. 設定事件日期（2021/3/16）\n",
        "event_date = pd.to_datetime(\"2021-03-16\")\n",
        "\n",
        "# 4. 計算總數、事件前、事件後文章數（以 publisher 分組）\n",
        "grouped = filtered_df.groupby(\"publisher\")\n",
        "\n",
        "stats = pd.DataFrame({\n",
        "    \"total_articles\": grouped.size(),\n",
        "    \"articles_before_event\": grouped.apply(lambda x: (x[\"date\"] < event_date).sum()),\n",
        "    \"articles_after_event\": grouped.apply(lambda x: (x[\"date\"] >= event_date).sum())\n",
        "}).reset_index()\n",
        "\n",
        "# 5. 儲存結果\n",
        "filtered_df.to_csv(\"articles_after_manual_delete.csv\", index=False)\n",
        "stats.to_csv(\"publisher_article_stats.csv\", index=False)\n",
        "\n",
        "# 顯示結果\n",
        "print(stats)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HOUsVGTfMcI",
        "outputId": "5e59c86b-92e6-413f-c31d-a25d2e771cfd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         publisher  total_articles  articles_before_event  \\\n",
            "0     Boston Globe              17                      4   \n",
            "1  Chicago Tribune               8                      4   \n",
            "2         LA Times              31                     11   \n",
            "3   New York Times              27                     16   \n",
            "4           Others             429                    217   \n",
            "5     Star Tribune               3                      0   \n",
            "6        USA Today              45                     19   \n",
            "7  Washington Post              24                     15   \n",
            "\n",
            "   articles_after_event  \n",
            "0                    13  \n",
            "1                     4  \n",
            "2                    20  \n",
            "3                    11  \n",
            "4                   212  \n",
            "5                     3  \n",
            "6                    26  \n",
            "7                     9  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-bc32e14bb68d>:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_df[\"date\"] = pd.to_datetime(filtered_df[\"date\"], format=\"%Y-%m-%d\", errors='coerce')\n",
            "<ipython-input-5-bc32e14bb68d>:21: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  \"articles_before_event\": grouped.apply(lambda x: (x[\"date\"] < event_date).sum()),\n",
            "<ipython-input-5-bc32e14bb68d>:22: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  \"articles_after_event\": grouped.apply(lambda x: (x[\"date\"] >= event_date).sum())\n"
          ]
        }
      ]
    }
  ]
}