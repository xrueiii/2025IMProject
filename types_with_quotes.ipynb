{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0a575515-4224-46c6-afd1-b0c2a534f223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Processing single test article using DeepSeek: Racism is the other virus sweeping America during this pandemic\n",
      "‚è≥ Waiting for DeepSeek response...\n",
      "‚úÖ Response received!\n",
      "üìù Concept 1: Verbal harassment\n",
      "üìå Quote: \"they are hearing the all-too-familiar vitriol toward Asian Americans being spewed from the dark and angry corners of social media and beyond.\"\n",
      "üìñ Source: concepts_definitions.csv\n",
      "\n",
      "üìù Concept 2: Online harassment\n",
      "üìå Quote: \"they are hearing the all-too-familiar vitriol toward Asian Americans being spewed from the dark and angry corners of social media and beyond.\"\n",
      "üìñ Source: concepts_definitions.csv\n",
      "\n",
      "üìù Concept 3: Worry about safety\n",
      "üìå Quote: \"This treatment of Asian Americans is immoral, but also dangerous to public health. Research shows that hateful speech and other actions against racial and ethnic minorities -- even seemingly small slights -- might make people sick, contributing to heart disease, respiratory illness and other chronic diseases.\"\n",
      "üìñ Source: concepts_definitions.csv\n",
      "\n",
      "üìù Concept 4: Support Asian Americans\n",
      "üìå Quote: \"During and after this pandemic, we must treat Asian Americans not as enemies, but as fellow victims of this insidious virus that does not distinguish by place, race, age or gender. This is the time to support our neighbors, not turn on them.\"\n",
      "üìñ Source: concepts_definitions.csv\n",
      "\n",
      "üìù Concept 5: Not confronting attacker/harasser or not reporting\n",
      "üìå Quote: \"In an environment where people of any race feel threatened, they might be less likely to seek treatment out of fear and distrust of our medical system.\"\n",
      "üìñ Source: concepts_definitions.csv\n",
      "\n",
      "üìù Concept 6: Structural Racism\n",
      "üìå Quote: \"Racism has its own virulence that is bad for the nation's soul and, as research has shown, is actually bad for the nation's health.\"\n",
      "üìñ Source: 4. structural_racism_and_health_equity_language_guide.pdf\n",
      "\n",
      "üìù Concept 7: Perpetual Foreigner\n",
      "üìå Quote: \"As Japanese Americans, they experienced as children the pain of internment during World War II, one of the terrible scars in our country's past that stemmed from fear, hate and racism.\"\n",
      "üìñ Source: 3. Anti-Hate Glossary.pdf\n",
      "\n",
      "üìù Concept 8: Model Minority\n",
      "üìå Quote: \"Given a chance, my parents thrived, both finishing high school and going on to college. My dad joined the Army and then went to dental school on the GI Bill. My mom became a high school art teacher.\"\n",
      "üìñ Source: 3. Anti-Hate Glossary.pdf\n",
      "\n",
      "üéâ Test complete! Results saved in test_concept_extraction_deepseek.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from docx import Document\n",
    "import pdfplumber\n",
    "\n",
    "# Set DeepSeek API Key\n",
    "with open(\"DEEPSEEK_API_KEY.txt\", \"r\") as file:\n",
    "    deepseek_api_key = file.read().strip()\n",
    "\n",
    "DEEPSEEK_API_URL = \"https://api.deepseek.com/v1/chat/completions\"\n",
    "\n",
    "# Load Concept Definitions CSV (Manually defined concepts)\n",
    "concepts_df = pd.read_csv(\"concepts_definitions.csv\")\n",
    "# Build a dictionary mapping concept name to its definition.\n",
    "concept_definitions = {row[\"concepts\"]: row[\"definitions\"] for _, row in concepts_df.iterrows()}\n",
    "\n",
    "# Function to extract text from Word files\n",
    "def extract_text_from_word(word_path):\n",
    "    doc = Document(word_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "# Function to extract text from PDF files\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        return \"\\n\".join([page.extract_text() for page in pdf.pages if page.extract_text()])\n",
    "\n",
    "# Function to load glossary terms from files (Word and PDF)\n",
    "def load_glossary_from_folder(glossary_folder_path, max_entries=5):\n",
    "    glossary = {}\n",
    "    entries_added = 0\n",
    "    for file_name in os.listdir(glossary_folder_path):\n",
    "        if entries_added >= max_entries:\n",
    "            break  # Limit glossary size to avoid exceeding token limits\n",
    "        file_path = os.path.join(glossary_folder_path, file_name)\n",
    "        if file_name.endswith(\".docx\"):\n",
    "            text = extract_text_from_word(file_path)\n",
    "        elif file_name.endswith(\".pdf\"):\n",
    "            text = extract_text_from_pdf(file_path)\n",
    "        else:\n",
    "            continue  # Skip unsupported file types\n",
    "        glossary[file_name] = text[:2000]  # Limit text length per glossary file\n",
    "        entries_added += 1\n",
    "    return glossary\n",
    "\n",
    "# Load limited glossary terms (max 5 entries)\n",
    "glossary_folder = \"glossary\"\n",
    "glossary = load_glossary_from_folder(glossary_folder)\n",
    "\n",
    "# Load articles CSV\n",
    "articles_df = pd.read_csv(\"articles.csv\")\n",
    "\n",
    "# Function to format concepts for DeepSeek API.\n",
    "# Manual concepts now include the concept name (with definition and source),\n",
    "# and glossary concepts are appended with their source.\n",
    "def format_concepts_for_deepseek(max_manual=10, max_glossary=2):\n",
    "    manual_concepts = \"\\n\".join([\n",
    "        f\"{concept} (Definition: {definition}, Source: concepts_definitions.csv)\"\n",
    "        for concept, definition in list(concept_definitions.items())[:max_manual]\n",
    "    ])\n",
    "    \n",
    "    glossary_concepts = \"\\n\\n\".join([\n",
    "        f\"{file_name} Terms: {text[:1000]} (Source: {file_name})\"\n",
    "        for file_name, text in list(glossary.items())[:max_glossary]\n",
    "    ])\n",
    "    \n",
    "    return manual_concepts, glossary_concepts\n",
    "\n",
    "# Function to classify articles and extract multiple concepts using DeepSeek.\n",
    "# The prompt instructs the AI to find all relevant concepts and even keywords that exactly match.\n",
    "def classify_article_with_deepseek(title, text):\n",
    "    manual_concepts, glossary_concepts = format_concepts_for_deepseek()\n",
    "\n",
    "    # Revised prompt:\n",
    "    prompt = f\"\"\"\n",
    "You are an expert in text analysis for racism-related themes. Below is a list of defined concepts and glossary keywords.\n",
    "\n",
    "Manual Concepts:\n",
    "{manual_concepts}\n",
    "\n",
    "Glossary Concepts:\n",
    "{glossary_concepts}\n",
    "\n",
    "Now, read the following article and do the following:\n",
    "1. Identify all concepts that are relevant to the article.\n",
    "2. For each detected concept, if the concept name or an exact keyword appears in the article, output that exact word as the \"Quote\".\n",
    "3. If the concept is only inferred (and no exact keyword appears), then include the most relevant excerpt from the article as the Quote.\n",
    "4. Include the source for each concept (either \"concepts_definitions.csv\" or the corresponding glossary file name).\n",
    "\n",
    "Article Title: {title}\n",
    "Article Text: {text}\n",
    "\n",
    "Provide the output in the following format:\n",
    "- Concept: [Detected Concept]\n",
    "  Quote: [Exact matching keyword or relevant excerpt]\n",
    "  Source: [Concept Source]\n",
    "Make sure to list every relevant concept separately.\n",
    "    \"\"\"\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {deepseek_api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"deepseek-coder\",  # You can adjust the model as needed\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in text analysis.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"max_tokens\": 2000\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(DEEPSEEK_API_URL, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå DeepSeek API failed: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Process only the first article for testing\n",
    "row = articles_df.iloc[0]\n",
    "title = row[\"title\"]\n",
    "article_text = row[\"ARTICLE_TEXT\"]\n",
    "\n",
    "print(f\"üîÑ Processing single test article using DeepSeek: {title}\")\n",
    "\n",
    "try:\n",
    "    print(\"‚è≥ Waiting for DeepSeek response...\")\n",
    "    extracted_info = classify_article_with_deepseek(title, article_text)\n",
    "    print(\"‚úÖ Response received!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå DeepSeek API failed: {e}\")\n",
    "    extracted_info = \"\"\n",
    "\n",
    "# Parse response to extract multiple concepts, quotes, and sources\n",
    "results = []\n",
    "try:\n",
    "    # We expect the output to be in a structured format with each concept starting with \"- Concept:\"\n",
    "    lines = extracted_info.split(\"\\n\")\n",
    "    concept, quote, source = None, None, None\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith(\"- Concept:\"):\n",
    "            if concept and quote and source:\n",
    "                results.append({\"title\": title, \"concept\": concept, \"quote\": quote, \"source\": source})\n",
    "            concept = line.replace(\"- Concept:\", \"\").strip()\n",
    "            quote = None\n",
    "            source = None\n",
    "        elif line.startswith(\"  Quote:\"):\n",
    "            quote = line.replace(\"  Quote:\", \"\").strip()\n",
    "        elif line.startswith(\"  Source:\"):\n",
    "            source = line.replace(\"  Source:\", \"\").strip()\n",
    "    if concept and quote and source:\n",
    "        results.append({\"title\": title, \"concept\": concept, \"quote\": quote, \"source\": source})\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error parsing response: {e}\")\n",
    "\n",
    "# Print results\n",
    "for idx, res in enumerate(results):\n",
    "    print(f\"üìù Concept {idx+1}: {res['concept']}\")\n",
    "    print(f\"üìå Quote: {res['quote']}\")\n",
    "    print(f\"üìñ Source: {res['source']}\\n\")\n",
    "\n",
    "# Save the result to a test CSV\n",
    "test_df = pd.DataFrame(results)\n",
    "test_df.to_csv(\"test_concept_extraction_deepseek.csv\", index=False)\n",
    "\n",
    "print(\"üéâ Test complete! Results saved in test_concept_extraction_deepseek.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e285a1e-a4b2-4a21-888b-d1cac497ce25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Processing single test article using DeepSeek: Racism is the other virus sweeping America during this pandemic\n",
      "‚è≥ Waiting for DeepSeek response...\n",
      "‚úÖ Response received!\n",
      "üìù Concept 1: Xenophobia\n",
      "üìå Quote: \"With the coronavirus pandemic today, they are hearing the all-too-familiar vitriol toward Asian Americans being spewed from the dark and angry corners of social media and beyond.\"\n",
      "üìñ Source: concepts_definitions.csv\n",
      "\n",
      "üìù Concept 2: Verbal harassment\n",
      "üìå Quote: \"With the coronavirus pandemic today, they are hearing the all-too-familiar vitriol toward Asian Americans being spewed from the dark and angry corners of social media and beyond.\"\n",
      "üìñ Source: concepts_definitions.csv\n",
      "\n",
      "üìù Concept 3: Online harassment\n",
      "üìå Quote: \"With the coronavirus pandemic today, they are hearing the all-too-familiar vitriol toward Asian Americans being spewed from the dark and angry corners of social media and beyond.\"\n",
      "üìñ Source: concepts_definitions.csv\n",
      "\n",
      "üìù Concept 4: Anti-Asian hate crimes(general)\n",
      "üìå Quote: \"Now, I must speak out in the face of new attacks on Asian Americans.\"\n",
      "üìñ Source: concepts_definitions.csv\n",
      "\n",
      "üìù Concept 5: Bigotry/prejudice\n",
      "üìå Quote: \"I can't remain silent as I witness the bigotry that has percolated in Chicago and across the country in response to the novel coronavirus pandemic that began in Wuhan, China.\"\n",
      "üìñ Source: 3. Anti-Hate Glossary.pdf\n",
      "\n",
      "üìù Concept 6: Structural racism\n",
      "üìå Quote: \"Racism has its own virulence that is bad for the nation's soul and, as research has shown, is actually bad for the nation's health.\"\n",
      "üìñ Source: 4. structural_racism_and_health_equity_language_guide.pdf\n",
      "\n",
      "üìù Concept 7: Perpetual Foreigner\n",
      "üìå Quote: \"During and after this pandemic, we must treat Asian Americans not as enemies, but as fellow victims of this insidious virus that does not distinguish by place, race, age or gender.\"\n",
      "üìñ Source: 3. Anti-Hate Glossary.pdf\n",
      "\n",
      "üìù Concept 8: Verbal harassment\n",
      "üìå Quote: \"My parents taught us to turn the other cheek, to be confident in our identity and values, no matter the situation.\"\n",
      "üìñ Source: concepts_definitions.csv\n",
      "\n",
      "üìù Concept 9: Anti-Asian hate crimes(general)\n",
      "üìå Quote: \"A preliminary analysis, in fact, examined nearly 1 million tweets focused on racial minorities between November 2019 and March 2020 -- the period when the novel coronavirus went from outbreak to pandemic. The percentage of tweets involving negative speech against Asian Americans increased 70% during that period while falling 4% for all other racial minority groups.\"\n",
      "üìñ Source: concepts_definitions.csv\n",
      "\n",
      "üìù Concept 10: Online harassment\n",
      "üìå Quote: \"A preliminary analysis, in fact, examined nearly 1 million tweets focused on racial minorities between November 2019 and March 2020 -- the period when the novel coronavirus went from outbreak to pandemic. The percentage of tweets involving negative speech against Asian Americans increased 70% during that period while falling 4% for all other racial minority groups.\"\n",
      "üìñ Source: concepts_definitions.csv\n",
      "\n",
      "üìù Concept 11: Xenophobia\n",
      "üìå Quote: \"This treatment of Asian Americans is immoral, but also dangerous to public health.\"\n",
      "üìñ Source: concepts_definitions.csv\n",
      "\n",
      "üìù Concept 12: Stop AAPI Hate\n",
      "üìå Quote: \"Now, I must speak out in the face of new attacks on Asian Americans.\"\n",
      "üìñ Source: concepts_definitions.csv\n",
      "\n",
      "üéâ Test complete! Results saved in test_concept_extraction_deepseek.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from docx import Document\n",
    "import pdfplumber\n",
    "\n",
    "# Set DeepSeek API Key\n",
    "with open(\"DEEPSEEK_API_KEY.txt\", \"r\") as file:\n",
    "    deepseek_api_key = file.read().strip()\n",
    "\n",
    "DEEPSEEK_API_URL = \"https://api.deepseek.com/v1/chat/completions\"\n",
    "\n",
    "# Load Concept Definitions CSV (Manually defined concepts)\n",
    "concepts_df = pd.read_csv(\"racism_types_definitions.csv\")\n",
    "# Build a dictionary mapping concept name to its definition.\n",
    "concept_definitions = {row[\"concepts\"]: row[\"definitions\"] for _, row in concepts_df.iterrows()}\n",
    "\n",
    "# Function to extract text from Word files\n",
    "def extract_text_from_word(word_path):\n",
    "    doc = Document(word_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "# Function to extract text from PDF files\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        return \"\\n\".join([page.extract_text() for page in pdf.pages if page.extract_text()])\n",
    "\n",
    "# Function to load glossary terms from files (Word and PDF)\n",
    "def load_glossary_from_folder(glossary_folder_path, max_entries=5):\n",
    "    glossary = {}\n",
    "    entries_added = 0\n",
    "    for file_name in os.listdir(glossary_folder_path):\n",
    "        if entries_added >= max_entries:\n",
    "            break  # Limit glossary size to avoid exceeding token limits\n",
    "        file_path = os.path.join(glossary_folder_path, file_name)\n",
    "        if file_name.endswith(\".docx\"):\n",
    "            text = extract_text_from_word(file_path)\n",
    "        elif file_name.endswith(\".pdf\"):\n",
    "            text = extract_text_from_pdf(file_path)\n",
    "        else:\n",
    "            continue  # Skip unsupported file types\n",
    "        glossary[file_name] = text[:2000]  # Limit text length per glossary file\n",
    "        entries_added += 1\n",
    "    return glossary\n",
    "\n",
    "# Load limited glossary terms (max 5 entries)\n",
    "glossary_folder = \"glossary\"\n",
    "glossary = load_glossary_from_folder(glossary_folder)\n",
    "\n",
    "# Load articles CSV\n",
    "articles_df = pd.read_csv(\"articles.csv\")\n",
    "\n",
    "# Function to format concepts for DeepSeek API.\n",
    "# Manual concepts now include the concept name (with definition and source),\n",
    "# and glossary concepts are appended with their source.\n",
    "def format_concepts_for_deepseek(max_manual=10, max_glossary=2):\n",
    "    manual_concepts = \"\\n\".join([\n",
    "        f\"{concept} (Definition: {definition}, Source: concepts_definitions.csv)\"\n",
    "        for concept, definition in list(concept_definitions.items())[:max_manual]\n",
    "    ])\n",
    "    \n",
    "    glossary_concepts = \"\\n\\n\".join([\n",
    "        f\"{file_name} Terms: {text[:1000]} (Source: {file_name})\"\n",
    "        for file_name, text in list(glossary.items())[:max_glossary]\n",
    "    ])\n",
    "    \n",
    "    return manual_concepts, glossary_concepts\n",
    "\n",
    "# Function to classify articles and extract multiple concepts using DeepSeek.\n",
    "# The prompt instructs the AI to find all relevant concepts and even keywords that exactly match.\n",
    "def classify_article_with_deepseek(title, text):\n",
    "    manual_concepts, glossary_concepts = format_concepts_for_deepseek()\n",
    "\n",
    "    # Revised prompt:\n",
    "    prompt = f\"\"\"\n",
    "You are an expert in text analysis for racism-related themes. Below is a list of defined racism types concepts and glossary keywords.\n",
    "\n",
    "Manual Concepts:\n",
    "{manual_concepts}\n",
    "\n",
    "Glossary Concepts:\n",
    "{glossary_concepts}\n",
    "\n",
    "Now, read the following article and do the following:\n",
    "1. For each racism type, search the article for any occurrence of its name or known synonyms. For example, if the concept \"Bigotry/prejudice\" is defined and the article contains the keyword \"bigotry\", then that concept should be detected.\n",
    "2. When a matching keyword or synonym is found, output the **entire sentence** that contains the match as the \"Quote\".\n",
    "3. Use the standardized concept label from the manual concepts (e.g. \"Bigotry/prejudice\") as the \"Concept\".\n",
    "4. If the concept is not directly mentioned but is inferred from the context, output the sentence that best represents it.\n",
    "5. If a racism type is detected but no manual concepts match, then you can label it with the glossary concepts.  \n",
    "6. Always include the source for each concept (e.g. \"concepts_definitions.csv\" or the corresponding glossary file name).\n",
    "6. There are many synonyms; if multiple variants are found in one sentence, still output that sentence once, with the standardized concept label.\n",
    "\n",
    "Article Title: {title}\n",
    "Article Text: {text}\n",
    "\n",
    "Provide the output in the following format:\n",
    "- Concept: [Detected Concept]\n",
    "  Quote: [Exact matching keyword or relevant excerpt]\n",
    "  Source: [Concept Source]\n",
    "Make sure to list every relevant concept separately.\n",
    "    \"\"\"\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {deepseek_api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"deepseek-coder\",  # You can adjust the model as needed\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in text analysis.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"max_tokens\": 2000\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(DEEPSEEK_API_URL, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå DeepSeek API failed: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Process only the first article for testing\n",
    "row = articles_df.iloc[0]\n",
    "title = row[\"title\"]\n",
    "article_text = row[\"ARTICLE_TEXT\"]\n",
    "\n",
    "print(f\"üîÑ Processing single test article using DeepSeek: {title}\")\n",
    "\n",
    "try:\n",
    "    print(\"‚è≥ Waiting for DeepSeek response...\")\n",
    "    extracted_info = classify_article_with_deepseek(title, article_text)\n",
    "    print(\"‚úÖ Response received!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå DeepSeek API failed: {e}\")\n",
    "    extracted_info = \"\"\n",
    "\n",
    "# Parse response to extract multiple concepts, quotes, and sources\n",
    "results = []\n",
    "try:\n",
    "    # We expect the output to be in a structured format with each concept starting with \"- Concept:\"\n",
    "    lines = extracted_info.split(\"\\n\")\n",
    "    concept, quote, source = None, None, None\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith(\"- Concept:\"):\n",
    "            if concept and quote and source:\n",
    "                results.append({\"title\": title, \"concept\": concept, \"quote\": quote, \"source\": source})\n",
    "            concept = line.replace(\"- Concept:\", \"\").strip()\n",
    "            quote = None\n",
    "            source = None\n",
    "        elif line.startswith(\"  Quote:\"):\n",
    "            quote = line.replace(\"  Quote:\", \"\").strip()\n",
    "        elif line.startswith(\"  Source:\"):\n",
    "            source = line.replace(\"  Source:\", \"\").strip()\n",
    "    if concept and quote and source:\n",
    "        results.append({\"title\": title, \"concept\": concept, \"quote\": quote, \"source\": source})\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error parsing response: {e}\")\n",
    "\n",
    "# Print results\n",
    "for idx, res in enumerate(results):\n",
    "    print(f\"üìù Concept {idx+1}: {res['concept']}\")\n",
    "    print(f\"üìå Quote: {res['quote']}\")\n",
    "    print(f\"üìñ Source: {res['source']}\\n\")\n",
    "\n",
    "# Save the result to a test CSV\n",
    "test_df = pd.DataFrame(results)\n",
    "test_df.to_csv(\"test_concept_extraction_deepseek.csv\", index=False)\n",
    "\n",
    "print(\"üéâ Test complete! Results saved in test_concept_extraction_deepseek.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cd261f-b4ed-4190-9d18-e8cb9c11d5ce",
   "metadata": {},
   "source": [
    "With synonyms & glossary\n",
    "Still using concept_definitions.csv(should change to racism_types_definitions.xlsx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c845767-877a-4ae0-8513-dd41eac39b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Processing article 0 from file 0.txt...\n",
      "- Concept: Worry about safety  \n",
      "  Quote: ‚ÄúI'm scared. Everybody's scared,\" she said. ‚ÄúI hope that the coronavirus passes soon and fast, and Chinatown comes back.\"  \n",
      "  Source: concepts_definitions.csv  \n",
      "\n",
      "- Concept: Verbal harassment  \n",
      "  Quote: ‚ÄúAll the financial factors are compounded by the stirring racism,\" said Church, a longtime resident of Chinatown. ‚ÄúEverybody's concerned about the so-called 'Chinese Virus.'\"  \n",
      "  Source: concepts_definitions.csv  \n",
      "\n",
      "- Concept: Support Asian Americans  \n",
      "  Quote: ‚ÄúThere's been a long fight to keep Chinatown affordable and not displace the residents and the businesses that make the community special,\" said Wu.  \n",
      "  Source: concepts_definitions.csv  \n",
      "\n",
      "- Concept: Worry about safety  \n",
      "  Quote: ‚ÄúThis is going to be a long haul for everyone,\" said Boston City Councilor Michelle Wu. ‚ÄúBut there is especially pain in the Asian-American community in Boston and across the country.\"  \n",
      "  Source: concepts_definitions.csv  \n",
      "\n",
      "- Concept: Verbal harassment  \n",
      "  Quote: Now the rise in anti-Asian sentiment and xenophobic attacks has only bolstered the economic downturn plaguing the restaurants.  \n",
      "  Source: concepts_definitions.csv  \n",
      "\n",
      "- Concept: Support Asian Americans  \n",
      "  Quote: Chinatown Main Street organizers host virtual panels for restaurant owners and offer translated information on city, state, and federal assistance.  \n",
      "  Source: concepts_definitions.csv  \n",
      "\n",
      "- Concept: Worry about safety  \n",
      "  Quote: Burdened by stress, Huang worries about reopening China King once the virus subsides.  \n",
      "  Source: concepts_definitions.csv  \n",
      "\n",
      "- Concept: Verbal harassment  \n",
      "  Quote: The fact that the illness originated in China's Hubei Province scared potential customers, driving restaurant sales down by 50 percent or more.  \n",
      "  Source: concepts_definitions.csv\n",
      "‚úÖ Processed article 0: 3 concept(s) detected.\n",
      "üîÑ Processing article 1 from file 1.txt...\n",
      "- Concept: Online harassment  \n",
      "  Quote: \"On Wednesday morning, a Newton South High School advanced placement Chinese class was underway on Zoom when hackers infiltrated the group, posting 'vile, hate-filled images and speech' directed at the students, officials said.\"  \n",
      "  Source: concepts_definitions.csv  \n",
      "\n",
      "- Concept: Verbal harassment  \n",
      "  Quote: \"On Wednesday morning, a Newton South High School advanced placement Chinese class was underway on Zoom when hackers infiltrated the group, posting 'vile, hate-filled images and speech' directed at the students, officials said.\"  \n",
      "  Source: concepts_definitions.csv  \n",
      "\n",
      "- Concept: Worry about safety  \n",
      "  Quote: \"This was a traumatic experience for our teacher and students, and we will continue to offer support to everyone involved.\"  \n",
      "  Source: concepts_definitions.csv  \n",
      "\n",
      "- Concept: Support Asian Americans  \n",
      "  Quote: \"To be clear, an attack on members of our NSHS Asian-American community is an attack on all of us. We will be working on a plan for responding ‚Äî I will share more about this soon.\"  \n",
      "  Source: concepts_definitions.csv  \n",
      "\n",
      "- Concept: Videotaping/confronting harasser/attacker  \n",
      "  Quote: \"Teachers and students were working with officials to gather information about the intruders, who did not appear to be associated with the high school, according to Stembridge's e-mail.\"  \n",
      "  Source: concepts_definitions.csv  \n",
      "\n",
      "- Concept: Hate speech  \n",
      "  Quote: \"Hate speech and racism have no place at South and won't be tolerated.\"  \n",
      "  Source: 3. Anti-Hate Glossary.pdf  \n",
      "\n",
      "- Concept: Structural racism  \n",
      "  Quote: \"It was also an example, school officials said, of coronavirus-related racism that has run rampant since the beginning of the pandemic, often directed at the Asian community.\"  \n",
      "  Source: 4. structural_racism_and_health_equity_language_guide.pdf\n",
      "‚úÖ Processed article 1: 7 concept(s) detected.\n",
      "üîÑ Processing article 2 from file 2.txt...\n",
      "- Concept: Verbal harassment  \n",
      "  Quote: \"The coronavirus pandemic has unleashed an explosion of anti-Asian racism in the United States, ranging from derogatory slurs, to blaming the outbreak on Chinese people, to violent physical attacks.\"  \n",
      "  Source: concepts_definitions.csv  \n",
      "\n",
      "- Concept: Worry about safety  \n",
      "  Quote: \"Our mental health is deteriorating, and our grades will drop, impacting our futures in higher education. We feel unsafe returning to schools with unaddressed racism.\"  \n",
      "  Source: concepts_definitions.csv  \n",
      "\n",
      "- Concept: Online harassment  \n",
      "  Quote: \"Stop AAPI Hate, an initiative of several California-based Asian American civil rights organizations, has tracked more than 2,500 hate incidents against Asian Americans and Pacific Islanders between mid-March and early August, including reports of workplace discrimination, online harassment, and physical assaults.\"  \n",
      "  Source: concepts_definitions.csv  \n",
      "\n",
      "- Concept: Physical harassment  \n",
      "  Quote: \"The coronavirus pandemic has unleashed an explosion of anti-Asian racism in the United States, ranging from derogatory slurs, to blaming the outbreak on Chinese people, to violent physical attacks.\"  \n",
      "  Source: concepts_definitions.csv  \n",
      "\n",
      "- Concept: Attacked by white or not specified  \n",
      "  Quote: \"She recalled an incident in the spring, when a white student at a nearby suburban high school wrote a scathing Facebook post blaming the virus on Chinese people.\"  \n",
      "  Source: concepts_definitions.csv  \n",
      "\n",
      "- Concept: Support Asian Americans  \n",
      "  Quote: \"The students asked school leaders 'to stand in solidarity with communities of color' by publicly speaking out against racism; hiring more diverse faculty and staff; hosting monthly anti-bias trainings; and teaching Ethnic Studies.\"  \n",
      "  Source: concepts_definitions.csv  \n",
      "\n",
      "- Concept: Hiring security guards  \n",
      "  Quote: (Not directly mentioned, but inferred from broader support efforts)  \n",
      "  Source: concepts_definitions.csv  \n",
      "\n",
      "- Concept: Videotaping/confronting harasser/attacker  \n",
      "  Quote: (Not directly mentioned, but inferred from broader advocacy efforts)  \n",
      "  Source: concepts_definitions.csv  \n",
      "\n",
      "- Concept: Not confronting attacker/harasser or not reporting  \n",
      "  Quote: (Not directly mentioned, but inferred from the students' call for a reporting system)  \n",
      "  Source: concepts_definitions.csv  \n",
      "\n",
      "- Concept: Other minorities (Black) attack AA  \n",
      "  Quote: (Not mentioned in the article)  \n",
      "  Source: concepts_definitions.csv  \n",
      "\n",
      "- Concept: WWee BBeelloonngg (Glossary Term)  \n",
      "  Quote: (Not directly mentioned, but related to broader themes of belonging and inclusion)  \n",
      "  Source: 3. Anti-Hate Glossary.pdf  \n",
      "\n",
      "- Concept: Structural Racism and Health Equity (Glossary Term)  \n",
      "  Quote: \"I think when you're doing anti-racism, anti-bias [work], you're trying to undo generations and, frankly, centuries of systemic oppression.\"  \n",
      "  Source: 4. structural_racism_and_health_equity_language_guide.pdf\n",
      "‚úÖ Processed article 2: 17 concept(s) detected.\n",
      "üîÑ Processing article 3 from file 3.txt...\n",
      "- Concept: Verbal harassment  \n",
      "  Quote: \"In one local incident reported to the organization, an Asian-American shopper said a woman at a grocery store stated, 'You will infect us.'\"  \n",
      "  Source: concepts_definitions.csv  \n",
      "\n",
      "- Concept: Verbal harassment  \n",
      "  Quote: \"In another report from Massachusetts, a pedestrian said a driver screamed, 'You're the reason I have to wear a mask.'\"  \n",
      "  Source: concepts_definitions.csv  \n",
      "\n",
      "- Concept: Verbal harassment  \n",
      "  Quote: \"An elementary school teacher in Massachusetts reported that a student taunted a Chinese classmate by claiming he had COVID-19 and telling other children not to go near him.\"  \n",
      "  Source: concepts_definitions.csv  \n",
      "\n",
      "- Concept: Physical harassment  \n",
      "  Quote: \"Almost 90 percent of the cases from Massachusetts involved verbal harassment, about 7.5 percent were reports of people being coughed on or spat upon, and 3 percent were physical assaults...\"  \n",
      "  Source: concepts_definitions.csv  \n",
      "\n",
      "- Concept: Worry about safety  \n",
      "  Quote: \"Boston City Councilor Ed Flynn, who represents Chinatown, said some of the neighborhood's elderly residents worry about leaving their homes to buy food or get fresh air because they fear being harassed in public.\"  \n",
      "  Source: concepts_definitions.csv  \n",
      "\n",
      "- Concept: Support Asian Americans  \n",
      "  Quote: \"Flynn. 'They need to know that we're on their side and we're going to do everything we can to be helpful when there is discrimination and bullying and intimidation or name calling.'\"  \n",
      "  Source: concepts_definitions.csv  \n",
      "\n",
      "- Concept: Support Asian Americans  \n",
      "  Quote: \"One way some advocates are trying to fight anti-Asian bias is by encouraging bystanders to speak up when they witness wrongdoing.\"  \n",
      "  Source: concepts_definitions.csv  \n",
      "\n",
      "- Concept: Videotaping/confronting harasser/attacker  \n",
      "  Quote: \"Distrust of law enforcement, language barriers, and immigration status sometimes prevent victims from coming forward and that's why it's important for others to step up, they said.\"  \n",
      "  Source: concepts_definitions.csv  \n",
      "\n",
      "- Concept: Support Asian Americans  \n",
      "  Quote: \"President Biden addressed the racism, harassment, and hate crimes directed in a video marking the Lunar New Year.\"  \n",
      "  Source: concepts_definitions.csv  \n",
      "\n",
      "- Concept: Support Asian Americans  \n",
      "  Quote: \"Last month, Biden signed an executive order condemning anti-Asian racism and directed the Department of Justice to tackle the problem more aggressively.\"  \n",
      "  Source: concepts_definitions.csv  \n",
      "\n",
      "- Concept: Support Asian Americans  \n",
      "  Quote: \"Jessica 'Jay' Wong, interim executive director of the Asian American Commission, said her community needs to hear more messages of support from local leaders.\"  \n",
      "  Source: concepts_definitions.csv\n",
      "‚úÖ Processed article 3: 7 concept(s) detected.\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'test_concept_extraction_deepseek.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 222\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m all_results:\n\u001b[0;32m    221\u001b[0m     output_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(all_results)\n\u001b[1;32m--> 222\u001b[0m     output_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_concept_extraction_deepseek.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müéâ All articles processed! Results saved in test_concept_extraction_deepseek.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\System\\Anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\System\\Anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3965\u001b[0m )\n\u001b[1;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[38;5;241m.\u001b[39mto_csv(\n\u001b[0;32m   3968\u001b[0m     path_or_buf,\n\u001b[0;32m   3969\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   3970\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   3971\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   3972\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   3973\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   3974\u001b[0m     quoting\u001b[38;5;241m=\u001b[39mquoting,\n\u001b[0;32m   3975\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   3976\u001b[0m     index_label\u001b[38;5;241m=\u001b[39mindex_label,\n\u001b[0;32m   3977\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m   3978\u001b[0m     chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m   3979\u001b[0m     quotechar\u001b[38;5;241m=\u001b[39mquotechar,\n\u001b[0;32m   3980\u001b[0m     date_format\u001b[38;5;241m=\u001b[39mdate_format,\n\u001b[0;32m   3981\u001b[0m     doublequote\u001b[38;5;241m=\u001b[39mdoublequote,\n\u001b[0;32m   3982\u001b[0m     escapechar\u001b[38;5;241m=\u001b[39mescapechar,\n\u001b[0;32m   3983\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   3984\u001b[0m )\n",
      "File \u001b[1;32m~\\System\\Anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1013\u001b[0m )\n\u001b[1;32m-> 1014\u001b[0m csv_formatter\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32m~\\System\\Anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath_or_buffer,\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    254\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    255\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,\n\u001b[0;32m    256\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression,\n\u001b[0;32m    257\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_options,\n\u001b[0;32m    258\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    268\u001b[0m     )\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32m~\\System\\Anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'test_concept_extraction_deepseek.csv'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from docx import Document\n",
    "import pdfplumber\n",
    "\n",
    "# -----------------------\n",
    "# SETUP & UTILITY FUNCTIONS\n",
    "# -----------------------\n",
    "\n",
    "# Set DeepSeek API Key\n",
    "with open(\"DEEPSEEK_API_KEY.txt\", \"r\") as file:\n",
    "    deepseek_api_key = file.read().strip()\n",
    "\n",
    "DEEPSEEK_API_URL = \"https://api.deepseek.com/v1/chat/completions\"\n",
    "\n",
    "# Load Concept Definitions CSV (Manually defined concepts)\n",
    "concepts_df = pd.read_csv(\"concepts_definitions.csv\")\n",
    "# Build a dictionary mapping concept name to its definition.\n",
    "concept_definitions = {row[\"concepts\"]: row[\"definitions\"] for _, row in concepts_df.iterrows()}\n",
    "\n",
    "# Optionally, define synonyms mapping.\n",
    "# For example, if \"bigotry\" appears in an article, treat it as \"Bigotry/prejudice\".\n",
    "synonyms = {\n",
    "    \"bigotry\": \"Bigotry/prejudice\",\n",
    "    \"prejudice\": \"Bigotry/prejudice\",\n",
    "    \"Asian virus\": \"China/Chinese/Asian virus‚Äù/‚ÄúKung flu/plague/Ramen noodle flu\",\n",
    "    \"kung flu\": \"China/Chinese/Asian virus‚Äù/‚ÄúKung flu/plague/Ramen noodle flu\",\n",
    "    \"ramen noodle flu\" : \"China/Chinese/Asian virus‚Äù/‚ÄúKung flu/plague/Ramen noodle flu\",\n",
    "    \"racial injustice\": \"racial injustice/inequity and oppression\",\n",
    "    \"inequity and oppression\" :\"racial injustice/inequity and oppression\"\n",
    "\n",
    "}\n",
    "\n",
    "# Function to extract text from Word files\n",
    "def extract_text_from_word(word_path):\n",
    "    doc = Document(word_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "# Function to extract text from PDF files\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        return \"\\n\".join([page.extract_text() for page in pdf.pages if page.extract_text()])\n",
    "\n",
    "# Function to load glossary terms from files (Word and PDF)\n",
    "def load_glossary_from_folder(glossary_folder_path, max_entries=5):\n",
    "    glossary = {}\n",
    "    entries_added = 0\n",
    "    for file_name in os.listdir(glossary_folder_path):\n",
    "        if entries_added >= max_entries:\n",
    "            break  # Limit glossary size\n",
    "        file_path = os.path.join(glossary_folder_path, file_name)\n",
    "        if file_name.endswith(\".docx\"):\n",
    "            text = extract_text_from_word(file_path)\n",
    "        elif file_name.endswith(\".pdf\"):\n",
    "            text = extract_text_from_pdf(file_path)\n",
    "        else:\n",
    "            continue\n",
    "        glossary[file_name] = text[:2000]  # Limit text length per file\n",
    "        entries_added += 1\n",
    "    return glossary\n",
    "\n",
    "# Load limited glossary terms (max 5 entries)\n",
    "glossary_folder = \"glossary\"\n",
    "glossary = load_glossary_from_folder(glossary_folder)\n",
    "\n",
    "# Function to format concepts for DeepSeek API.\n",
    "def format_concepts_for_deepseek(max_manual=10, max_glossary=2):\n",
    "    manual_concepts = \"\\n\".join([\n",
    "        f\"{concept} (Definition: {definition}, Source: concepts_definitions.csv)\"\n",
    "        for concept, definition in list(concept_definitions.items())[:max_manual]\n",
    "    ])\n",
    "    glossary_concepts = \"\\n\\n\".join([\n",
    "        f\"{file_name} Terms: {text[:1000]} (Source: {file_name})\"\n",
    "        for file_name, text in list(glossary.items())[:max_glossary]\n",
    "    ])\n",
    "    return manual_concepts, glossary_concepts\n",
    "\n",
    "# -----------------------\n",
    "# DEEPSEEK CALL & EXACT MATCHING\n",
    "# -----------------------\n",
    "\n",
    "# Function to classify an article using DeepSeek and also perform exact word matching.\n",
    "def classify_article_with_deepseek(title, text):\n",
    "    results = []\n",
    "\n",
    "    # --- Step 1: Exact matching for each concept (from concepts_definitions.csv)\n",
    "    for concept in concept_definitions.keys():\n",
    "        # Check if the exact concept (as a whole word) appears in the text.\n",
    "        if re.search(rf'\\b{re.escape(concept)}\\b', text, flags=re.IGNORECASE):\n",
    "            results.append({\n",
    "                \"concept\": concept,\n",
    "                \"quote\": concept,  # Use the matched word as quote.\n",
    "                \"source\": \"concepts_definitions.csv\"\n",
    "            })\n",
    "    \n",
    "    # --- Step 2: Check synonyms.\n",
    "    for term, mapped_concept in synonyms.items():\n",
    "        if re.search(rf'\\b{re.escape(term)}\\b', text, flags=re.IGNORECASE):\n",
    "            # Add the mapped concept if not already added.\n",
    "            already_added = any(r[\"concept\"].lower() == mapped_concept.lower() for r in results)\n",
    "            if not already_added:\n",
    "                results.append({\n",
    "                    \"concept\": mapped_concept,\n",
    "                    \"quote\": term,  # Use the exact matching keyword.\n",
    "                    \"source\": \"concepts_definitions.csv\"\n",
    "                })\n",
    "\n",
    "    # --- Step 3: DeepSeek API call for additional concepts.\n",
    "    manual_concepts, glossary_concepts = format_concepts_for_deepseek()\n",
    "    prompt = f\"\"\"\n",
    "You are an expert in text analysis for racism-related themes. Below is a list of defined concepts and glossary keywords.\n",
    "\n",
    "Manual Concepts:\n",
    "{manual_concepts}\n",
    "\n",
    "Glossary Concepts:\n",
    "{glossary_concepts}\n",
    "\n",
    "Now, read the following article and do the following. There's no need to further response like \"Here are the relevant concepts detected in the article along with their corresponding quotes and sources::\n",
    "1. For each concept, search the article for any occurrence of its name or known synonyms. For example, if the concept \"Bigotry/prejudice\" is defined and the article contains the keyword \"bigotry\", then that concept should be detected.\n",
    "2. When a matching keyword or synonym is found, output the **entire sentence** that contains the match as the \"Quote\".\n",
    "3. Use the standardized concept label from the provided list (e.g. \"Bigotry/prejudice\") as the \"Concept\".\n",
    "4. If the concept is not directly mentioned but is inferred from the context, output the sentence that best represents it.\n",
    "5. Always include the source for each concept (e.g. \"concepts_definitions.csv\" or the corresponding glossary file name).\n",
    "6. There are many synonyms; if multiple variants are found in one sentence, still output that sentence once, with the standardized concept label.\n",
    "\n",
    "Article Title: {title}\n",
    "Article Text: {text}\n",
    "\n",
    "Provide the output in the following format:\n",
    "- Concept: [Detected Concept]\n",
    "  Quote: [Exact matching keyword or relevant excerpt]\n",
    "  Source: [Concept Source]\n",
    "Make sure to list every relevant concept separately.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {deepseek_api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": \"deepseek-coder\",  # Adjust model as needed.\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in text analysis.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"max_tokens\": 2000\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(DEEPSEEK_API_URL, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        ai_output = response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå DeepSeek API failed: {e}\")\n",
    "        ai_output = \"\"\n",
    "    print(ai_output)\n",
    "    # --- Step 4: Parse the AI output.\n",
    "    ai_matches = []\n",
    "    try:\n",
    "        lines = ai_output.split(\"\\n\")\n",
    "        concept, quote, source = None, None, None\n",
    "        for line in lines:\n",
    "            if line.startswith(\"- Concept:\"):\n",
    "                if concept and quote and source:\n",
    "                    ai_matches.append({\"concept\": concept, \"quote\": quote, \"source\": source})\n",
    "                concept = line.replace(\"- Concept:\", \"\").strip()\n",
    "                quote = None\n",
    "                source = None\n",
    "            elif line.startswith(\"  Quote:\"):\n",
    "                quote = line.replace(\"  Quote:\", \"\").strip()\n",
    "            elif line.startswith(\"  Source:\"):\n",
    "                source = line.replace(\"  Source:\", \"\").strip()\n",
    "        if concept and quote and source:\n",
    "            ai_matches.append({\"concept\": concept, \"quote\": quote, \"source\": source})\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error parsing AI output: {e}\")\n",
    "    \n",
    "    # --- Step 5: Merge the exact matches with AI matches, avoiding duplicates.\n",
    "    merged = {r[\"concept\"].lower(): r for r in results}  # Use lower-case keys.\n",
    "    for m in ai_matches:\n",
    "        key = m[\"concept\"].lower()\n",
    "        #if key not in merged:\n",
    "        merged[key] = m\n",
    "    return list(merged.values())\n",
    "\n",
    "# -----------------------\n",
    "# READ ARTICLES FROM TXT FILES\n",
    "# -----------------------\n",
    "\n",
    "# Assume the txt files are named \"0.txt\", \"1.txt\", ... in the folder \"txt\".\n",
    "txt_folder = \"txt\"\n",
    "article_files = sorted([f for f in os.listdir(txt_folder) if f.endswith(\".txt\")],\n",
    "                       key=lambda x: int(os.path.splitext(x)[0]))\n",
    "\n",
    "all_results = []\n",
    "# Process each txt file.\n",
    "for file_name in article_files:\n",
    "    file_path = os.path.join(txt_folder, file_name)\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        article_text = f.read()\n",
    "    # Use file name (without extension) as article title.\n",
    "    title = os.path.splitext(file_name)[0]\n",
    "    print(f\"üîÑ Processing article {title} from file {file_name}...\")\n",
    "    try:\n",
    "        detected_concepts = classify_article_with_deepseek(title, article_text)\n",
    "        # Add a title field to each result.\n",
    "        for item in detected_concepts:\n",
    "            item[\"title\"] = title\n",
    "        all_results.extend(detected_concepts)\n",
    "        print(f\"‚úÖ Processed article {title}: {len(detected_concepts)} concept(s) detected.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed processing article {title}: {e}\")\n",
    "\n",
    "# -----------------------\n",
    "# SAVE RESULTS TO CSV\n",
    "# -----------------------\n",
    "if all_results:\n",
    "    output_df = pd.DataFrame(all_results)\n",
    "    output_df.to_csv(\"test_concept_extraction_deepseek.csv\", index=False)\n",
    "    print(\"üéâ All articles processed! Results saved in test_concept_extraction_deepseek.csv\")\n",
    "else:\n",
    "    print(\"No results to save.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b69e9d4-2755-4728-bcb4-feb122891bee",
   "metadata": {},
   "source": [
    "Without Glossary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97633511-5e33-450c-a1bf-b691e48b95a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from docx import Document\n",
    "import pdfplumber\n",
    "\n",
    "# -----------------------\n",
    "# SETUP & UTILITY FUNCTIONS\n",
    "# -----------------------\n",
    "\n",
    "# Set DeepSeek API Key\n",
    "with open(\"DEEPSEEK_API_KEY.txt\", \"r\") as file:\n",
    "    deepseek_api_key = file.read().strip()\n",
    "\n",
    "DEEPSEEK_API_URL = \"https://api.deepseek.com/v1/chat/completions\"\n",
    "\n",
    "# Load Concept Definitions CSV (Manually defined concepts)\n",
    "concepts_df = pd.read_excel(\"racism_types_definitions.xlsx\")\n",
    "# Build a dictionary mapping concept name to its definition.\n",
    "concept_definitions = {row[\"concepts\"]: row[\"definitions\"] for _, row in concepts_df.iterrows()}\n",
    "\n",
    "# Optionally, define synonyms mapping.\n",
    "# For example, if \"bigotry\" appears in an article, treat it as \"Bigotry/prejudice\".\n",
    "synonyms = {\n",
    "    \"bigotry\": \"Bigotry/prejudice\",\n",
    "    \"prejudice\": \"Bigotry/prejudice\",\n",
    "    \"Asian virus\": \"China/Chinese/Asian virus‚Äù/‚ÄúKung flu/plague/Ramen noodle flu\",\n",
    "    \"kung flu\": \"China/Chinese/Asian virus‚Äù/‚ÄúKung flu/plague/Ramen noodle flu\",\n",
    "    \"ramen noodle flu\" : \"China/Chinese/Asian virus‚Äù/‚ÄúKung flu/plague/Ramen noodle flu\",\n",
    "    \"racial injustice\": \"racial injustice/inequity and oppression\",\n",
    "    \"inequity and oppression\" :\"racial injustice/inequity and oppression\"\n",
    "\n",
    "}\n",
    "# Function to format concepts for DeepSeek API.\n",
    "def format_concepts_for_deepseek(max_manual=10):\n",
    "    manual_concepts = \"\\n\".join([\n",
    "        f\"{concept} (Definition: {definition}, Source: racism_types_definitions.xlsx)\"\n",
    "        for concept, definition in list(concept_definitions.items())[:max_manual]\n",
    "    ])\n",
    "    return manual_concepts\n",
    "\n",
    "# -----------------------\n",
    "# DEEPSEEK CALL & EXACT MATCHING\n",
    "# -----------------------\n",
    "\n",
    "# Function to classify an article using DeepSeek and also perform exact word matching.\n",
    "def classify_article_with_deepseek(title, text):\n",
    "    results = []\n",
    "\n",
    "    # --- Step 1: Exact matching for each concept (from concepts_definitions.csv)\n",
    "    for concept in concept_definitions.keys():\n",
    "        # Check if the exact concept (as a whole word) appears in the text.\n",
    "        if re.search(rf'\\b{re.escape(concept)}\\b', text, flags=re.IGNORECASE):\n",
    "            results.append({\n",
    "                \"concept\": concept,\n",
    "                \"quote\": concept,  # Use the matched word as quote.\n",
    "                \"source\": \"racism_types_definitions.xlsx\"\n",
    "            })\n",
    "    \n",
    "    # --- Step 2: Check synonyms.\n",
    "    for term, mapped_concept in synonyms.items():\n",
    "        if re.search(rf'\\b{re.escape(term)}\\b', text, flags=re.IGNORECASE):\n",
    "            # Add the mapped concept if not already added.\n",
    "            already_added = any(r[\"concept\"].lower() == mapped_concept.lower() for r in results)\n",
    "            if not already_added:\n",
    "                results.append({\n",
    "                    \"concept\": mapped_concept,\n",
    "                    \"quote\": term,  # Use the exact matching keyword.\n",
    "                    \"source\": \"racism_types_definitions.xlsx\"\n",
    "                })\n",
    "\n",
    "    # --- Step 3: DeepSeek API call for additional concepts.\n",
    "    manual_concepts = format_concepts_for_deepseek()\n",
    "    prompt = f\"\"\"\n",
    "You are an expert in text analysis for racism-related themes. Below is a list of defined racism types with their definitions.\n",
    "\n",
    "Defined Racism Types:\n",
    "{manual_concepts}\n",
    "\n",
    "Now, read the following article and identify instances of these racism types. For each detected racism type, output an object with the following keys:\n",
    "- \"Concept\": standardized racism type label from the provided list.\n",
    "- \"Quote\": the entire sentence from the article where the racism type or its known synonym appears.\n",
    "- \"Source\": \"racism_types_definitions.xlsx\".\n",
    "\n",
    "\n",
    "Article Title: {title}\n",
    "Article Text: {text}\n",
    "\n",
    "Provide the output in the following format,There's no need to further response like \"Here are the relevant concepts detected in the article along with their corresponding quotes and sources:\n",
    "- Concept: [Detected Concept]\n",
    "  Quote: [Exact matching keyword or relevant excerpt]\n",
    "  Source: [Concept Source]\n",
    "Make sure to list every relevant concept separately.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {deepseek_api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": \"deepseek-coder\",  # Adjust model as needed.\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in text analysis.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"max_tokens\": 2000\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(DEEPSEEK_API_URL, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        ai_output = response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå DeepSeek API failed: {e}\")\n",
    "        ai_output = \"\"\n",
    "    print(ai_output)\n",
    "    # --- Step 4: Parse the AI output.\n",
    "    ai_matches = []\n",
    "    try:\n",
    "        lines = ai_output.split(\"\\n\")\n",
    "        concept, quote, source = None, None, None\n",
    "        for line in lines:\n",
    "            if line.startswith(\"- Concept:\"):\n",
    "                if concept and quote and source:\n",
    "                    ai_matches.append({\"concept\": concept, \"quote\": quote, \"source\": source})\n",
    "                concept = line.replace(\"- Concept:\", \"\").strip()\n",
    "                quote = None\n",
    "                source = None\n",
    "            elif line.startswith(\"  Quote:\"):\n",
    "                quote = line.replace(\"  Quote:\", \"\").strip()\n",
    "            elif line.startswith(\"  Source:\"):\n",
    "                source = line.replace(\"  Source:\", \"\").strip()\n",
    "        if concept and quote and source:\n",
    "            ai_matches.append({\"concept\": concept, \"quote\": quote, \"source\": source})\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error parsing AI output: {e}\")\n",
    "    \n",
    "    # --- Step 5: Merge the exact matches with AI matches, avoiding duplicates.\n",
    "    merged = {r[\"concept\"].lower(): r for r in results}  # Use lower-case keys.\n",
    "    for m in ai_matches:\n",
    "        key = m[\"concept\"].lower()\n",
    "        #if key not in merged:\n",
    "        merged[key] = m\n",
    "    return list(merged.values())\n",
    "\n",
    "# -----------------------\n",
    "# READ ARTICLES FROM TXT FILES\n",
    "# -----------------------\n",
    "\n",
    "# Assume the txt files are named \"0.txt\", \"1.txt\", ... in the folder \"txt\".\n",
    "txt_folder = \"txt\"\n",
    "article_files = sorted([f for f in os.listdir(txt_folder) if f.endswith(\".txt\")],\n",
    "                       key=lambda x: int(os.path.splitext(x)[0]))\n",
    "\n",
    "all_results = []\n",
    "# Process each txt file.\n",
    "for file_name in article_files:\n",
    "    file_path = os.path.join(txt_folder, file_name)\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        article_text = f.read()\n",
    "    # Use file name (without extension) as article title.\n",
    "    title = os.path.splitext(file_name)[0]\n",
    "    print(f\"üîÑ Processing article {title} from file {file_name}...\")\n",
    "    try:\n",
    "        detected_concepts = classify_article_with_deepseek(title, article_text)\n",
    "        # Add a title field to each result.\n",
    "        for item in detected_concepts:\n",
    "            item[\"title\"] = title\n",
    "        all_results.extend(detected_concepts)\n",
    "        print(f\"‚úÖ Processed article {title}: {len(detected_concepts)} concept(s) detected.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed processing article {title}: {e}\")\n",
    "\n",
    "# -----------------------\n",
    "# SAVE RESULTS TO CSV\n",
    "# -----------------------\n",
    "if all_results:\n",
    "    output_df = pd.DataFrame(all_results)\n",
    "    output_df.to_csv(\"test_concept_extraction_deepseek.csv\", index=False)\n",
    "    print(\"üéâ All articles processed! Results saved in test_concept_extraction_deepseek.csv\")\n",
    "else:\n",
    "    print(\"No results to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfe3c1e-facd-4c50-8736-2e8972a353d5",
   "metadata": {},
   "source": [
    "Whole article passed into prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9d87a714-1131-4454-8670-ff4208f9be8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [01:01<00:00, 30.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è JSON error for article 2: Unterminated string starting at: line 13 column 4 (char 2426)\n",
      "üîç Model output:\n",
      " [\n",
      "  {\"quote\": \"F--- China!\", \"concepts\": [\"Verbal harassment\"]},\n",
      "  {\"quote\": \"Then the man spat on Nguyen, he said. The saliva splattered on his jacket.\", \"concepts\": [\"Physical harassment\"]},\n",
      "  {\"quote\": \"Nguyen worries that East Asians in the United States will face even more harassment and attacks as coronavirus cases continue to rise.\", \"concepts\": [\"Anti-Asian hate crimes(general)\", \"COVID-19 or coronavirus or pandemic\"]},\n",
      "  {\"quote\": \"He said he believes President Donald Trump stoked such hate-filled reaction during a news briefing last week when he defended his use of 'Chinese virus.'\", \"concepts\": [\"Donald Trump\", \"‚ÄúChina/Chinese virus‚Äù or ‚ÄúKung flu/plague‚Äù or ‚ÄúWuhan virus‚Äù or ‚ÄúDiseased Chinese‚Äù or ‚ÄúAsian Virus‚Äù or ‚ÄúRamen Noodle flu‚Äù\"]},\n",
      "  {\"quote\": \"Nguyen and other Asians in Chicago said they have felt growing apprehension that people take the president's comments as a license for racism.\", \"concepts\": [\"Donald Trump\", \"Racism (general)\"]},\n",
      "  {\"quote\": \"Elsewhere in the U.S., reports of hate crimes have cropped up from New York to San Francisco, some caught on video and circulated through social media.\", \"concepts\": [\"Anti-Asian hate crimes(general)\"]},\n",
      "  {\"quote\": \"Chinese Americans and constituents of mine understand this is a situation in which they could potentially be scapegoats for the uncertainty people feel.\", \"concepts\": [\"Scapegoat\"]},\n",
      "  {\"quote\": \"A man, very intoxicated, made eye contact with her and said, 'Do you have the corona?'\", \"concepts\": [\"Verbal harassment\"]},\n",
      "  {\"quote\": \"Tuyet Anh, who is of Vietnamese descent, has since been saddened to read social media comments from her fellow classmates using the terms 'Chinese virus' and 'Wu flu.'\", \"concepts\": [\"Online harassment\", \"‚ÄúChina/Chinese virus‚Äù or ‚ÄúKung flu/plague‚Äù or ‚ÄúWuhan virus‚Äù or ‚ÄúDiseased Chinese‚Äù or ‚ÄúAsian Virus‚Äù or ‚ÄúRamen Noodle flu‚Äù\"]},\n",
      "  {\"quote\": \"We are labeled and demonized as this threat to white American safety.\", \"concepts\": [\"Otherized (or othering)\"]},\n",
      "  {\"quote\": \"Menard, president of Chicago's chapter of the advocacy group OCA-Asian Pacific American Advocates, said the tense environment reminds her of the 1882 Chinese Exclusion Act barring Chinese immigrants from entering the country, the first immigration law to exclude an ethnic group, as well as the World War II Japanese internment camps forcing Japanese Americans into incarceration.\", \"concepts\": [\"Reconstructive history\"]},\n",
      "  {\"\n",
      "‚úÖ Done! Saved to classification_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "article_num = 2\n",
    "\n",
    "# Load API key from .env file\n",
    "load_dotenv()\n",
    "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Load your files\n",
    "articles_df = pd.read_csv(\"articles.csv\").head(article_num)\n",
    "definitions_df = pd.read_excel(\"racism_types_definitions.xlsx\")\n",
    "samples_df = pd.read_excel(\"sample_racism_classification.xlsx\").head(140)\n",
    "\n",
    "concept_defs = \"\\n\".join(\n",
    "    f\"{row['concepts']}: {row['definitions'][:300]}\"\n",
    "    for _, row in definitions_df.iterrows()\n",
    ")\n",
    "\n",
    "examples = \"\\n\".join(\n",
    "    f'\"{row[\"annotated_sentence\"]}\" ‚Üí {row[\"annotation_content\"]}'\n",
    "    for _, row in samples_df.iterrows()\n",
    ")\n",
    "\n",
    "# Prompt builder\n",
    "def build_full_article_prompt(article_text, concept_defs, examples):\n",
    "    return f\"\"\"\n",
    "You are a sociology professor with 30 years of experience analyzing the effects and causes of Asian racism.\n",
    "Your task is to identify the quotes in articles that match your list of types of racism concepts.\n",
    "\n",
    "First, read through the racism concept definitions:\n",
    "{concept_defs}\n",
    "\n",
    "Next, read through some example labeled quotes:\n",
    "{examples}\n",
    "\n",
    "Now, read the article below. For each quote that matches a concept, return:\n",
    "- The quote (exact text from article)\n",
    "- The matched concept(s)\n",
    "\n",
    "ARTICLE:\n",
    "{article_text}\n",
    "\n",
    "Return a list of quote/concept pairs in this format:\n",
    "[\n",
    "  {{\"quote\": \"...\", \"concepts\": [\"concept1\", \"concept2\"]}},\n",
    "  ...\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "# Output container\n",
    "all_results = []\n",
    "\n",
    "# Loop through a few articles (start small to avoid token overload)\n",
    "for idx, row in tqdm(articles_df.iterrows(), total=article_num):\n",
    "    article_text = row[\"ARTICLE_TEXT\"]\n",
    "    article_id = row[\"id\"]\n",
    "    title = row[\"title\"]\n",
    "\n",
    "    prompt = build_full_article_prompt(article_text, concept_defs, examples)\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a sociology professor analyzing racism in text. Label quotes using provided concepts.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.2\n",
    "        )\n",
    "\n",
    "        output = response.choices[0].message.content\n",
    "\n",
    "        # Parse the model's JSON response\n",
    "        try:\n",
    "            quote_labels = json.loads(output)\n",
    "            for q in quote_labels:\n",
    "                for concept in q[\"concepts\"]:\n",
    "                    all_results.append({\n",
    "                        \"article_id\": article_id,\n",
    "                        \"title\": title,\n",
    "                        \"quote\": q[\"quote\"],\n",
    "                        \"concept\": concept\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è JSON error for article {article_id}: {e}\")\n",
    "            print(\"üîç Model output:\\n\", output)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå API error for article {article_id}: {e}\")\n",
    "\n",
    "# Save results\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df.to_csv(\"classification_results.csv\", index=False)\n",
    "print(\"‚úÖ Done! Saved to classification_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30520f4b-f700-4c01-aecc-9eb3a615fe47",
   "metadata": {},
   "source": [
    "Divide articles into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d774412a-d843-4cab-b696-c7bdd34ccda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "article_num = 2\n",
    "\n",
    "# Load API key from .env file\n",
    "load_dotenv()\n",
    "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Load your files\n",
    "articles_df = pd.read_csv(\"articles.csv\").head(article_num)\n",
    "definitions_df = pd.read_excel(\"racism_types_definitions.xlsx\")\n",
    "samples_df = pd.read_excel(\"sample_racism_classification.xlsx\").head(150)\n",
    "\n",
    "concept_defs = \"\\n\".join(\n",
    "    f\"{row['concepts']}: {row['definitions'][:300]}\"\n",
    "    for _, row in definitions_df.iterrows()\n",
    ")\n",
    "\n",
    "examples = \"\\n\".join(\n",
    "    f'\"{row[\"annotated_sentence\"]}\" ‚Üí {row[\"annotation_content\"]}'\n",
    "    for _, row in samples_df.iterrows()\n",
    ")\n",
    "def split_text(text, max_chars=3000):\n",
    "    \"\"\"Splits text into chunks of approximately max_chars, preferably at sentence boundaries.\"\"\"\n",
    "    sentences = text.split('. ')\n",
    "    chunks = []\n",
    "    current_chunk = ''\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) < max_chars:\n",
    "            current_chunk += sentence + '. '\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence + '. '\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    return chunks\n",
    "\n",
    "# Prompt builder\n",
    "def build_full_article_prompt(article_text, concept_defs, examples):\n",
    "    return f\"\"\"\n",
    "You are a sociology professor with 30 years of experience analyzing the effects and causes of Asian racism.\n",
    "Your task is to identify the quotes in articles that match your list of types of racism concepts.\n",
    "\n",
    "First, read through the racism concept definitions. \n",
    "You need to understand these definitions so you can accurately recognize when a quote fits one or more of these concepts:\n",
    "{concept_defs}\n",
    "\n",
    "Next, review the example labeled quotes provided. \n",
    "You need to study these examples to see how quotes have been matched to concepts in practice, which will guide your own labeling decisions:\n",
    "{examples}\n",
    "\n",
    "Now, read the article below. For each quote that matches a concept, return:\n",
    "- The quote (exact text from article)\n",
    "- The matched concept(s)\n",
    "\n",
    "ARTICLE:\n",
    "{article_text}\n",
    "\n",
    "Return a list of quote/concept pairs in this format:\n",
    "[\n",
    "  {{\"quote\": \"...\", \"concepts\": [\"concept1\", \"concept2\"]}},\n",
    "  ...\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "# Output container\n",
    "all_results = []\n",
    "\n",
    "# Loop through a few articles (start small to avoid token overload)\n",
    "for idx, row in tqdm(articles_df.iterrows(), total=article_num):\n",
    "    article_text = row[\"ARTICLE_TEXT\"]\n",
    "    article_id = row[\"id\"]\n",
    "    title = row[\"title\"]\n",
    "\n",
    "    # Split into chunks\n",
    "    chunks = split_text(article_text)\n",
    "\n",
    "    for chunk_idx, chunk_text in enumerate(chunks):\n",
    "        prompt = build_full_article_prompt(chunk_text, concept_defs, examples)\n",
    "\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a sociology professor analyzing racism in text. Label quotes using provided concepts.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.2\n",
    "            )\n",
    "\n",
    "            output = response.choices[0].message.content\n",
    "\n",
    "            # Parse JSON output\n",
    "            try:\n",
    "                quote_labels = json.loads(output)\n",
    "                for q in quote_labels:\n",
    "                    for concept in q[\"concepts\"]:\n",
    "                        all_results.append({\n",
    "                            \"article_id\": article_id,\n",
    "                            \"title\": title,\n",
    "                            \"quote\": q[\"quote\"],\n",
    "                            \"concept\": concept\n",
    "                        })\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è JSON error for article {article_id} chunk {chunk_idx}: {e}\")\n",
    "                print(\"üîç Model output:\\n\", output)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå API error for article {article_id} chunk {chunk_idx}: {e}\")\n",
    "\n",
    "# Save results\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df.to_csv(\"classification_results.csv\", index=False)\n",
    "print(\"‚úÖ Done! Saved to classification_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
