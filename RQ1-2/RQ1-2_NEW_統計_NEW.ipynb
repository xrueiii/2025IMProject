{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607ee0b8-a95f-4482-8cbd-86ba0803c0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "import numpy as np\n",
    "\n",
    "# è¨­å®šåˆ†ç•Œæ—¥\n",
    "split_date = \"2021-03-16\"\n",
    "\n",
    "def get_before_after_counts(file_path):\n",
    "    \"\"\"è®€å–æª”æ¡ˆï¼Œåˆ‡åˆ† before/afterï¼Œå›å‚³ entity_type çµ±è¨ˆ\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    # åˆ‡åˆ†\n",
    "    before_df = df[df['date'] < split_date]\n",
    "    after_df = df[df['date'] >= split_date]\n",
    "\n",
    "    # éæ¿¾æ‰ç„¡æ•ˆçš„ entity_type\n",
    "    exclude_types = ['Cannot be inferred', 'unknown', 'not applicable']\n",
    "    before_df = before_df[~before_df['entity_type'].isin(exclude_types)]\n",
    "    after_df = after_df[~after_df['entity_type'].isin(exclude_types)]\n",
    "\n",
    "    # è¨ˆç®— counts\n",
    "    before_counts = before_df['entity_type'].value_counts()\n",
    "    after_counts = after_df['entity_type'].value_counts()\n",
    "\n",
    "    # åˆä½µæˆ DataFrame\n",
    "    comparison = pd.DataFrame({\n",
    "        'Before': before_counts,\n",
    "        'After': after_counts\n",
    "    }).fillna(0).astype(int)\n",
    "\n",
    "    # åŠ ä¸Šç¸½å’Œ\n",
    "    comparison['Total'] = comparison['Before'] + comparison['After']\n",
    "\n",
    "    # æ’åºï¼ˆä¾ç…§ Total ç”±å¤§åˆ°å°ï¼‰\n",
    "    comparison = comparison.sort_values(by='Total', ascending=False)\n",
    "\n",
    "    return comparison\n",
    "\n",
    "def chi_square_with_residuals(comparison_df):\n",
    "    \"\"\"å¡æ–¹æª¢å®š + æ¨™æº–åŒ–æ®˜å·®\"\"\"\n",
    "    contingency_table = comparison_df[['Before', 'After']].T.values\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "    # æ¨™æº–åŒ–æ®˜å·®\n",
    "    residuals = (contingency_table - expected) / np.sqrt(expected)\n",
    "    residuals_df = pd.DataFrame(\n",
    "        residuals.T,\n",
    "        index=comparison_df.index,\n",
    "        columns=['Before_resid', 'After_resid']\n",
    "    )\n",
    "\n",
    "    return chi2, p, dof, residuals_df\n",
    "\n",
    "# åˆ†åˆ¥è·‘ step3 å’Œ step4\n",
    "comparison_step3 = get_before_after_counts(\"step3_all_new.csv\")\n",
    "comparison_step4 = get_before_after_counts(\"step4_all_with_date.csv\")\n",
    "\n",
    "print(\"=== Step3 çµ±è¨ˆçµæœ ===\")\n",
    "print(comparison_step3)\n",
    "chi2, p, dof, residuals_df = chi_square_with_residuals(comparison_step3)\n",
    "print(f\"\\n[Step3] Chi-square = {chi2:.2f}, df = {dof}, p-value = {p:.4f}\")\n",
    "print(\"\\n[Step3] æ¨™æº–åŒ–æ®˜å·®ï¼š\")\n",
    "print(residuals_df)\n",
    "\n",
    "print(\"\\n=== Step4 çµ±è¨ˆçµæœ ===\")\n",
    "print(comparison_step4)\n",
    "chi2, p, dof, residuals_df = chi_square_with_residuals(comparison_step4)\n",
    "print(f\"\\n[Step4] Chi-square = {chi2:.2f}, df = {dof}, p-value = {p:.4f}\")\n",
    "print(\"\\n[Step4] æ¨™æº–åŒ–æ®˜å·®ï¼š\")\n",
    "print(residuals_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681fc50e-1caf-4102-9146-2d89d3f33726",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === 1. è®€å–è³‡æ–™ ===\n",
    "df = pd.read_csv(\"step3_all_new.csv\")\n",
    "\n",
    "# ç¢ºèªæ¬„ä½åç¨±æ­£ç¢º\n",
    "print(\"ğŸ‘‰ æ¬„ä½åç¨±ï¼š\", df.columns.tolist())\n",
    "\n",
    "# === 2. æ—¥æœŸè½‰æ› ===\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "# === 3. è¨­å®šåˆ†ç•Œæ—¥ ===\n",
    "cutoff_date = pd.to_datetime(\"2021-03-16\")\n",
    "\n",
    "before_df = df[df['date'] < cutoff_date]\n",
    "after_df = df[df['date'] >= cutoff_date]\n",
    "\n",
    "# === 4. æ’é™¤ç„¡æ•ˆ reaction ===\n",
    "valid_reactions = ['Cannot be inferred', 'unknown']\n",
    "before_df_filtered = before_df[~before_df['reaction'].isin(valid_reactions)]\n",
    "after_df_filtered = after_df[~after_df['reaction'].isin(valid_reactions)]\n",
    "\n",
    "# === 5. è¨ˆç®— reaction å‡ºç¾æ¬¡æ•¸ ===\n",
    "before_counts = before_df_filtered['reaction'].value_counts()\n",
    "after_counts = after_df_filtered['reaction'].value_counts()\n",
    "\n",
    "# === 6. å»ºç«‹æ¯”è¼ƒè¡¨ ===\n",
    "all_reactions = sorted(set(before_counts.index).union(set(after_counts.index)))\n",
    "reaction_comparison = pd.DataFrame(index=all_reactions)\n",
    "reaction_comparison['Before'] = before_counts\n",
    "reaction_comparison['After'] = after_counts\n",
    "reaction_comparison = reaction_comparison.fillna(0).astype(int)\n",
    "\n",
    "# === 7. åŠ å…¥ Total èˆ‡ç™¾åˆ†æ¯” ===\n",
    "reaction_comparison['Total'] = reaction_comparison['Before'] + reaction_comparison['After']\n",
    "\n",
    "total_before = reaction_comparison['Before'].sum()\n",
    "total_after = reaction_comparison['After'].sum()\n",
    "\n",
    "reaction_comparison['Before(%)'] = (reaction_comparison['Before'] / total_before * 100).round(2) if total_before > 0 else 0\n",
    "reaction_comparison['After(%)'] = (reaction_comparison['After'] / total_after * 100).round(2) if total_after > 0 else 0\n",
    "\n",
    "# === 8. æ’åº ===\n",
    "reaction_comparison = reaction_comparison.sort_values(by='Total', ascending=False)\n",
    "\n",
    "# === 9. è¼¸å‡ºçµæœ ===\n",
    "pd.set_option(\"display.width\", 200)  # é è¨­ 80\n",
    "pd.set_option(\"display.max_columns\", None)  # é¡¯ç¤ºæ‰€æœ‰æ¬„ä½\n",
    "pd.set_option(\"display.max_colwidth\", None)  # ä¸æˆªæ–·æ–‡å­—\n",
    "print(\"ğŸ“Š Reaction æ•¸é‡èˆ‡ç™¾åˆ†æ¯”æ¯”è¼ƒï¼š\")\n",
    "print(reaction_comparison)\n",
    "\n",
    "# å¦‚æœè¦å­˜æˆ CSV\n",
    "# reaction_comparison.to_csv(\"reaction_comparison.csv\", encoding=\"utf-8-sig\")\n",
    "\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "# === 10. Wilcoxon æª¢å®š ===\n",
    "before_vals = reaction_comparison[\"Before(%)\"]\n",
    "after_vals = reaction_comparison[\"After(%)\"]\n",
    "\n",
    "# ç¢ºä¿ä¸æ˜¯å®Œå…¨ä¸€æ¨£çš„æ•¸æ“šï¼Œå¦å‰‡ wilcoxon æœƒå ±éŒ¯\n",
    "if (before_vals != after_vals).any():\n",
    "    stat, p = wilcoxon(after_vals, before_vals)\n",
    "    print(\"\\nğŸ“Š Wilcoxon ç¬¦è™Ÿç­‰ç´šæª¢å®šçµæœ\")\n",
    "    print(\"Statistic =\", stat, \"  p-value =\", p)\n",
    "    if p < 0.05:\n",
    "        print(\"â¡ï¸ çµè«–ï¼šäº‹ä»¶å‰å¾Œæ•´é«”åæ‡‰åˆ†å¸ƒæœ‰é¡¯è‘—å·®ç•°\")\n",
    "    else:\n",
    "        print(\"â¡ï¸ çµè«–ï¼šäº‹ä»¶å‰å¾Œæ•´é«”åæ‡‰åˆ†å¸ƒæ²’æœ‰é¡¯è‘—å·®ç•°\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Before% å’Œ After% å®Œå…¨ç›¸åŒï¼Œç„¡æ³•é€²è¡Œ Wilcoxon æª¢å®š\")\n",
    "\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "import numpy as np\n",
    "\n",
    "# === 11. å¡æ–¹æª¢å®š ===\n",
    "contingency_table = reaction_comparison[['Before', 'After']].T.values\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "print(\"\\nğŸ“Š å¡æ–¹æª¢å®šçµæœ\")\n",
    "print(f\"Chi-square = {chi2:.2f}, df = {dof}, p-value = {p:.4f}\")\n",
    "if p < 0.05:\n",
    "    print(\"â¡ï¸ çµè«–ï¼šäº‹ä»¶å‰å¾Œåæ‡‰åˆ†å¸ƒæœ‰é¡¯è‘—å·®ç•°\")\n",
    "else:\n",
    "    print(\"â¡ï¸ çµè«–ï¼šäº‹ä»¶å‰å¾Œåæ‡‰åˆ†å¸ƒæ²’æœ‰é¡¯è‘—å·®ç•°\")\n",
    "\n",
    "# === 12. è¨ˆç®—æ¨™æº–åŒ–æ®˜å·® ===\n",
    "residuals = (contingency_table - expected) / np.sqrt(expected)\n",
    "residuals_df = pd.DataFrame(\n",
    "    residuals.T,\n",
    "    index=reaction_comparison.index,\n",
    "    columns=['Before_resid', 'After_resid']\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ“Š å„åæ‡‰é¡åˆ¥çš„æ¨™æº–åŒ–æ®˜å·®ï¼š\")\n",
    "print(residuals_df)\n",
    "\n",
    "# å¦‚æœè¦ä¸€èµ·å­˜æˆ CSV\n",
    "# result_with_resid = reaction_comparison.join(residuals_df)\n",
    "# result_with_resid.to_csv(\"reaction_comparison_with_resid.csv\", encoding=\"utf-8-sig\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc56d640-78e1-412f-bc93-238b5b29163d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import wilcoxon, chi2_contingency\n",
    "import numpy as np\n",
    "\n",
    "# === 1. è®€å–è³‡æ–™ ===\n",
    "df = pd.read_csv(\"step4_all_with_date.csv\")\n",
    "\n",
    "# ç¢ºèªæ¬„ä½\n",
    "print(\"ğŸ‘‰ æ¬„ä½åç¨±ï¼š\", df.columns.tolist())\n",
    "\n",
    "# === 2. æ—¥æœŸè½‰æ› ===\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "# === 3. è¨­å®šåˆ†ç•Œæ—¥ ===\n",
    "cutoff_date = pd.to_datetime(\"2021-03-16\")\n",
    "before_df = df[df['date'] < cutoff_date]\n",
    "after_df = df[df['date'] >= cutoff_date]\n",
    "\n",
    "# === 4. æ’é™¤ç„¡æ•ˆ emotions ===\n",
    "valid_emotions = ['cannot be inferred', 'unknown']\n",
    "before_df_filtered = before_df[~before_df['emotion'].isin(valid_emotions)]\n",
    "after_df_filtered = after_df[~after_df['emotion'].isin(valid_emotions)]\n",
    "\n",
    "# === 5. è¨ˆç®— emotion å‡ºç¾æ¬¡æ•¸ ===\n",
    "before_counts = before_df_filtered['emotion'].value_counts()\n",
    "after_counts = after_df_filtered['emotion'].value_counts()\n",
    "\n",
    "# === 6. å»ºç«‹æ¯”è¼ƒè¡¨ ===\n",
    "all_emotions = sorted(set(before_counts.index).union(set(after_counts.index)))\n",
    "emotion_comparison = pd.DataFrame(index=all_emotions)\n",
    "emotion_comparison['Before'] = before_counts\n",
    "emotion_comparison['After'] = after_counts\n",
    "emotion_comparison = emotion_comparison.fillna(0).astype(int)\n",
    "\n",
    "# === 7. åŠ å…¥ Total èˆ‡ç™¾åˆ†æ¯” ===\n",
    "emotion_comparison['Total'] = emotion_comparison['Before'] + emotion_comparison['After']\n",
    "\n",
    "total_before = emotion_comparison['Before'].sum()\n",
    "total_after = emotion_comparison['After'].sum()\n",
    "\n",
    "emotion_comparison['Before(%)'] = (emotion_comparison['Before'] / total_before * 100).round(2) if total_before > 0 else 0\n",
    "emotion_comparison['After(%)'] = (emotion_comparison['After'] / total_after * 100).round(2) if total_after > 0 else 0\n",
    "\n",
    "# === 8. æ’åº ===\n",
    "emotion_comparison = emotion_comparison.sort_values(by='Total', ascending=False)\n",
    "\n",
    "# === 9. é¡¯ç¤º ===\n",
    "pd.set_option(\"display.width\", 200)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "print(\"ğŸ“Š Emotion æ•¸é‡èˆ‡ç™¾åˆ†æ¯”æ¯”è¼ƒï¼š\")\n",
    "print(emotion_comparison)\n",
    "\n",
    "# === 10. Wilcoxon æª¢å®š ===\n",
    "before_vals = emotion_comparison[\"Before(%)\"]\n",
    "after_vals = emotion_comparison[\"After(%)\"]\n",
    "\n",
    "if (before_vals != after_vals).any():\n",
    "    stat, p = wilcoxon(after_vals, before_vals)\n",
    "    print(\"\\nğŸ“Š Wilcoxon ç¬¦è™Ÿç­‰ç´šæª¢å®šçµæœ\")\n",
    "    print(\"Statistic =\", stat, \"  p-value =\", p)\n",
    "    if p < 0.05:\n",
    "        print(\"â¡ï¸ çµè«–ï¼šäº‹ä»¶å‰å¾Œæ•´é«”æƒ…ç·’åˆ†å¸ƒæœ‰é¡¯è‘—å·®ç•°\")\n",
    "    else:\n",
    "        print(\"â¡ï¸ çµè«–ï¼šäº‹ä»¶å‰å¾Œæ•´é«”æƒ…ç·’åˆ†å¸ƒæ²’æœ‰é¡¯è‘—å·®ç•°\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Before% å’Œ After% å®Œå…¨ç›¸åŒï¼Œç„¡æ³•é€²è¡Œ Wilcoxon æª¢å®š\")\n",
    "\n",
    "# === 11. å¡æ–¹æª¢å®š ===\n",
    "contingency_table = emotion_comparison[['Before', 'After']].T.values\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "print(\"\\nğŸ“Š å¡æ–¹æª¢å®šçµæœ\")\n",
    "print(f\"Chi-square = {chi2:.2f}, df = {dof}, p-value = {p:.4f}\")\n",
    "if p < 0.05:\n",
    "    print(\"â¡ï¸ çµè«–ï¼šäº‹ä»¶å‰å¾Œæƒ…ç·’åˆ†å¸ƒæœ‰é¡¯è‘—å·®ç•°\")\n",
    "else:\n",
    "    print(\"â¡ï¸ çµè«–ï¼šäº‹ä»¶å‰å¾Œæƒ…ç·’åˆ†å¸ƒæ²’æœ‰é¡¯è‘—å·®ç•°\")\n",
    "\n",
    "# === 12. æ¨™æº–åŒ–æ®˜å·® ===\n",
    "residuals = (contingency_table - expected) / np.sqrt(expected)\n",
    "residuals_df = pd.DataFrame(\n",
    "    residuals.T,\n",
    "    index=emotion_comparison.index,\n",
    "    columns=['Before_resid', 'After_resid']\n",
    ")\n",
    "\n",
    "# ä¹Ÿå¯ä»¥åŠ ä¸Šæ˜¯å¦é¡¯è‘—æ¨™è¨˜\n",
    "residuals_df['Before_sig'] = residuals_df['Before_resid'].apply(lambda x: \"*\" if abs(x) > 2 else \"\")\n",
    "residuals_df['After_sig'] = residuals_df['After_resid'].apply(lambda x: \"*\" if abs(x) > 2 else \"\")\n",
    "\n",
    "print(\"\\nğŸ“Š å„æƒ…ç·’é¡åˆ¥çš„æ¨™æº–åŒ–æ®˜å·®ï¼š\")\n",
    "print(residuals_df)\n",
    "\n",
    "# === 13. åˆä½µè¼¸å‡º (å¯å­˜æª”) ===\n",
    "# result_with_resid = emotion_comparison.join(residuals_df)\n",
    "# result_with_resid.to_csv(\"emotion_comparison_with_resid.csv\", encoding=\"utf-8-sig\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1092374c-2ebe-4647-ad4a-78993e212bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from scipy.stats import wilcoxon\n",
    "import numpy as np\n",
    "\n",
    "# === 1. è¼‰å…¥ CSV ===\n",
    "df = pd.read_csv(\"step3_all_new.csv\")\n",
    "\n",
    "# === 2. æ—¥æœŸè½‰æ› & åˆ‡åˆ† ===\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "cutoff_date = pd.to_datetime(\"2021-03-16\")\n",
    "\n",
    "before_df = df[df['date'] < cutoff_date].copy()\n",
    "after_df = df[df['date'] >= cutoff_date].copy()\n",
    "\n",
    "before_df['period'] = 'before'\n",
    "after_df['period'] = 'after'\n",
    "\n",
    "df = pd.concat([before_df, after_df], ignore_index=True)\n",
    "\n",
    "# === 3. éæ¿¾ç„¡æ•ˆå€¼ ===\n",
    "invalid_reactions = ['Cannot be inferred', 'unknown', 'Cannot be inferred.']\n",
    "invalid_entity_types = ['Cannot be inferred', 'unknown']\n",
    "\n",
    "df = df[~df['reaction'].isin(invalid_reactions)]\n",
    "df = df[~df['entity_type'].isin(invalid_entity_types)]\n",
    "\n",
    "# === 4. å»é‡ï¼šåŒç¯‡æ–‡ç« ã€åŒä¸€å€‹ entityã€åŒä¸€å€‹ reaction åªä¿ç•™ä¸€æ¬¡ ===\n",
    "df_unique = df.drop_duplicates(subset=['article_id', 'entity', 'reaction'])\n",
    "\n",
    "# === 5. æº–å‚™æ‰€æœ‰ entity_type ===\n",
    "entity_types = sorted(df_unique['entity_type'].dropna().unique())\n",
    "\n",
    "all_rows = []\n",
    "for entity in entity_types:\n",
    "    be = df_unique[(df_unique['entity_type'] == entity) & (df_unique['period'] == 'before')]['reaction'].dropna().astype(str)\n",
    "    af = df_unique[(df_unique['entity_type'] == entity) & (df_unique['period'] == 'after')]['reaction'].dropna().astype(str)\n",
    "    \n",
    "    be_counts = Counter(be)\n",
    "    af_counts = Counter(af)\n",
    "    \n",
    "    all_reactions = sorted(set(be_counts.keys()) | set(af_counts.keys()))\n",
    "\n",
    "    total_be = sum(be_counts.values())\n",
    "    total_af = sum(af_counts.values())\n",
    "\n",
    "    for reaction in all_reactions:\n",
    "        be_n = be_counts.get(reaction, 0)\n",
    "        af_n = af_counts.get(reaction, 0)\n",
    "        row = {\n",
    "            'entity_type': entity,\n",
    "            'reaction': reaction,\n",
    "            'Before': be_n,\n",
    "            'After': af_n,\n",
    "            'Total': be_n + af_n,\n",
    "            'Before(%)': round(be_n / total_be * 100, 2) if total_be > 0 else 0,\n",
    "            'After(%)': round(af_n / total_af * 100, 2) if total_af > 0 else 0\n",
    "        }\n",
    "        all_rows.append(row)\n",
    "\n",
    "# === 6. å»ºç«‹ DataFrame ä¸¦å–å‰10å ===\n",
    "comparison_df = pd.DataFrame(all_rows)\n",
    "comparison_df = comparison_df.sort_values(by=['entity_type', 'Total'], ascending=[True, False])\n",
    "top10_df = comparison_df.groupby('entity_type').head(10).reset_index(drop=True)\n",
    "\n",
    "# === 7. å„ entity_type çš„ Wilcoxon æª¢å®š ===\n",
    "print(\"\\nğŸ“Š å„ entity_type çš„ Wilcoxon æª¢å®šçµæœ\")\n",
    "entity_results = []\n",
    "\n",
    "for entity in entity_types:\n",
    "    sub_df = comparison_df[comparison_df['entity_type'] == entity]\n",
    "    before_vals = sub_df[\"Before(%)\"]\n",
    "    after_vals = sub_df[\"After(%)\"]\n",
    "\n",
    "    # é¿å…å…¨ 0 æˆ–å®Œå…¨ç›¸åŒ\n",
    "    if (before_vals != after_vals).any() and len(sub_df) > 0:\n",
    "        try:\n",
    "            stat, p = wilcoxon(after_vals, before_vals)\n",
    "            entity_results.append({\n",
    "                \"entity_type\": entity,\n",
    "                \"Statistic\": stat,\n",
    "                \"p-value\": p,\n",
    "                \"Significant\": \"Yes\" if p < 0.05 else \"No\"\n",
    "            })\n",
    "        except ValueError as e:\n",
    "            entity_results.append({\n",
    "                \"entity_type\": entity,\n",
    "                \"Statistic\": None,\n",
    "                \"p-value\": None,\n",
    "                \"Significant\": \"N/A\",\n",
    "                \"Note\": str(e)\n",
    "            })\n",
    "    else:\n",
    "        entity_results.append({\n",
    "            \"entity_type\": entity,\n",
    "            \"Statistic\": None,\n",
    "            \"p-value\": None,\n",
    "            \"Significant\": \"N/A\",\n",
    "            \"Note\": \"Before% å’Œ After% å®Œå…¨ç›¸åŒæˆ–ç‚ºç©º\"\n",
    "        })\n",
    "\n",
    "entity_results_df = pd.DataFrame(entity_results)\n",
    "print(entity_results_df)\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# === å¡æ–¹æª¢å®šï¼šå„ entity_type çš„ Before vs After ===\n",
    "print(\"\\nğŸ“Š å„ entity_type çš„å¡æ–¹æª¢å®šçµæœ\")\n",
    "chi_results = []\n",
    "\n",
    "for entity in entity_types:\n",
    "    sub_df = comparison_df[comparison_df['entity_type'] == entity]\n",
    "\n",
    "    # å»ºç«‹ åæ‡‰ Ã— æ™‚é–“ çš„åˆ—è¯è¡¨\n",
    "    contingency = sub_df[['Before','After']].to_numpy()\n",
    "\n",
    "    # å¦‚æœç¸½æ•¸å¤ªå°æˆ–åªæœ‰ä¸€å€‹åæ‡‰ï¼Œè·³é\n",
    "    if contingency.shape[0] > 1 and contingency.sum() > 0:\n",
    "        chi2, p, dof, expected = chi2_contingency(contingency)\n",
    "        chi_results.append({\n",
    "            \"entity_type\": entity,\n",
    "            \"Chi2\": round(chi2, 3),\n",
    "            \"df\": dof,\n",
    "            \"p-value\": round(p, 4),\n",
    "            \"Significant\": \"Yes\" if p < 0.05 else \"No\"\n",
    "        })\n",
    "    else:\n",
    "        chi_results.append({\n",
    "            \"entity_type\": entity,\n",
    "            \"Chi2\": None,\n",
    "            \"df\": None,\n",
    "            \"p-value\": None,\n",
    "            \"Significant\": \"N/A\"\n",
    "        })\n",
    "\n",
    "chi_results_df = pd.DataFrame(chi_results)\n",
    "print(chi_results_df)\n",
    "\n",
    "print(\"\\nğŸ“Š é¡¯è‘—çš„ entity_type çš„æ¨™æº–åŒ–æ®˜å·®åˆ†æ\")\n",
    "residuals_results = {}\n",
    "\n",
    "for entity in entity_types:\n",
    "    sub_df = comparison_df[comparison_df['entity_type'] == entity]\n",
    "    contingency = sub_df[['Before','After']].to_numpy()\n",
    "\n",
    "    if contingency.shape[0] > 1 and contingency.sum() > 0:\n",
    "        chi2, p, dof, expected = chi2_contingency(contingency)\n",
    "\n",
    "        if p < 0.05:  # åªé‡å°é¡¯è‘—çš„ç¾¤é«”\n",
    "            residuals = (contingency - expected) / np.sqrt(expected)\n",
    "\n",
    "            # æ•´ç†æˆ DataFrame\n",
    "            resid_df = pd.DataFrame(\n",
    "                residuals,\n",
    "                index=sub_df['reaction'],\n",
    "                columns=['Before_resid','After_resid']\n",
    "            ).round(2)\n",
    "\n",
    "            residuals_results[entity] = resid_df\n",
    "            print(f\"\\nğŸ” {entity} æ¨™æº–åŒ–æ®˜å·®\")\n",
    "            print(resid_df)\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# === One-way ANOVAï¼šå„ entity_type Before% vs After% ===\n",
    "print(\"\\nğŸ“Š å„ entity_type çš„ One-way ANOVA çµæœ\")\n",
    "anova_results = []\n",
    "\n",
    "for entity in entity_types:\n",
    "    sub_df = comparison_df[comparison_df['entity_type'] == entity]\n",
    "\n",
    "    before_vals = sub_df[\"Before(%)\"].values\n",
    "    after_vals = sub_df[\"After(%)\"].values\n",
    "\n",
    "    # ç¢ºä¿ä¸æ˜¯ç©ºé›†åˆ\n",
    "    if len(before_vals) > 0 and len(after_vals) > 0:\n",
    "        try:\n",
    "            f_stat, p_val = f_oneway(before_vals, after_vals)\n",
    "\n",
    "            # åˆ¤æ–·è¶¨å‹¢ï¼ˆçœ‹ç¸½å’Œæ¯”è¼ƒï¼‰\n",
    "            trend = \"Increase\" if after_vals.mean() > before_vals.mean() else \"Decrease\"\n",
    "\n",
    "            anova_results.append({\n",
    "                \"entity_type\": entity,\n",
    "                \"F-stat\": round(f_stat, 3),\n",
    "                \"p-value\": round(p_val, 4),\n",
    "                \"Significant\": \"Yes\" if p_val < 0.05 else \"No\",\n",
    "                \"Trend\": trend\n",
    "            })\n",
    "        except Exception as e:\n",
    "            anova_results.append({\n",
    "                \"entity_type\": entity,\n",
    "                \"F-stat\": None,\n",
    "                \"p-value\": None,\n",
    "                \"Significant\": \"N/A\",\n",
    "                \"Trend\": \"N/A\",\n",
    "                \"Note\": str(e)\n",
    "            })\n",
    "    else:\n",
    "        anova_results.append({\n",
    "            \"entity_type\": entity,\n",
    "            \"F-stat\": None,\n",
    "            \"p-value\": None,\n",
    "            \"Significant\": \"N/A\",\n",
    "            \"Trend\": \"N/A\",\n",
    "            \"Note\": \"Empty values\"\n",
    "        })\n",
    "\n",
    "anova_results_df = pd.DataFrame(anova_results)\n",
    "print(anova_results_df)\n",
    "\n",
    "\n",
    "\n",
    "# === 8. é¡¯ç¤ºå‰åå ===\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 200)\n",
    "pd.set_option('colheader_justify', 'left')\n",
    "\n",
    "print(\"\\nğŸ“Š å„ entity_type çš„å‰åå Reaction æ•¸é‡èˆ‡ç™¾åˆ†æ¯”æ¯”è¼ƒï¼ˆäº‹ä»¶å‰å¾Œï¼Œå»é‡å¾Œï¼‰\")\n",
    "print(top10_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795d3ebd-9ffa-4e83-9a9a-89341552d751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "# === 1. å‡è¨­ comparison_df å·²ç¶“å»ºå¥½ ===\n",
    "df = comparison_df.copy()\n",
    "\n",
    "# === 2. è¨­å®šé–¾å€¼ï¼Œéå°çš„ reaction åˆä½µ ===\n",
    "threshold = 10\n",
    "df['reaction_merged'] = df.apply(\n",
    "    lambda row: row['reaction'] if row['Total'] >= threshold else \"Other reactions\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# === 3. é‡æ–°å½™ç¸½ ===\n",
    "merged = (\n",
    "    df.groupby(['entity_type','reaction_merged'])\n",
    "      .agg({'Before':'sum','After':'sum'})\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "# è¨ˆç®—ç™¾åˆ†æ¯”\n",
    "results = []\n",
    "for entity, sub_df in merged.groupby('entity_type'):\n",
    "    total_be = sub_df['Before'].sum()\n",
    "    total_af = sub_df['After'].sum()\n",
    "    for _, row in sub_df.iterrows():\n",
    "        results.append({\n",
    "            'entity_type': entity,\n",
    "            'reaction': row['reaction_merged'],\n",
    "            'Before': row['Before'],\n",
    "            'After': row['After'],\n",
    "            'Total': row['Before'] + row['After'],\n",
    "            'Before(%)': round(row['Before'] / total_be * 100, 2) if total_be > 0 else 0,\n",
    "            'After(%)': round(row['After'] / total_af * 100, 2) if total_af > 0 else 0\n",
    "        })\n",
    "\n",
    "merged_df = pd.DataFrame(results)\n",
    "\n",
    "# === 4. å„ entity_type çš„ Wilcoxon æª¢å®š ===\n",
    "wilcoxon_results = []\n",
    "for entity, sub_df in merged_df.groupby('entity_type'):\n",
    "    before_vals = sub_df['Before(%)']\n",
    "    after_vals = sub_df['After(%)']\n",
    "    if (before_vals != after_vals).any() and len(sub_df) > 1:\n",
    "        try:\n",
    "            stat, p = wilcoxon(after_vals, before_vals)\n",
    "            wilcoxon_results.append({\n",
    "                \"entity_type\": entity,\n",
    "                \"Statistic\": stat,\n",
    "                \"p-value\": p,\n",
    "                \"Significant\": \"Yes\" if p < 0.05 else \"No\"\n",
    "            })\n",
    "        except ValueError as e:\n",
    "            wilcoxon_results.append({\n",
    "                \"entity_type\": entity,\n",
    "                \"Statistic\": None,\n",
    "                \"p-value\": None,\n",
    "                \"Significant\": \"N/A\",\n",
    "                \"Note\": str(e)\n",
    "            })\n",
    "\n",
    "wilcoxon_df = pd.DataFrame(wilcoxon_results)\n",
    "print(\"\\nğŸ“Š å„ entity_type çš„ Wilcoxon æª¢å®šçµæœï¼ˆåˆä½µä½é »åæ‡‰å¾Œï¼‰\")\n",
    "print(wilcoxon_df)\n",
    "\n",
    "print(\"\\nğŸ“Š å„ entity_type çš„å¡æ–¹æª¢å®šçµæœï¼ˆåˆä½µä½é »åæ‡‰å¾Œï¼‰\")\n",
    "chi_results = []\n",
    "\n",
    "for entity, sub_df in merged_df.groupby('entity_type'):\n",
    "    contingency = sub_df[['Before','After']].to_numpy()\n",
    "\n",
    "    if contingency.shape[0] > 1 and contingency.sum() > 0:\n",
    "        chi2, p, dof, expected = chi2_contingency(contingency)\n",
    "        chi_results.append({\n",
    "            \"entity_type\": entity,\n",
    "            \"Chi2\": round(chi2, 3),\n",
    "            \"df\": dof,\n",
    "            \"p-value\": round(p, 4),\n",
    "            \"Significant\": \"Yes\" if p < 0.05 else \"No\"\n",
    "        })\n",
    "    else:\n",
    "        chi_results.append({\n",
    "            \"entity_type\": entity,\n",
    "            \"Chi2\": None,\n",
    "            \"df\": None,\n",
    "            \"p-value\": None,\n",
    "            \"Significant\": \"N/A\"\n",
    "        })\n",
    "\n",
    "chi_results_df = pd.DataFrame(chi_results)\n",
    "print(chi_results_df)\n",
    "\n",
    "\n",
    "# === 6. é¡¯ç¤ºåˆä½µå¾Œçš„åæ‡‰ç¸½è¡¨ï¼ˆå‰ååï¼‰===\n",
    "merged_df = merged_df.sort_values(by=['entity_type', 'Total'], ascending=[True, False])\n",
    "top10_merged = merged_df.groupby('entity_type').head(10).reset_index(drop=True)\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 200)\n",
    "pd.set_option('colheader_justify', 'left')\n",
    "\n",
    "print(\"\\nğŸ“Š å„ entity_type çš„å‰åå Reaction æ•¸é‡èˆ‡ç™¾åˆ†æ¯”æ¯”è¼ƒï¼ˆäº‹ä»¶å‰å¾Œï¼Œåˆä½µä½é »å¾Œï¼‰\")\n",
    "print(top10_merged)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5447b151-9e1a-4a23-b39d-793de8540784",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# === 1. è¼‰å…¥ CSV ===\n",
    "df = pd.read_csv(\"step4_all_with_date.csv\")\n",
    "\n",
    "# === 2. æ—¥æœŸè½‰æ› & åˆ‡åˆ† ===\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "cutoff_date = pd.to_datetime(\"2021-03-16\")\n",
    "\n",
    "before_df = df[df['date'] < cutoff_date].copy()\n",
    "after_df = df[df['date'] >= cutoff_date].copy()\n",
    "\n",
    "# === 3. æ’é™¤ç„¡æ•ˆ emotion å€¼çš„ row ===\n",
    "invalid_emotions = ['cannot be inferred', 'unknown']\n",
    "valid_before = before_df[~before_df['emotion'].isin(invalid_emotions)]\n",
    "valid_after = after_df[~after_df['emotion'].isin(invalid_emotions)]\n",
    "\n",
    "# === 4. æ’é™¤ç„¡æ•ˆ entity_type çš„ row ===\n",
    "invalid_entity_types = ['Cannot be inferred', 'unknown']\n",
    "valid_before = valid_before[~valid_before['entity_type'].isin(invalid_entity_types)]\n",
    "valid_after = valid_after[~valid_after['entity_type'].isin(invalid_entity_types)]\n",
    "\n",
    "# === 5. æ‰¾å‡ºæ‰€æœ‰å‡ºç¾éçš„ entity_type ===\n",
    "entity_types = sorted(set(valid_before['entity_type'].dropna()) | set(valid_after['entity_type'].dropna()))\n",
    "\n",
    "# === 6. çµ±è¨ˆæ¯å€‹ entity_type çš„ emotion ===\n",
    "all_rows = []\n",
    "\n",
    "def extract_emotions(series):\n",
    "    all_emotions = []\n",
    "    for item in series.dropna():\n",
    "        parts = [e.strip() for e in item.split('|') if e.strip() and e.strip() not in invalid_emotions]\n",
    "        all_emotions.extend(parts)\n",
    "    return Counter(all_emotions)\n",
    "\n",
    "for entity in entity_types:\n",
    "    be_series = valid_before[valid_before['entity_type'] == entity]['emotion']\n",
    "    af_series = valid_after[valid_after['entity_type'] == entity]['emotion']\n",
    "\n",
    "    be_counts = extract_emotions(be_series)\n",
    "    af_counts = extract_emotions(af_series)\n",
    "\n",
    "    all_emotions = sorted(set(be_counts.keys()) | set(af_counts.keys()))\n",
    "    total_be = sum(be_counts.values())\n",
    "    total_af = sum(af_counts.values())\n",
    "\n",
    "    for emotion in all_emotions:\n",
    "        be_n = be_counts.get(emotion, 0)\n",
    "        af_n = af_counts.get(emotion, 0)\n",
    "        row = {\n",
    "            'entity_type': entity,\n",
    "            'emotion': emotion,\n",
    "            'Before': be_n,\n",
    "            'After': af_n,\n",
    "            'Total': be_n + af_n,\n",
    "            'Before(%)': round(be_n / total_be * 100, 2) if total_be > 0 else 0,\n",
    "            'After(%)': round(af_n / total_af * 100, 2) if total_af > 0 else 0\n",
    "        }\n",
    "        all_rows.append(row)\n",
    "\n",
    "# === 7. å»ºç«‹ DataFrame ä¸¦å–æ¯å€‹ entity_type çš„å‰10å€‹æƒ…ç·’ ===\n",
    "comparison_df = pd.DataFrame(all_rows)\n",
    "comparison_df = comparison_df.sort_values(by=['entity_type', 'Total'], ascending=[True, False])\n",
    "top10_df = comparison_df.groupby('entity_type').head(10).reset_index(drop=True)\n",
    "\n",
    "# === 8. é¡¯ç¤ºè¨­å®š ===\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 200)\n",
    "pd.set_option('display.colheader_justify', 'left')\n",
    "\n",
    "# === 9. é¡¯ç¤ºçµæœ ===\n",
    "print(\"ğŸ“Š å„ entity_type çš„å‰åå Emotion æ•¸é‡èˆ‡ç™¾åˆ†æ¯”æ¯”è¼ƒï¼ˆäº‹ä»¶å‰å¾Œï¼‰\")\n",
    "print(top10_df)\n",
    "\n",
    "# âœ…ï¼ˆå¯é¸ï¼‰è¼¸å‡ºæˆ CSV\n",
    "# top10_df.to_csv(\"step4_top10_entity_emotions.csv\", index=False, encoding=\"utf-8-sig\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a71717-28a0-44f2-a129-8d9ddc7477dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# === 1. è¼‰å…¥è³‡æ–™ ===\n",
    "df = pd.read_csv(\"step3_all_with_date.csv\")\n",
    "\n",
    "# === 2. æ—¥æœŸåˆ‡åˆ† ===\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "cutoff_date = pd.to_datetime(\"2021-03-16\")\n",
    "\n",
    "before_df = df[df['date'] < cutoff_date].copy()\n",
    "after_df = df[df['date'] >= cutoff_date].copy()\n",
    "\n",
    "# === 3. åˆ†é¡æˆ individual / organization ===\n",
    "def classify_entity_group(e):\n",
    "    if e in ['victims', 'other_individuals', 'professionals', 'politicians', 'perpetrators', 'celebrities']:\n",
    "        return 'individual'\n",
    "    elif e in ['ngo_or_advocacy_groups', 'law_enforcement_agencies', 'community_groups',\n",
    "               'government_bodies', 'business_entities']:\n",
    "        return 'organization'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "before_df['entity_group'] = before_df['entity_type'].apply(classify_entity_group)\n",
    "after_df['entity_group'] = after_df['entity_type'].apply(classify_entity_group)\n",
    "\n",
    "# === 4. éæ¿¾ç„¡æ•ˆå€¼ ===\n",
    "invalid_reactions = ['Cannot be inferred', 'unknown']\n",
    "valid_before = before_df[\n",
    "    (~before_df['reaction'].isin(invalid_reactions)) &\n",
    "    (before_df['entity_group'].isin(['individual', 'organization']))\n",
    "]\n",
    "valid_after = after_df[\n",
    "    (~after_df['reaction'].isin(invalid_reactions)) &\n",
    "    (after_df['entity_group'].isin(['individual', 'organization']))\n",
    "]\n",
    "\n",
    "# === 5. çµ±è¨ˆå„ group çš„ reaction ===\n",
    "all_rows = []\n",
    "\n",
    "for group in ['individual', 'organization']:\n",
    "    be = valid_before[valid_before['entity_group'] == group]['reaction'].dropna().astype(str)\n",
    "    af = valid_after[valid_after['entity_group'] == group]['reaction'].dropna().astype(str)\n",
    "\n",
    "    be_counts = Counter(be)\n",
    "    af_counts = Counter(af)\n",
    "\n",
    "    all_reactions = sorted(set(be_counts.keys()) | set(af_counts.keys()))\n",
    "    total_be = sum(be_counts.values())\n",
    "    total_af = sum(af_counts.values())\n",
    "\n",
    "    for reaction in all_reactions:\n",
    "        be_n = be_counts.get(reaction, 0)\n",
    "        af_n = af_counts.get(reaction, 0)\n",
    "        row = {\n",
    "            'entity_group': group,\n",
    "            'reaction': reaction,\n",
    "            'Before': be_n,\n",
    "            'After': af_n,\n",
    "            'Total': be_n + af_n,\n",
    "            'Before(%)': round(be_n / total_be * 100, 2) if total_be > 0 else 0,\n",
    "            'After(%)': round(af_n / total_af * 100, 2) if total_af > 0 else 0,\n",
    "        }\n",
    "        all_rows.append(row)\n",
    "\n",
    "# === 6. æ•´ç†çµæœ ===\n",
    "reaction_comparison_df = pd.DataFrame(all_rows)\n",
    "reaction_comparison_df = reaction_comparison_df.sort_values(by=['entity_group', 'Total'], ascending=[True, False])\n",
    "\n",
    "# === 7. åªå–å„ group å‰ 20 ===\n",
    "top20_reaction_df = reaction_comparison_df.groupby('entity_group').head(20).reset_index(drop=True)\n",
    "\n",
    "# === 8. é¡¯ç¤º ===\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 150)\n",
    "pd.set_option('display.colheader_justify', 'left')\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "print(\"ğŸ“Š å€‹äºº vs çµ„ç¹”çš„ Reaction å‰ 20 å æ•¸é‡èˆ‡ç™¾åˆ†æ¯”æ¯”è¼ƒï¼š\")\n",
    "print(top20_reaction_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82417193-14f1-4408-846d-f51ffd755620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# === 1. è®€å–è³‡æ–™ ===\n",
    "df = pd.read_csv(\"step4_all_with_date.csv\")\n",
    "\n",
    "# === 2. æ—¥æœŸåˆ‡åˆ† ===\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "cutoff_date = pd.to_datetime(\"2021-03-16\")\n",
    "\n",
    "before_df = df[df['date'] < cutoff_date].copy()\n",
    "after_df = df[df['date'] >= cutoff_date].copy()\n",
    "\n",
    "# === 3. åˆ†é¡ entity_group ===\n",
    "def classify_entity_group(e):\n",
    "    if e in ['victims', 'other_individuals', 'professionals', 'politicians', 'perpetrators', 'celebrities']:\n",
    "        return 'individual'\n",
    "    elif e in ['ngo_or_advocacy_groups', 'law_enforcement_agencies', 'community_groups',\n",
    "               'government_bodies', 'business_entities']:\n",
    "        return 'organization'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "before_df['entity_group'] = before_df['entity_type'].apply(classify_entity_group)\n",
    "after_df['entity_group'] = after_df['entity_type'].apply(classify_entity_group)\n",
    "\n",
    "# === 4. éæ¿¾ valid rows (æ’é™¤ç„¡æ•ˆæƒ…ç·’ & åªå– individual / organization) ===\n",
    "invalid_emotions = ['cannot be inferred', 'unknown']\n",
    "\n",
    "valid_before = before_df[\n",
    "    (before_df['entity_group'].isin(['individual', 'organization'])) &\n",
    "    (before_df['emotion'].notna())\n",
    "]\n",
    "valid_after = after_df[\n",
    "    (after_df['entity_group'].isin(['individual', 'organization'])) &\n",
    "    (after_df['emotion'].notna())\n",
    "]\n",
    "\n",
    "# === 5. è™•ç†å¤šå€‹ emotion çš„æ¬„ä½ ===\n",
    "def extract_emotions(series):\n",
    "    all_emotions = []\n",
    "    for item in series.dropna():\n",
    "        parts = [e.strip().lower() for e in item.split('|') if e.strip().lower() not in invalid_emotions]\n",
    "        all_emotions.extend(parts)\n",
    "    return Counter(all_emotions)\n",
    "\n",
    "# === 6. çµ±è¨ˆæ¯å€‹ entity_group çš„æƒ…ç·’ ===\n",
    "all_rows = []\n",
    "\n",
    "for group in ['individual', 'organization']:\n",
    "    be_series = valid_before[valid_before['entity_group'] == group]['emotion']\n",
    "    af_series = valid_after[valid_after['entity_group'] == group]['emotion']\n",
    "\n",
    "    be_counts = extract_emotions(be_series)\n",
    "    af_counts = extract_emotions(af_series)\n",
    "\n",
    "    all_emotions = sorted(set(be_counts.keys()) | set(af_counts.keys()))\n",
    "    total_be = sum(be_counts.values())\n",
    "    total_af = sum(af_counts.values())\n",
    "\n",
    "    for emotion in all_emotions:\n",
    "        be_n = be_counts.get(emotion, 0)\n",
    "        af_n = af_counts.get(emotion, 0)\n",
    "        row = {\n",
    "            'entity_group': group,\n",
    "            'emotion': emotion,\n",
    "            'Before': be_n,\n",
    "            'After': af_n,\n",
    "            'Total': be_n + af_n,\n",
    "            'Before(%)': round(be_n / total_be * 100, 2) if total_be > 0 else 0,\n",
    "            'After(%)': round(af_n / total_af * 100, 2) if total_af > 0 else 0,\n",
    "        }\n",
    "        all_rows.append(row)\n",
    "\n",
    "# === 7. å»ºç«‹ DataFrame ä¸¦æ’åº ===\n",
    "emotion_comparison_df = pd.DataFrame(all_rows)\n",
    "emotion_comparison_df = emotion_comparison_df.sort_values(by=['entity_group', 'Total'], ascending=[True, False])\n",
    "\n",
    "# === 8. åªå–å‰ 20 å€‹ emotion ===\n",
    "top20_emotion_df = emotion_comparison_df.groupby('entity_group').head(20).reset_index(drop=True)\n",
    "\n",
    "# === 9. é¡¯ç¤ºè¨­å®š ===\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 150)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "pd.set_option('display.colheader_justify', 'left')\n",
    "\n",
    "# === 10. é¡¯ç¤ºçµæœ ===\n",
    "print(\"ğŸ“Š å€‹äºº vs çµ„ç¹”çš„ Emotion å‰ 20 å æ•¸é‡èˆ‡ç™¾åˆ†æ¯”æ¯”è¼ƒï¼š\")\n",
    "print(top20_emotion_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
