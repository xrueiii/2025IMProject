{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "925aefd2-80c7-4d84-9617-3b2f1f26cc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ CSV ä¸­çš„æ–‡ç« ç·¨è™Ÿï¼š\n",
      "['Article_1', 'Article_2', 'Article_3', 'Article_4', 'Article_5', 'Article_6', 'Article_8', 'Article_9', 'Article_10', 'Article_11', 'Article_12', 'Article_13', 'Article_14', 'Article_15', 'Article_16', 'Article_17', 'Article_18', 'Article_19', 'Article_20', 'Article_21', 'Article_22', 'Article_23', 'Article_24', 'Article_25', 'Article_26', 'Article_27', 'Article_28', 'Article_30', 'Article_31', 'Article_32', 'Article_33', 'Article_34', 'Article_35', 'Article_36', 'Article_37', 'Article_38', 'Article_39', 'Article_40', 'Article_42', 'Article_43', 'Article_44', 'Article_45', 'Article_46', 'Article_47', 'Article_48', 'Article_49', 'Article_50', 'Article_52', 'Article_53', 'Article_56', 'Article_57', 'Article_58', 'Article_59', 'Article_60', 'Article_61', 'Article_62', 'Article_65', 'Article_66', 'Article_67', 'Article_68', 'Article_69', 'Article_70', 'Article_71', 'Article_72', 'Article_73', 'Article_74', 'Article_75', 'Article_76', 'Article_79', 'Article_80', 'Article_81', 'Article_82', 'Article_83', 'Article_84', 'Article_85', 'Article_86', 'Article_87', 'Article_88', 'Article_89', 'Article_90', 'Article_91', 'Article_92', 'Article_93', 'Article_94', 'Article_96', 'Article_97', 'Article_98', 'Article_99', 'Article_100', 'Article_101', 'Article_102', 'Article_103', 'Article_104', 'Article_105', 'Article_107', 'Article_108', 'Article_109', 'Article_110', 'Article_112', 'Article_113', 'Article_114', 'Article_115', 'Article_116', 'Article_117', 'Article_118', 'Article_119', 'Article_120', 'Article_121', 'Article_122', 'Article_123', 'Article_124', 'Article_125', 'Article_126', 'Article_127', 'Article_128', 'Article_129', 'Article_130', 'Article_131', 'Article_132', 'Article_133', 'Article_134', 'Article_136', 'Article_137', 'Article_139', 'Article_140', 'Article_141', 'Article_142', 'Article_143', 'Article_144', 'Article_145', 'Article_147', 'Article_148', 'Article_149', 'Article_150', 'Article_151', 'Article_152', 'Article_153', 'Article_154', 'Article_155', 'Article_156', 'Article_157', 'Article_158', 'Article_159', 'Article_160', 'Article_161', 'Article_162', 'Article_163', 'Article_164', 'Article_165', 'Article_166', 'Article_167', 'Article_168', 'Article_169', 'Article_170', 'Article_171', 'Article_172', 'Article_173', 'Article_174', 'Article_175', 'Article_176', 'Article_177', 'Article_178', 'Article_179', 'Article_180', 'Article_181', 'Article_182', 'Article_183', 'Article_184', 'Article_185', 'Article_186', 'Article_187', 'Article_188', 'Article_189', 'Article_190', 'Article_191', 'Article_192', 'Article_193', 'Article_194', 'Article_195', 'Article_196', 'Article_197', 'Article_198', 'Article_199', 'Article_200', 'Article_201', 'Article_202', 'Article_203', 'Article_204', 'Article_205', 'Article_206', 'Article_207', 'Article_208', 'Article_209', 'Article_210', 'Article_211', 'Article_212', 'Article_213', 'Article_214', 'Article_216', 'Article_217', 'Article_218', 'Article_219', 'Article_220', 'Article_221', 'Article_222', 'Article_223', 'Article_224', 'Article_225', 'Article_226', 'Article_227', 'Article_228', 'Article_230', 'Article_231', 'Article_232', 'Article_233', 'Article_235', 'Article_236', 'Article_237', 'Article_238', 'Article_239', 'Article_240', 'Article_241', 'Article_242', 'Article_243', 'Article_244', 'Article_245', 'Article_246', 'Article_247', 'Article_248', 'Article_249', 'Article_250', 'Article_251', 'Article_252', 'Article_253', 'Article_255', 'Article_256', 'Article_257', 'Article_258', 'Article_259', 'Article_260', 'Article_262', 'Article_263', 'Article_264', 'Article_265', 'Article_266', 'Article_267', 'Article_268', 'Article_270', 'Article_272', 'Article_273', 'Article_274', 'Article_275', 'Article_276', 'Article_277', 'Article_278', 'Article_279', 'Article_280', 'Article_281', 'Article_282', 'Article_283', 'Article_284', 'Article_285', 'Article_286', 'Article_288', 'Article_290', 'Article_291', 'Article_292', 'Article_293', 'Article_294', 'Article_296', 'Article_297', 'Article_298', 'Article_299', 'Article_300', 'Article_301', 'Article_302', 'Article_303', 'Article_304', 'Article_305', 'Article_306', 'Article_307', 'Article_308', 'Article_309', 'Article_310', 'Article_311', 'Article_312', 'Article_313', 'Article_314', 'Article_315', 'Article_316', 'Article_317', 'Article_318', 'Article_319', 'Article_320', 'Article_321', 'Article_322', 'Article_323', 'Article_324', 'Article_325', 'Article_326', 'Article_327', 'Article_328', 'Article_329', 'Article_330', 'Article_331', 'Article_332', 'Article_333', 'Article_334', 'Article_335', 'Article_336', 'Article_337', 'Article_338', 'Article_339', 'Article_340', 'Article_341', 'Article_342', 'Article_343', 'Article_344', 'Article_345', 'Article_346', 'Article_347', 'Article_348', 'Article_349', 'Article_350', 'Article_351', 'Article_352', 'Article_355', 'Article_356', 'Article_357', 'Article_358', 'Article_359', 'Article_360', 'Article_361', 'Article_363', 'Article_364', 'Article_365', 'Article_366', 'Article_367', 'Article_368', 'Article_369', 'Article_370', 'Article_371', 'Article_373', 'Article_374', 'Article_375', 'Article_376', 'Article_377', 'Article_379', 'Article_380', 'Article_381', 'Article_382', 'Article_383', 'Article_384', 'Article_385', 'Article_386', 'Article_387', 'Article_388', 'Article_389', 'Article_390', 'Article_391', 'Article_392', 'Article_393', 'Article_394', 'Article_395', 'Article_396', 'Article_397', 'Article_398', 'Article_399', 'Article_400', 'Article_401', 'Article_402', 'Article_403', 'Article_404', 'Article_405', 'Article_406', 'Article_409', 'Article_410', 'Article_411', 'Article_412', 'Article_413', 'Article_414', 'Article_415', 'Article_416', 'Article_417', 'Article_418', 'Article_419', 'Article_420', 'Article_421', 'Article_422', 'Article_423', 'Article_424', 'Article_425', 'Article_426', 'Article_427', 'Article_428', 'Article_429', 'Article_430', 'Article_431', 'Article_433', 'Article_434', 'Article_435', 'Article_436', 'Article_437', 'Article_438', 'Article_439', 'Article_440', 'Article_441', 'Article_442', 'Article_443', 'Article_445', 'Article_446', 'Article_448', 'Article_449', 'Article_450', 'Article_451', 'Article_452', 'Article_453', 'Article_454', 'Article_455', 'Article_456', 'Article_457', 'Article_458', 'Article_459', 'Article_460', 'Article_461', 'Article_463', 'Article_465', 'Article_466', 'Article_467', 'Article_468', 'Article_469', 'Article_470', 'Article_471', 'Article_473', 'Article_474', 'Article_475', 'Article_476', 'Article_477', 'Article_478', 'Article_479', 'Article_480', 'Article_481', 'Article_483', 'Article_484', 'Article_485', 'Article_486', 'Article_487', 'Article_488', 'Article_489', 'Article_490', 'Article_492', 'Article_493', 'Article_494', 'Article_495', 'Article_496', 'Article_499', 'Article_500', 'Article_501', 'Article_503', 'Article_504', 'Article_505', 'Article_506', 'Article_507', 'Article_508', 'Article_510', 'Article_511', 'Article_512', 'Article_513', 'Article_514', 'Article_515', 'Article_516', 'Article_517', 'Article_518', 'Article_519', 'Article_520', 'Article_521', 'Article_522', 'Article_523', 'Article_524', 'Article_525', 'Article_526', 'Article_527', 'Article_528', 'Article_529', 'Article_530', 'Article_531', 'Article_532', 'Article_533', 'Article_534', 'Article_535', 'Article_536', 'Article_537', 'Article_538', 'Article_539', 'Article_540', 'Article_541', 'Article_542', 'Article_543', 'Article_544', 'Article_545', 'Article_546', 'Article_547', 'Article_548', 'Article_549', 'Article_550', 'Article_551', 'Article_552', 'Article_553', 'Article_554', 'Article_555', 'Article_556', 'Article_557', 'Article_558', 'Article_559', 'Article_560', 'Article_561', 'Article_562', 'Article_563', 'Article_564', 'Article_565', 'Article_566', 'Article_567', 'Article_568', 'Article_569', 'Article_570', 'Article_571', 'Article_572', 'Article_573', 'Article_574', 'Article_575', 'Article_576', 'Article_577', 'Article_578', 'Article_579', 'Article_580', 'Article_581', 'Article_582', 'Article_583', 'Article_584', 'Article_585', 'Article_586', 'Article_587', 'Article_588', 'Article_589', 'Article_591', 'Article_592', 'Article_593', 'Article_594', 'Article_596', 'Article_597', 'Article_598', 'Article_599', 'Article_600', 'Article_601', 'Article_602', 'Article_603', 'Article_604', 'Article_605', 'Article_606', 'Article_607', 'Article_609', 'Article_610', 'Article_611', 'Article_613', 'Article_614', 'Article_616', 'Article_617', 'Article_618', 'Article_619', 'Article_620', 'Article_621', 'Article_622', 'Article_623', 'Article_624', 'Article_625', 'Article_626', 'Article_627', 'Article_628', 'Article_629', 'Article_630', 'Article_631', 'Article_632', 'Article_633', 'Article_634']\n",
      "âœ… ç¸½å…±æœ‰ 584 ç¯‡æ–‡ç« \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from typing import Dict, Any\n",
    "from openai import OpenAI\n",
    "\n",
    "# ==============================\n",
    "# OpenAI è¨­å®šèˆ‡å‘¼å«\n",
    "# ==============================\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "def get_response(prompt: str, temperature: float = 0.0) -> str:\n",
    "    \"\"\"å‘¼å« OpenAI APIï¼Œå›å‚³æ¨¡å‹è¼¸å‡ºï¼ˆæ‡‰è©²æ˜¯ JSON å­—ä¸²ï¼‰\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperature,\n",
    "            stream=False,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"API èª¿ç”¨éŒ¯èª¤: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# ==============================\n",
    "# è®€å–æ–‡ç« ï¼ˆåªç‚ºäº†ä¿ç•™é †åºï¼Œä¸ä¸Ÿé€²æ¨¡å‹ï¼‰\n",
    "# ==============================\n",
    "import pandas as pd\n",
    "\n",
    "# è®€å– CSV\n",
    "df = pd.read_csv(\"./articles_584.csv\", encoding=\"ISO-8859-1\")\n",
    "\n",
    "# åªä¿ç•™æœ‰æ–‡ç« çš„è³‡æ–™\n",
    "df = df.dropna(subset=[\"ARTICLE_TEXT\"])\n",
    "\n",
    "# ç¢ºä¿ id æ¬„ä½æ˜¯æ•´æ•¸å‹æ…‹ï¼ˆé¿å…å¾Œé¢æ‹¼æ¥å‡ºå•é¡Œï¼‰\n",
    "df[\"id\"] = df[\"id\"].astype(int)\n",
    "\n",
    "# å»ºç«‹ dictï¼šç”¨ CSV è£¡çš„ id ç•¶ç·¨è™Ÿ\n",
    "articles = {f\"Article_{row['id']}\": row[\"ARTICLE_TEXT\"] for _, row in df.iterrows()}\n",
    "\n",
    "# åˆ—å‡ºå…¨éƒ¨æ–‡ç« ç·¨è™Ÿ\n",
    "all_articles = list(articles.keys())\n",
    "print(\"ğŸ“„ CSV ä¸­çš„æ–‡ç« ç·¨è™Ÿï¼š\")\n",
    "print(all_articles)  # å…ˆåªå°å‰ 20 ç­†\n",
    "print(f\"âœ… ç¸½å…±æœ‰ {len(all_articles)} ç¯‡æ–‡ç« \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c453f1f-c7bb-4338-8446-6a9e374f52db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å¯¦éš›æ–‡ç« æ•¸é‡: 584 / 584\n",
      "âŒ ç¼ºå°‘çš„ Article ç·¨è™Ÿï¼ˆå…± 45 ç¯‡ï¼‰:\n",
      "Article_7, Article_29, Article_41, Article_51, Article_54, Article_55, Article_63, Article_64, Article_77, Article_78, Article_95, Article_106, Article_111, Article_135, Article_138, Article_146, Article_215, Article_229, Article_234, Article_254, Article_261, Article_269, Article_271, Article_287, Article_289, Article_295, Article_353, Article_354, Article_362, Article_372, Article_378, Article_407, Article_408, Article_432, Article_444, Article_447, Article_462, Article_464, Article_472, Article_482, Article_491, Article_497, Article_498, Article_502, Article_509\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# è¼¸å…¥ä½ çš„ CSV æª”æ¡ˆåç¨±\n",
    "csv_file = \"articles_584.csv\"\n",
    "\n",
    "# å‡è¨­ä½ çŸ¥é“ç¸½å…±æ‡‰è©²æœ‰å¤šå°‘ç¯‡ï¼ˆä¾‹ï¼š584ï¼‰\n",
    "expected_total = 584\n",
    "\n",
    "# è®€å– CSV\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# å–å¾—å¯¦éš›å‡ºç¾éçš„ idï¼ˆè½‰æˆ intï¼‰\n",
    "existing_numbers = set(df[\"id\"].dropna().astype(int))\n",
    "\n",
    "# å»ºç«‹å®Œæ•´æ‡‰æœ‰çš„ ID é›†åˆ\n",
    "# âš ï¸ å‡è¨­ id å¾ 1 é–‹å§‹ç·¨è™Ÿ\n",
    "expected_numbers = set(range(1, expected_total + 1))\n",
    "\n",
    "# æ‰¾å‡ºç¼ºå°‘çš„ç·¨è™Ÿ\n",
    "missing = sorted(expected_numbers - existing_numbers)\n",
    "\n",
    "print(f\"âœ… å¯¦éš›æ–‡ç« æ•¸é‡: {len(existing_numbers)} / {expected_total}\")\n",
    "print(f\"âŒ ç¼ºå°‘çš„ Article ç·¨è™Ÿï¼ˆå…± {len(missing)} ç¯‡ï¼‰:\")\n",
    "print(\", \".join(f\"Article_{i}\" for i in missing))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4936bcaa-d529-457a-92ad-55356de3f848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 1/12...\n",
      "âœ… æ‰¹æ¬¡ 1/12 å·²å®Œæˆï¼Œå„²å­˜è‡³ step1_batches\\step1_batch_1.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 2/12...\n",
      "âœ… æ‰¹æ¬¡ 2/12 å·²å®Œæˆï¼Œå„²å­˜è‡³ step1_batches\\step1_batch_2.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 3/12...\n",
      "âœ… æ‰¹æ¬¡ 3/12 å·²å®Œæˆï¼Œå„²å­˜è‡³ step1_batches\\step1_batch_3.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 4/12...\n",
      "âœ… æ‰¹æ¬¡ 4/12 å·²å®Œæˆï¼Œå„²å­˜è‡³ step1_batches\\step1_batch_4.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 5/12...\n",
      "âœ… æ‰¹æ¬¡ 5/12 å·²å®Œæˆï¼Œå„²å­˜è‡³ step1_batches\\step1_batch_5.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 6/12...\n",
      "âœ… æ‰¹æ¬¡ 6/12 å·²å®Œæˆï¼Œå„²å­˜è‡³ step1_batches\\step1_batch_6.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 7/12...\n",
      "âœ… æ‰¹æ¬¡ 7/12 å·²å®Œæˆï¼Œå„²å­˜è‡³ step1_batches\\step1_batch_7.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 8/12...\n",
      "âœ… æ‰¹æ¬¡ 8/12 å·²å®Œæˆï¼Œå„²å­˜è‡³ step1_batches\\step1_batch_8.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 9/12...\n",
      "âœ… æ‰¹æ¬¡ 9/12 å·²å®Œæˆï¼Œå„²å­˜è‡³ step1_batches\\step1_batch_9.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 10/12...\n",
      "âœ… æ‰¹æ¬¡ 10/12 å·²å®Œæˆï¼Œå„²å­˜è‡³ step1_batches\\step1_batch_10.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 11/12...\n",
      "âœ… æ‰¹æ¬¡ 11/12 å·²å®Œæˆï¼Œå„²å­˜è‡³ step1_batches\\step1_batch_11.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 12/12...\n",
      "âœ… æ‰¹æ¬¡ 12/12 å·²å®Œæˆï¼Œå„²å­˜è‡³ step1_batches\\step1_batch_12.json\n",
      "\n",
      "ğŸ‰ æ‰€æœ‰æ‰¹æ¬¡è™•ç†å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Colab ç¨‹å¼ç¢¼å€å¡Š 1: åˆå§‹åŒ–å’Œæ­¥é©Ÿ1\n",
    "# ==========================================\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Step 1 çš„æç¤ºè©\n",
    "step1_prompt = \"\"\"You are a sociology professor with 30 years of experience analyzing Asian racism in the United States.\n",
    "Your task is to analyze the following article by identifying named entities and classifying them into the appropriate social roles and institutional categories. Focus especially on people or groups reacting to or affected by anti-Asian incidents.\n",
    "\n",
    "Step 1: Named Entity Recognition and Categorization\n",
    "\n",
    "1. Identify named entities.\n",
    "2. Classify them into appropriate social roles or institutional categories.\n",
    "3. Determine whether each **individual** is **Asian**, **Non-Asian**, or **Cannot be inferred** based on the text.\n",
    "4. For each entity, include a `\"reference\"` field that reflects **the exact phrase or wording** used in the article to refer to them.\n",
    "\n",
    "Please extract all named entities from the text and categorize them according to the following schema:\n",
    "\n",
    "---\n",
    "\n",
    "**INDIVIDUALS** (Specific persons or actors representing individual agency)\n",
    "\n",
    "1. politicians\n",
    "   - Elected officials acting in an individual capacity.\n",
    "   - Examples: senators, representatives, mayors, governors\n",
    "\n",
    "2. professionals\n",
    "   - Individuals recognized by their expertise or institutional role.\n",
    "   - Examples: professors, doctors, lawyers, foundation presidents\n",
    "\n",
    "3. celebrities\n",
    "   - Public figures in entertainment or sports (e.g., actors, athletes) unless clearly acting in a professional or political role.\n",
    "   - If overlapping with another role, assign to the more institutionally grounded category.\n",
    "\n",
    "4. perpetrators\n",
    "   - Individuals directly identified as committing or responsible for anti-Asian actions.\n",
    "   - Do not include vague or generalized public unless clearly specified.\n",
    "\n",
    "5. victims\n",
    "   - Individuals or racial/ethnic groups explicitly targeted by anti-Asian acts.\n",
    "   - Examples: â€œa woman attacked on the subway,â€ â€œJapanese Americans during WWIIâ€\n",
    "\n",
    "6. other_individuals\n",
    "   - All other named or unnamed individuals who do not fall into the above categories.\n",
    "   - Includes the general public, community members, business owners, or relatives (e.g., â€œmy mom,â€ â€œa neighborâ€).\n",
    "\n",
    "---\n",
    "\n",
    "**ORGANIZATIONS** (Named institutions or collectives)\n",
    "\n",
    "1. law_enforcement_agencies\n",
    "   - Official police or investigative institutions.\n",
    "   - Examples: Chicago Police Department, FBI, local sheriffâ€™s office\n",
    "\n",
    "2. government_bodies\n",
    "   - Government agencies, departments, or offices at any level (local/state/federal).\n",
    "   - Examples: CDC, Department of Justice, City Council\n",
    "\n",
    "3. ngo_or_advocacy_groups\n",
    "   - Civil rights organizations, foundations, or advocacy nonprofits.\n",
    "   - Examples: Stop AAPI Hate, Robert Wood Johnson Foundation\n",
    "\n",
    "4. business_entities\n",
    "   - Named companies, hotels, restaurants, or stores.\n",
    "   - Examples: Wrap-on Tools, Edgewater Beach Hotel\n",
    "\n",
    "5. community_groups\n",
    "   - Named cultural, ethnic, or neighborhood associations.\n",
    "   - Examples: Chinatown Association, Asian-American Coalition\n",
    "\n",
    "---\n",
    "\n",
    "**ETHNICITY INFERENCE RULES:**\n",
    "\n",
    "- For each **individual**, determine whether they are **Asian**, **Non-Asian**, or **Cannot be inferred**.\n",
    "- Use contextual clues such as ethnicity indicators, names, or explicit mentions.\n",
    "- If ethnicity is ambiguous or not stated, return `\"Cannot be inferred\"`.\n",
    "\n",
    "---\n",
    "\n",
    "**ADDITIONAL INSTRUCTIONS:**\n",
    "\n",
    "- Use `\"reference\"` to capture how the person/group was referred to in the original article (e.g., `\"an 80-year-old woman\"`, `\"Lee\"`, `\"the attacker\"`).\n",
    "- Normalize all name variants to a canonical form (e.g., â€œDr. Church,â€ â€œJ. Church,â€ and â€œChurchâ€ â†’ â€œJacqueline Churchâ€).\n",
    "- If an individual belongs to multiple roles, assign them to the most institutionally specific one (e.g., categorize a lawyer-celebrity as a professional).\n",
    "- Include only individuals explicitly involved in specific incidents under â€œvictimsâ€ and â€œperpetrators.â€\n",
    "- Do not classify individual police officers or sheriffs as individualsâ€”assign them under law_enforcement_agencies.\n",
    "- Classify individual owners under â€œbusiness_actorsâ€ and company names under â€œbusiness_entities.â€\n",
    "\n",
    "---\n",
    "\n",
    "**Output format (in JSON):**\n",
    "\n",
    "For all individuals, return an object with \"name\" and \"asian_status\" fields.\n",
    "For all organizations, return an object with \"name\" and \"asian_status\": \"Not applicable\".\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"individuals\": {\n",
    "    \"politicians\": [\n",
    "      {\n",
    "        \"name\": \"Tammy Duckworth\",\n",
    "        \"reference\": \"Senator Tammy Duckworth\",\n",
    "        \"asian_status\": \"Asian\"\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"Joe Biden\",\n",
    "        \"reference\": \"President Joe Biden\",\n",
    "        \"asian_status\": \"Non-Asian\"\n",
    "      }\n",
    "    ],\n",
    "    \"professionals\": [\n",
    "      {\n",
    "        \"name\": \"Julie Morita\",\n",
    "        \"reference\": \"Julie Morita\",\n",
    "        \"asian_status\": \"Asian\"\n",
    "      }\n",
    "    ],\n",
    "    \"celebrities\": [\n",
    "      {\n",
    "        \"name\": \"Awkwafina\",\n",
    "        \"reference\": \"Awkwafina\",\n",
    "        \"asian_status\": \"Asian\"\n",
    "      }\n",
    "    ],\n",
    "    \"perpetrators\": [\n",
    "      {\n",
    "        \"name\": \"Unknown Attacker\",\n",
    "        \"reference\": \"the attacker\",\n",
    "        \"asian_status\": \"Non-Asian\"\n",
    "      }\n",
    "    ],\n",
    "    \"victims\": [\n",
    "      {\n",
    "        \"name\": \"Asian Elderly Woman\",\n",
    "        \"reference\": \"an 80-year-old woman\",\n",
    "        \"asian_status\": \"Asian\"\n",
    "      }\n",
    "    ],\n",
    "    \"other_individuals\": [\n",
    "      {\n",
    "        \"name\": \"my mom\",\n",
    "        \"reference\": \"my mom\",\n",
    "        \"asian_status\": \"Asian\"\n",
    "      }\n",
    "    ]\n",
    "    \n",
    "  },\n",
    "  \"organizations\": {\n",
    "    \"law_enforcement_agencies\": [\n",
    "      {\n",
    "        \"name\": \"Chicago Police Department\",\n",
    "        \"reference\": \"Chicago Police Department\",\n",
    "        \"asian_status\": \"Not applicable\"\n",
    "      }\n",
    "    ],\n",
    "    \"government_bodies\": [\n",
    "      {\n",
    "        \"name\": \"City Council\",\n",
    "        \"reference\": \"City Council\",\n",
    "        \"asian_status\": \"Not applicable\"\n",
    "      }\n",
    "    ],\n",
    "    \"ngo_or_advocacy_groups\": [\n",
    "      {\n",
    "        \"name\": \"Stop AAPI Hate\",\n",
    "        \"reference\": \"Stop AAPI Hate\",\n",
    "        \"asian_status\": \"Not applicable\"\n",
    "      }\n",
    "    ],\n",
    "    \"business_entities\": [\n",
    "      {\n",
    "        \"name\": \"Edgewater Beach Hotel\",\n",
    "        \"reference\": \"Edgewater Beach Hotel\",\n",
    "        \"asian_status\": \"Not applicable\"\n",
    "      }\n",
    "    ],\n",
    "    \"community_groups\": [\n",
    "      {\n",
    "        \"name\": \"Chinatown Association\",\n",
    "        \"reference\": \"Chinatown Association\",\n",
    "        \"asian_status\": \"Not applicable\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# å‘¼å« Step 1\n",
    "\n",
    "# å»ºç«‹è¼¸å‡ºè³‡æ–™å¤¾\n",
    "output_dir = \"step1_batches\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# æ¯æ‰¹æ–‡ç« æ•¸é‡\n",
    "batch_size = 50\n",
    "article_items = list(articles.items())\n",
    "total_articles = len(article_items)\n",
    "total_batches = (total_articles + batch_size - 1) // batch_size  # ç„¡æ¢ä»¶é€²ä½\n",
    "\n",
    "# é€æ‰¹è™•ç†\n",
    "for batch_idx in range(0, total_articles, batch_size):\n",
    "    batch_number = batch_idx // batch_size + 1\n",
    "    filename = os.path.join(output_dir, f\"step1_batch_{batch_number}.json\")\n",
    "\n",
    "    # å¦‚æœæª”æ¡ˆå·²ç¶“å­˜åœ¨ï¼Œå°±è·³éé€™æ‰¹\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"â­ï¸ æ‰¹æ¬¡ {batch_number}/{total_batches} å·²å­˜åœ¨ï¼Œè·³é\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ {batch_number}/{total_batches}...\")\n",
    "\n",
    "    step1_result = {}\n",
    "    batch = article_items[batch_idx: batch_idx + batch_size]\n",
    "\n",
    "    for title, content in batch:\n",
    "        full_prompt = step1_prompt + \"\\n\\nArticle Text:\\n\" + content\n",
    "        response = get_response(full_prompt)\n",
    "        step1_result[title] = response\n",
    "\n",
    "    # å„²å­˜é€™ä¸€æ‰¹çµæœ\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(step1_result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"âœ… æ‰¹æ¬¡ {batch_number}/{total_batches} å·²å®Œæˆï¼Œå„²å­˜è‡³ {filename}\")\n",
    "\n",
    "print(\"\\nğŸ‰ æ‰€æœ‰æ‰¹æ¬¡è™•ç†å®Œæˆï¼\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97f46f96-40d0-4d23-a71d-fdf5e7931277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ é–‹å§‹è™•ç† step1_batches\\step1_batch_1.json â†’ step2_batches\\step2_batch_1.json\n",
      "âœ”ï¸ å·²è™•ç† Article_1\n",
      "âœ”ï¸ å·²è™•ç† Article_2\n",
      "âœ”ï¸ å·²è™•ç† Article_3\n",
      "âœ”ï¸ å·²è™•ç† Article_4\n",
      "âœ”ï¸ å·²è™•ç† Article_5\n",
      "âœ”ï¸ å·²è™•ç† Article_6\n",
      "âœ”ï¸ å·²è™•ç† Article_8\n",
      "âœ”ï¸ å·²è™•ç† Article_9\n",
      "âœ”ï¸ å·²è™•ç† Article_10\n",
      "âœ”ï¸ å·²è™•ç† Article_11\n",
      "âœ”ï¸ å·²è™•ç† Article_12\n",
      "âœ”ï¸ å·²è™•ç† Article_13\n",
      "âœ”ï¸ å·²è™•ç† Article_14\n",
      "âœ”ï¸ å·²è™•ç† Article_15\n",
      "âœ”ï¸ å·²è™•ç† Article_16\n",
      "âœ”ï¸ å·²è™•ç† Article_17\n",
      "âœ”ï¸ å·²è™•ç† Article_18\n",
      "âœ”ï¸ å·²è™•ç† Article_19\n",
      "âœ”ï¸ å·²è™•ç† Article_20\n",
      "âœ”ï¸ å·²è™•ç† Article_21\n",
      "âœ”ï¸ å·²è™•ç† Article_22\n",
      "âœ”ï¸ å·²è™•ç† Article_23\n",
      "âœ”ï¸ å·²è™•ç† Article_24\n",
      "âœ”ï¸ å·²è™•ç† Article_25\n",
      "âœ”ï¸ å·²è™•ç† Article_26\n",
      "âœ”ï¸ å·²è™•ç† Article_27\n",
      "âœ”ï¸ å·²è™•ç† Article_28\n",
      "âœ”ï¸ å·²è™•ç† Article_30\n",
      "âœ”ï¸ å·²è™•ç† Article_31\n",
      "âœ”ï¸ å·²è™•ç† Article_32\n",
      "âœ”ï¸ å·²è™•ç† Article_33\n",
      "âœ”ï¸ å·²è™•ç† Article_34\n",
      "âœ”ï¸ å·²è™•ç† Article_35\n",
      "âœ”ï¸ å·²è™•ç† Article_36\n",
      "âœ”ï¸ å·²è™•ç† Article_37\n",
      "âœ”ï¸ å·²è™•ç† Article_38\n",
      "âœ”ï¸ å·²è™•ç† Article_39\n",
      "âœ”ï¸ å·²è™•ç† Article_40\n",
      "âœ”ï¸ å·²è™•ç† Article_42\n",
      "âœ”ï¸ å·²è™•ç† Article_43\n",
      "âœ”ï¸ å·²è™•ç† Article_44\n",
      "âœ”ï¸ å·²è™•ç† Article_45\n",
      "âœ”ï¸ å·²è™•ç† Article_46\n",
      "âœ”ï¸ å·²è™•ç† Article_47\n",
      "âœ”ï¸ å·²è™•ç† Article_48\n",
      "âœ”ï¸ å·²è™•ç† Article_49\n",
      "âœ”ï¸ å·²è™•ç† Article_50\n",
      "âœ”ï¸ å·²è™•ç† Article_52\n",
      "âœ”ï¸ å·²è™•ç† Article_53\n",
      "âœ”ï¸ å·²è™•ç† Article_56\n",
      "âœ… step2_batch_1.json å·²å®Œæˆä¸¦å„²å­˜è‡³ step2_batches\\step2_batch_1.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç† step1_batches\\step1_batch_10.json â†’ step2_batches\\step2_batch_10.json\n",
      "âœ”ï¸ å·²è™•ç† Article_492\n",
      "âœ”ï¸ å·²è™•ç† Article_493\n",
      "âœ”ï¸ å·²è™•ç† Article_494\n",
      "âœ”ï¸ å·²è™•ç† Article_495\n",
      "âœ”ï¸ å·²è™•ç† Article_496\n",
      "âœ”ï¸ å·²è™•ç† Article_499\n",
      "âœ”ï¸ å·²è™•ç† Article_500\n",
      "âœ”ï¸ å·²è™•ç† Article_501\n",
      "âœ”ï¸ å·²è™•ç† Article_503\n",
      "âœ”ï¸ å·²è™•ç† Article_504\n",
      "âœ”ï¸ å·²è™•ç† Article_505\n",
      "âœ”ï¸ å·²è™•ç† Article_506\n",
      "âœ”ï¸ å·²è™•ç† Article_507\n",
      "âœ”ï¸ å·²è™•ç† Article_508\n",
      "âœ”ï¸ å·²è™•ç† Article_510\n",
      "âœ”ï¸ å·²è™•ç† Article_511\n",
      "âœ”ï¸ å·²è™•ç† Article_512\n",
      "âœ”ï¸ å·²è™•ç† Article_513\n",
      "âœ”ï¸ å·²è™•ç† Article_514\n",
      "âœ”ï¸ å·²è™•ç† Article_515\n",
      "âœ”ï¸ å·²è™•ç† Article_516\n",
      "âœ”ï¸ å·²è™•ç† Article_517\n",
      "âœ”ï¸ å·²è™•ç† Article_518\n",
      "âœ”ï¸ å·²è™•ç† Article_519\n",
      "âœ”ï¸ å·²è™•ç† Article_520\n",
      "âœ”ï¸ å·²è™•ç† Article_521\n",
      "âœ”ï¸ å·²è™•ç† Article_522\n",
      "âœ”ï¸ å·²è™•ç† Article_523\n",
      "âœ”ï¸ å·²è™•ç† Article_524\n",
      "âœ”ï¸ å·²è™•ç† Article_525\n",
      "âœ”ï¸ å·²è™•ç† Article_526\n",
      "âœ”ï¸ å·²è™•ç† Article_527\n",
      "âœ”ï¸ å·²è™•ç† Article_528\n",
      "âœ”ï¸ å·²è™•ç† Article_529\n",
      "âœ”ï¸ å·²è™•ç† Article_530\n",
      "âœ”ï¸ å·²è™•ç† Article_531\n",
      "âœ”ï¸ å·²è™•ç† Article_532\n",
      "âœ”ï¸ å·²è™•ç† Article_533\n",
      "âœ”ï¸ å·²è™•ç† Article_534\n",
      "âœ”ï¸ å·²è™•ç† Article_535\n",
      "âœ”ï¸ å·²è™•ç† Article_536\n",
      "âœ”ï¸ å·²è™•ç† Article_537\n",
      "âœ”ï¸ å·²è™•ç† Article_538\n",
      "âœ”ï¸ å·²è™•ç† Article_539\n",
      "âœ”ï¸ å·²è™•ç† Article_540\n",
      "âœ”ï¸ å·²è™•ç† Article_541\n",
      "âœ”ï¸ å·²è™•ç† Article_542\n",
      "âœ”ï¸ å·²è™•ç† Article_543\n",
      "âœ”ï¸ å·²è™•ç† Article_544\n",
      "âœ”ï¸ å·²è™•ç† Article_545\n",
      "âœ… step2_batch_10.json å·²å®Œæˆä¸¦å„²å­˜è‡³ step2_batches\\step2_batch_10.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç† step1_batches\\step1_batch_11.json â†’ step2_batches\\step2_batch_11.json\n",
      "âœ”ï¸ å·²è™•ç† Article_546\n",
      "âœ”ï¸ å·²è™•ç† Article_547\n",
      "âœ”ï¸ å·²è™•ç† Article_548\n",
      "âœ”ï¸ å·²è™•ç† Article_549\n",
      "âœ”ï¸ å·²è™•ç† Article_550\n",
      "âœ”ï¸ å·²è™•ç† Article_551\n",
      "âœ”ï¸ å·²è™•ç† Article_552\n",
      "âœ”ï¸ å·²è™•ç† Article_553\n",
      "âœ”ï¸ å·²è™•ç† Article_554\n",
      "âœ”ï¸ å·²è™•ç† Article_555\n",
      "âœ”ï¸ å·²è™•ç† Article_556\n",
      "âœ”ï¸ å·²è™•ç† Article_557\n",
      "âœ”ï¸ å·²è™•ç† Article_558\n",
      "âœ”ï¸ å·²è™•ç† Article_559\n",
      "âœ”ï¸ å·²è™•ç† Article_560\n",
      "âœ”ï¸ å·²è™•ç† Article_561\n",
      "âœ”ï¸ å·²è™•ç† Article_562\n",
      "âœ”ï¸ å·²è™•ç† Article_563\n",
      "âœ”ï¸ å·²è™•ç† Article_564\n",
      "âœ”ï¸ å·²è™•ç† Article_565\n",
      "âœ”ï¸ å·²è™•ç† Article_566\n",
      "âœ”ï¸ å·²è™•ç† Article_567\n",
      "âœ”ï¸ å·²è™•ç† Article_568\n",
      "âœ”ï¸ å·²è™•ç† Article_569\n",
      "âœ”ï¸ å·²è™•ç† Article_570\n",
      "âœ”ï¸ å·²è™•ç† Article_571\n",
      "âœ”ï¸ å·²è™•ç† Article_572\n",
      "âœ”ï¸ å·²è™•ç† Article_573\n",
      "âœ”ï¸ å·²è™•ç† Article_574\n",
      "âœ”ï¸ å·²è™•ç† Article_575\n",
      "âœ”ï¸ å·²è™•ç† Article_576\n",
      "âœ”ï¸ å·²è™•ç† Article_577\n",
      "âœ”ï¸ å·²è™•ç† Article_578\n",
      "âœ”ï¸ å·²è™•ç† Article_579\n",
      "âœ”ï¸ å·²è™•ç† Article_580\n",
      "âœ”ï¸ å·²è™•ç† Article_581\n",
      "âœ”ï¸ å·²è™•ç† Article_582\n",
      "âœ”ï¸ å·²è™•ç† Article_583\n",
      "âœ”ï¸ å·²è™•ç† Article_584\n",
      "âœ”ï¸ å·²è™•ç† Article_585\n",
      "âœ”ï¸ å·²è™•ç† Article_586\n",
      "âœ”ï¸ å·²è™•ç† Article_587\n",
      "âœ”ï¸ å·²è™•ç† Article_588\n",
      "âœ”ï¸ å·²è™•ç† Article_589\n",
      "âœ”ï¸ å·²è™•ç† Article_591\n",
      "âœ”ï¸ å·²è™•ç† Article_592\n",
      "âœ”ï¸ å·²è™•ç† Article_593\n",
      "âœ”ï¸ å·²è™•ç† Article_594\n",
      "âœ”ï¸ å·²è™•ç† Article_596\n",
      "âœ”ï¸ å·²è™•ç† Article_597\n",
      "âœ… step2_batch_11.json å·²å®Œæˆä¸¦å„²å­˜è‡³ step2_batches\\step2_batch_11.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç† step1_batches\\step1_batch_12.json â†’ step2_batches\\step2_batch_12.json\n",
      "âœ”ï¸ å·²è™•ç† Article_598\n",
      "âœ”ï¸ å·²è™•ç† Article_599\n",
      "âœ”ï¸ å·²è™•ç† Article_600\n",
      "âœ”ï¸ å·²è™•ç† Article_601\n",
      "âœ”ï¸ å·²è™•ç† Article_602\n",
      "âœ”ï¸ å·²è™•ç† Article_603\n",
      "âœ”ï¸ å·²è™•ç† Article_604\n",
      "âœ”ï¸ å·²è™•ç† Article_605\n",
      "âœ”ï¸ å·²è™•ç† Article_606\n",
      "âœ”ï¸ å·²è™•ç† Article_607\n",
      "âœ”ï¸ å·²è™•ç† Article_609\n",
      "âœ”ï¸ å·²è™•ç† Article_610\n",
      "âœ”ï¸ å·²è™•ç† Article_611\n",
      "âœ”ï¸ å·²è™•ç† Article_613\n",
      "âœ”ï¸ å·²è™•ç† Article_614\n",
      "âœ”ï¸ å·²è™•ç† Article_616\n",
      "âœ”ï¸ å·²è™•ç† Article_617\n",
      "âœ”ï¸ å·²è™•ç† Article_618\n",
      "âœ”ï¸ å·²è™•ç† Article_619\n",
      "âœ”ï¸ å·²è™•ç† Article_620\n",
      "âœ”ï¸ å·²è™•ç† Article_621\n",
      "âœ”ï¸ å·²è™•ç† Article_622\n",
      "âœ”ï¸ å·²è™•ç† Article_623\n",
      "âœ”ï¸ å·²è™•ç† Article_624\n",
      "âœ”ï¸ å·²è™•ç† Article_625\n",
      "âœ”ï¸ å·²è™•ç† Article_626\n",
      "âœ”ï¸ å·²è™•ç† Article_627\n",
      "âœ”ï¸ å·²è™•ç† Article_628\n",
      "âœ”ï¸ å·²è™•ç† Article_629\n",
      "âœ”ï¸ å·²è™•ç† Article_630\n",
      "âœ”ï¸ å·²è™•ç† Article_631\n",
      "âœ”ï¸ å·²è™•ç† Article_632\n",
      "âœ”ï¸ å·²è™•ç† Article_633\n",
      "âœ”ï¸ å·²è™•ç† Article_634\n",
      "âœ… step2_batch_12.json å·²å®Œæˆä¸¦å„²å­˜è‡³ step2_batches\\step2_batch_12.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç† step1_batches\\step1_batch_2.json â†’ step2_batches\\step2_batch_2.json\n",
      "âœ”ï¸ å·²è™•ç† Article_57\n",
      "âœ”ï¸ å·²è™•ç† Article_58\n",
      "âœ”ï¸ å·²è™•ç† Article_59\n",
      "âœ”ï¸ å·²è™•ç† Article_60\n",
      "âœ”ï¸ å·²è™•ç† Article_61\n",
      "âœ”ï¸ å·²è™•ç† Article_62\n",
      "âœ”ï¸ å·²è™•ç† Article_65\n",
      "âœ”ï¸ å·²è™•ç† Article_66\n",
      "âœ”ï¸ å·²è™•ç† Article_67\n",
      "âœ”ï¸ å·²è™•ç† Article_68\n",
      "âœ”ï¸ å·²è™•ç† Article_69\n",
      "âœ”ï¸ å·²è™•ç† Article_70\n",
      "âœ”ï¸ å·²è™•ç† Article_71\n",
      "âœ”ï¸ å·²è™•ç† Article_72\n",
      "âœ”ï¸ å·²è™•ç† Article_73\n",
      "âœ”ï¸ å·²è™•ç† Article_74\n",
      "âœ”ï¸ å·²è™•ç† Article_75\n",
      "âœ”ï¸ å·²è™•ç† Article_76\n",
      "âœ”ï¸ å·²è™•ç† Article_79\n",
      "âœ”ï¸ å·²è™•ç† Article_80\n",
      "âœ”ï¸ å·²è™•ç† Article_81\n",
      "âœ”ï¸ å·²è™•ç† Article_82\n",
      "âœ”ï¸ å·²è™•ç† Article_83\n",
      "âœ”ï¸ å·²è™•ç† Article_84\n",
      "âœ”ï¸ å·²è™•ç† Article_85\n",
      "âœ”ï¸ å·²è™•ç† Article_86\n",
      "âœ”ï¸ å·²è™•ç† Article_87\n",
      "âœ”ï¸ å·²è™•ç† Article_88\n",
      "âœ”ï¸ å·²è™•ç† Article_89\n",
      "âœ”ï¸ å·²è™•ç† Article_90\n",
      "âœ”ï¸ å·²è™•ç† Article_91\n",
      "âœ”ï¸ å·²è™•ç† Article_92\n",
      "âœ”ï¸ å·²è™•ç† Article_93\n",
      "âœ”ï¸ å·²è™•ç† Article_94\n",
      "âœ”ï¸ å·²è™•ç† Article_96\n",
      "âœ”ï¸ å·²è™•ç† Article_97\n",
      "âœ”ï¸ å·²è™•ç† Article_98\n",
      "âœ”ï¸ å·²è™•ç† Article_99\n",
      "âœ”ï¸ å·²è™•ç† Article_100\n",
      "âœ”ï¸ å·²è™•ç† Article_101\n",
      "âœ”ï¸ å·²è™•ç† Article_102\n",
      "âœ”ï¸ å·²è™•ç† Article_103\n",
      "âœ”ï¸ å·²è™•ç† Article_104\n",
      "âœ”ï¸ å·²è™•ç† Article_105\n",
      "âœ”ï¸ å·²è™•ç† Article_107\n",
      "âœ”ï¸ å·²è™•ç† Article_108\n",
      "âœ”ï¸ å·²è™•ç† Article_109\n",
      "âœ”ï¸ å·²è™•ç† Article_110\n",
      "âœ”ï¸ å·²è™•ç† Article_112\n",
      "âœ”ï¸ å·²è™•ç† Article_113\n",
      "âœ… step2_batch_2.json å·²å®Œæˆä¸¦å„²å­˜è‡³ step2_batches\\step2_batch_2.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç† step1_batches\\step1_batch_3.json â†’ step2_batches\\step2_batch_3.json\n",
      "âœ”ï¸ å·²è™•ç† Article_114\n",
      "âœ”ï¸ å·²è™•ç† Article_115\n",
      "âœ”ï¸ å·²è™•ç† Article_116\n",
      "âœ”ï¸ å·²è™•ç† Article_117\n",
      "âœ”ï¸ å·²è™•ç† Article_118\n",
      "âœ”ï¸ å·²è™•ç† Article_119\n",
      "âœ”ï¸ å·²è™•ç† Article_120\n",
      "âœ”ï¸ å·²è™•ç† Article_121\n",
      "âœ”ï¸ å·²è™•ç† Article_122\n",
      "âœ”ï¸ å·²è™•ç† Article_123\n",
      "âœ”ï¸ å·²è™•ç† Article_124\n",
      "âœ”ï¸ å·²è™•ç† Article_125\n",
      "âœ”ï¸ å·²è™•ç† Article_126\n",
      "âœ”ï¸ å·²è™•ç† Article_127\n",
      "âœ”ï¸ å·²è™•ç† Article_128\n",
      "âœ”ï¸ å·²è™•ç† Article_129\n",
      "âœ”ï¸ å·²è™•ç† Article_130\n",
      "âœ”ï¸ å·²è™•ç† Article_131\n",
      "âœ”ï¸ å·²è™•ç† Article_132\n",
      "âœ”ï¸ å·²è™•ç† Article_133\n",
      "âœ”ï¸ å·²è™•ç† Article_134\n",
      "âœ”ï¸ å·²è™•ç† Article_136\n",
      "âœ”ï¸ å·²è™•ç† Article_137\n",
      "âœ”ï¸ å·²è™•ç† Article_139\n",
      "âœ”ï¸ å·²è™•ç† Article_140\n",
      "âœ”ï¸ å·²è™•ç† Article_141\n",
      "âœ”ï¸ å·²è™•ç† Article_142\n",
      "âœ”ï¸ å·²è™•ç† Article_143\n",
      "âœ”ï¸ å·²è™•ç† Article_144\n",
      "âœ”ï¸ å·²è™•ç† Article_145\n",
      "âœ”ï¸ å·²è™•ç† Article_147\n",
      "âœ”ï¸ å·²è™•ç† Article_148\n",
      "âœ”ï¸ å·²è™•ç† Article_149\n",
      "âœ”ï¸ å·²è™•ç† Article_150\n",
      "âœ”ï¸ å·²è™•ç† Article_151\n",
      "âœ”ï¸ å·²è™•ç† Article_152\n",
      "âœ”ï¸ å·²è™•ç† Article_153\n",
      "âœ”ï¸ å·²è™•ç† Article_154\n",
      "âœ”ï¸ å·²è™•ç† Article_155\n",
      "âœ”ï¸ å·²è™•ç† Article_156\n",
      "âœ”ï¸ å·²è™•ç† Article_157\n",
      "âœ”ï¸ å·²è™•ç† Article_158\n",
      "âœ”ï¸ å·²è™•ç† Article_159\n",
      "âœ”ï¸ å·²è™•ç† Article_160\n",
      "âœ”ï¸ å·²è™•ç† Article_161\n",
      "âœ”ï¸ å·²è™•ç† Article_162\n",
      "âœ”ï¸ å·²è™•ç† Article_163\n",
      "âœ”ï¸ å·²è™•ç† Article_164\n",
      "âœ”ï¸ å·²è™•ç† Article_165\n",
      "âœ”ï¸ å·²è™•ç† Article_166\n",
      "âœ… step2_batch_3.json å·²å®Œæˆä¸¦å„²å­˜è‡³ step2_batches\\step2_batch_3.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç† step1_batches\\step1_batch_4.json â†’ step2_batches\\step2_batch_4.json\n",
      "âœ”ï¸ å·²è™•ç† Article_167\n",
      "âœ”ï¸ å·²è™•ç† Article_168\n",
      "âœ”ï¸ å·²è™•ç† Article_169\n",
      "âœ”ï¸ å·²è™•ç† Article_170\n",
      "âœ”ï¸ å·²è™•ç† Article_171\n",
      "âœ”ï¸ å·²è™•ç† Article_172\n",
      "âœ”ï¸ å·²è™•ç† Article_173\n",
      "âœ”ï¸ å·²è™•ç† Article_174\n",
      "âœ”ï¸ å·²è™•ç† Article_175\n",
      "âœ”ï¸ å·²è™•ç† Article_176\n",
      "âœ”ï¸ å·²è™•ç† Article_177\n",
      "âœ”ï¸ å·²è™•ç† Article_178\n",
      "âœ”ï¸ å·²è™•ç† Article_179\n",
      "âœ”ï¸ å·²è™•ç† Article_180\n",
      "âœ”ï¸ å·²è™•ç† Article_181\n",
      "âœ”ï¸ å·²è™•ç† Article_182\n",
      "âœ”ï¸ å·²è™•ç† Article_183\n",
      "âœ”ï¸ å·²è™•ç† Article_184\n",
      "âœ”ï¸ å·²è™•ç† Article_185\n",
      "âœ”ï¸ å·²è™•ç† Article_186\n",
      "âœ”ï¸ å·²è™•ç† Article_187\n",
      "âœ”ï¸ å·²è™•ç† Article_188\n",
      "âœ”ï¸ å·²è™•ç† Article_189\n",
      "âœ”ï¸ å·²è™•ç† Article_190\n",
      "âœ”ï¸ å·²è™•ç† Article_191\n",
      "âœ”ï¸ å·²è™•ç† Article_192\n",
      "âœ”ï¸ å·²è™•ç† Article_193\n",
      "âœ”ï¸ å·²è™•ç† Article_194\n",
      "âœ”ï¸ å·²è™•ç† Article_195\n",
      "âœ”ï¸ å·²è™•ç† Article_196\n",
      "âœ”ï¸ å·²è™•ç† Article_197\n",
      "âœ”ï¸ å·²è™•ç† Article_198\n",
      "âœ”ï¸ å·²è™•ç† Article_199\n",
      "âœ”ï¸ å·²è™•ç† Article_200\n",
      "âœ”ï¸ å·²è™•ç† Article_201\n",
      "âœ”ï¸ å·²è™•ç† Article_202\n",
      "âœ”ï¸ å·²è™•ç† Article_203\n",
      "âœ”ï¸ å·²è™•ç† Article_204\n",
      "âœ”ï¸ å·²è™•ç† Article_205\n",
      "âœ”ï¸ å·²è™•ç† Article_206\n",
      "âœ”ï¸ å·²è™•ç† Article_207\n",
      "âœ”ï¸ å·²è™•ç† Article_208\n",
      "âœ”ï¸ å·²è™•ç† Article_209\n",
      "âœ”ï¸ å·²è™•ç† Article_210\n",
      "âœ”ï¸ å·²è™•ç† Article_211\n",
      "âœ”ï¸ å·²è™•ç† Article_212\n",
      "âœ”ï¸ å·²è™•ç† Article_213\n",
      "âœ”ï¸ å·²è™•ç† Article_214\n",
      "âœ”ï¸ å·²è™•ç† Article_216\n",
      "âœ”ï¸ å·²è™•ç† Article_217\n",
      "âœ… step2_batch_4.json å·²å®Œæˆä¸¦å„²å­˜è‡³ step2_batches\\step2_batch_4.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç† step1_batches\\step1_batch_5.json â†’ step2_batches\\step2_batch_5.json\n",
      "âœ”ï¸ å·²è™•ç† Article_218\n",
      "âœ”ï¸ å·²è™•ç† Article_219\n",
      "âœ”ï¸ å·²è™•ç† Article_220\n",
      "âœ”ï¸ å·²è™•ç† Article_221\n",
      "âœ”ï¸ å·²è™•ç† Article_222\n",
      "âœ”ï¸ å·²è™•ç† Article_223\n",
      "âœ”ï¸ å·²è™•ç† Article_224\n",
      "âœ”ï¸ å·²è™•ç† Article_225\n",
      "âœ”ï¸ å·²è™•ç† Article_226\n",
      "âœ”ï¸ å·²è™•ç† Article_227\n",
      "âœ”ï¸ å·²è™•ç† Article_228\n",
      "âœ”ï¸ å·²è™•ç† Article_230\n",
      "âœ”ï¸ å·²è™•ç† Article_231\n",
      "âœ”ï¸ å·²è™•ç† Article_232\n",
      "âœ”ï¸ å·²è™•ç† Article_233\n",
      "âœ”ï¸ å·²è™•ç† Article_235\n",
      "âœ”ï¸ å·²è™•ç† Article_236\n",
      "âœ”ï¸ å·²è™•ç† Article_237\n",
      "âœ”ï¸ å·²è™•ç† Article_238\n",
      "âœ”ï¸ å·²è™•ç† Article_239\n",
      "âœ”ï¸ å·²è™•ç† Article_240\n",
      "âœ”ï¸ å·²è™•ç† Article_241\n",
      "âœ”ï¸ å·²è™•ç† Article_242\n",
      "âœ”ï¸ å·²è™•ç† Article_243\n",
      "âœ”ï¸ å·²è™•ç† Article_244\n",
      "âœ”ï¸ å·²è™•ç† Article_245\n",
      "âœ”ï¸ å·²è™•ç† Article_246\n",
      "âœ”ï¸ å·²è™•ç† Article_247\n",
      "âœ”ï¸ å·²è™•ç† Article_248\n",
      "âœ”ï¸ å·²è™•ç† Article_249\n",
      "âœ”ï¸ å·²è™•ç† Article_250\n",
      "âœ”ï¸ å·²è™•ç† Article_251\n",
      "âœ”ï¸ å·²è™•ç† Article_252\n",
      "âœ”ï¸ å·²è™•ç† Article_253\n",
      "âœ”ï¸ å·²è™•ç† Article_255\n",
      "âœ”ï¸ å·²è™•ç† Article_256\n",
      "âœ”ï¸ å·²è™•ç† Article_257\n",
      "âœ”ï¸ å·²è™•ç† Article_258\n",
      "âœ”ï¸ å·²è™•ç† Article_259\n",
      "âœ”ï¸ å·²è™•ç† Article_260\n",
      "âœ”ï¸ å·²è™•ç† Article_262\n",
      "âœ”ï¸ å·²è™•ç† Article_263\n",
      "âœ”ï¸ å·²è™•ç† Article_264\n",
      "âœ”ï¸ å·²è™•ç† Article_265\n",
      "âœ”ï¸ å·²è™•ç† Article_266\n",
      "âœ”ï¸ å·²è™•ç† Article_267\n",
      "âœ”ï¸ å·²è™•ç† Article_268\n",
      "âœ”ï¸ å·²è™•ç† Article_270\n",
      "âœ”ï¸ å·²è™•ç† Article_272\n",
      "âœ”ï¸ å·²è™•ç† Article_273\n",
      "âœ… step2_batch_5.json å·²å®Œæˆä¸¦å„²å­˜è‡³ step2_batches\\step2_batch_5.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç† step1_batches\\step1_batch_6.json â†’ step2_batches\\step2_batch_6.json\n",
      "âœ”ï¸ å·²è™•ç† Article_274\n",
      "âœ”ï¸ å·²è™•ç† Article_275\n",
      "âœ”ï¸ å·²è™•ç† Article_276\n",
      "âœ”ï¸ å·²è™•ç† Article_277\n",
      "âœ”ï¸ å·²è™•ç† Article_278\n",
      "âœ”ï¸ å·²è™•ç† Article_279\n",
      "âœ”ï¸ å·²è™•ç† Article_280\n",
      "âœ”ï¸ å·²è™•ç† Article_281\n",
      "âœ”ï¸ å·²è™•ç† Article_282\n",
      "âœ”ï¸ å·²è™•ç† Article_283\n",
      "âœ”ï¸ å·²è™•ç† Article_284\n",
      "âœ”ï¸ å·²è™•ç† Article_285\n",
      "âœ”ï¸ å·²è™•ç† Article_286\n",
      "âœ”ï¸ å·²è™•ç† Article_288\n",
      "âœ”ï¸ å·²è™•ç† Article_290\n",
      "âœ”ï¸ å·²è™•ç† Article_291\n",
      "âœ”ï¸ å·²è™•ç† Article_292\n",
      "âœ”ï¸ å·²è™•ç† Article_293\n",
      "âœ”ï¸ å·²è™•ç† Article_294\n",
      "âœ”ï¸ å·²è™•ç† Article_296\n",
      "âœ”ï¸ å·²è™•ç† Article_297\n",
      "âœ”ï¸ å·²è™•ç† Article_298\n",
      "âœ”ï¸ å·²è™•ç† Article_299\n",
      "âœ”ï¸ å·²è™•ç† Article_300\n",
      "âœ”ï¸ å·²è™•ç† Article_301\n",
      "âœ”ï¸ å·²è™•ç† Article_302\n",
      "âœ”ï¸ å·²è™•ç† Article_303\n",
      "âœ”ï¸ å·²è™•ç† Article_304\n",
      "âœ”ï¸ å·²è™•ç† Article_305\n",
      "âœ”ï¸ å·²è™•ç† Article_306\n",
      "âœ”ï¸ å·²è™•ç† Article_307\n",
      "âœ”ï¸ å·²è™•ç† Article_308\n",
      "âœ”ï¸ å·²è™•ç† Article_309\n",
      "âœ”ï¸ å·²è™•ç† Article_310\n",
      "âœ”ï¸ å·²è™•ç† Article_311\n",
      "âœ”ï¸ å·²è™•ç† Article_312\n",
      "âœ”ï¸ å·²è™•ç† Article_313\n",
      "âœ”ï¸ å·²è™•ç† Article_314\n",
      "âœ”ï¸ å·²è™•ç† Article_315\n",
      "âœ”ï¸ å·²è™•ç† Article_316\n",
      "âœ”ï¸ å·²è™•ç† Article_317\n",
      "âœ”ï¸ å·²è™•ç† Article_318\n",
      "âœ”ï¸ å·²è™•ç† Article_319\n",
      "âœ”ï¸ å·²è™•ç† Article_320\n",
      "âœ”ï¸ å·²è™•ç† Article_321\n",
      "âœ”ï¸ å·²è™•ç† Article_322\n",
      "âœ”ï¸ å·²è™•ç† Article_323\n",
      "âœ”ï¸ å·²è™•ç† Article_324\n",
      "âœ”ï¸ å·²è™•ç† Article_325\n",
      "âœ”ï¸ å·²è™•ç† Article_326\n",
      "âœ… step2_batch_6.json å·²å®Œæˆä¸¦å„²å­˜è‡³ step2_batches\\step2_batch_6.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç† step1_batches\\step1_batch_7.json â†’ step2_batches\\step2_batch_7.json\n",
      "âœ”ï¸ å·²è™•ç† Article_327\n",
      "âœ”ï¸ å·²è™•ç† Article_328\n",
      "âœ”ï¸ å·²è™•ç† Article_329\n",
      "âœ”ï¸ å·²è™•ç† Article_330\n",
      "âœ”ï¸ å·²è™•ç† Article_331\n",
      "âœ”ï¸ å·²è™•ç† Article_332\n",
      "âœ”ï¸ å·²è™•ç† Article_333\n",
      "âœ”ï¸ å·²è™•ç† Article_334\n",
      "âœ”ï¸ å·²è™•ç† Article_335\n",
      "âœ”ï¸ å·²è™•ç† Article_336\n",
      "âœ”ï¸ å·²è™•ç† Article_337\n",
      "âœ”ï¸ å·²è™•ç† Article_338\n",
      "âœ”ï¸ å·²è™•ç† Article_339\n",
      "âœ”ï¸ å·²è™•ç† Article_340\n",
      "âœ”ï¸ å·²è™•ç† Article_341\n",
      "âœ”ï¸ å·²è™•ç† Article_342\n",
      "âœ”ï¸ å·²è™•ç† Article_343\n",
      "âœ”ï¸ å·²è™•ç† Article_344\n",
      "âœ”ï¸ å·²è™•ç† Article_345\n",
      "âœ”ï¸ å·²è™•ç† Article_346\n",
      "âœ”ï¸ å·²è™•ç† Article_347\n",
      "âœ”ï¸ å·²è™•ç† Article_348\n",
      "âœ”ï¸ å·²è™•ç† Article_349\n",
      "âœ”ï¸ å·²è™•ç† Article_350\n",
      "âœ”ï¸ å·²è™•ç† Article_351\n",
      "âœ”ï¸ å·²è™•ç† Article_352\n",
      "âœ”ï¸ å·²è™•ç† Article_355\n",
      "âœ”ï¸ å·²è™•ç† Article_356\n",
      "âœ”ï¸ å·²è™•ç† Article_357\n",
      "âœ”ï¸ å·²è™•ç† Article_358\n",
      "âœ”ï¸ å·²è™•ç† Article_359\n",
      "âœ”ï¸ å·²è™•ç† Article_360\n",
      "âœ”ï¸ å·²è™•ç† Article_361\n",
      "âœ”ï¸ å·²è™•ç† Article_363\n",
      "âœ”ï¸ å·²è™•ç† Article_364\n",
      "âœ”ï¸ å·²è™•ç† Article_365\n",
      "âœ”ï¸ å·²è™•ç† Article_366\n",
      "âœ”ï¸ å·²è™•ç† Article_367\n",
      "âœ”ï¸ å·²è™•ç† Article_368\n",
      "âœ”ï¸ å·²è™•ç† Article_369\n",
      "âœ”ï¸ å·²è™•ç† Article_370\n",
      "âœ”ï¸ å·²è™•ç† Article_371\n",
      "âœ”ï¸ å·²è™•ç† Article_373\n",
      "âœ”ï¸ å·²è™•ç† Article_374\n",
      "âœ”ï¸ å·²è™•ç† Article_375\n",
      "âœ”ï¸ å·²è™•ç† Article_376\n",
      "âœ”ï¸ å·²è™•ç† Article_377\n",
      "âœ”ï¸ å·²è™•ç† Article_379\n",
      "âœ”ï¸ å·²è™•ç† Article_380\n",
      "âœ”ï¸ å·²è™•ç† Article_381\n",
      "âœ… step2_batch_7.json å·²å®Œæˆä¸¦å„²å­˜è‡³ step2_batches\\step2_batch_7.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç† step1_batches\\step1_batch_8.json â†’ step2_batches\\step2_batch_8.json\n",
      "âœ”ï¸ å·²è™•ç† Article_382\n",
      "âœ”ï¸ å·²è™•ç† Article_383\n",
      "âœ”ï¸ å·²è™•ç† Article_384\n",
      "âœ”ï¸ å·²è™•ç† Article_385\n",
      "âœ”ï¸ å·²è™•ç† Article_386\n",
      "âœ”ï¸ å·²è™•ç† Article_387\n",
      "âœ”ï¸ å·²è™•ç† Article_388\n",
      "âœ”ï¸ å·²è™•ç† Article_389\n",
      "âœ”ï¸ å·²è™•ç† Article_390\n",
      "âœ”ï¸ å·²è™•ç† Article_391\n",
      "âœ”ï¸ å·²è™•ç† Article_392\n",
      "âœ”ï¸ å·²è™•ç† Article_393\n",
      "âœ”ï¸ å·²è™•ç† Article_394\n",
      "âœ”ï¸ å·²è™•ç† Article_395\n",
      "âœ”ï¸ å·²è™•ç† Article_396\n",
      "âœ”ï¸ å·²è™•ç† Article_397\n",
      "âœ”ï¸ å·²è™•ç† Article_398\n",
      "âœ”ï¸ å·²è™•ç† Article_399\n",
      "âœ”ï¸ å·²è™•ç† Article_400\n",
      "âœ”ï¸ å·²è™•ç† Article_401\n",
      "âœ”ï¸ å·²è™•ç† Article_402\n",
      "âœ”ï¸ å·²è™•ç† Article_403\n",
      "âœ”ï¸ å·²è™•ç† Article_404\n",
      "âœ”ï¸ å·²è™•ç† Article_405\n",
      "âœ”ï¸ å·²è™•ç† Article_406\n",
      "âœ”ï¸ å·²è™•ç† Article_409\n",
      "âœ”ï¸ å·²è™•ç† Article_410\n",
      "âœ”ï¸ å·²è™•ç† Article_411\n",
      "âœ”ï¸ å·²è™•ç† Article_412\n",
      "âœ”ï¸ å·²è™•ç† Article_413\n",
      "âœ”ï¸ å·²è™•ç† Article_414\n",
      "âœ”ï¸ å·²è™•ç† Article_415\n",
      "âœ”ï¸ å·²è™•ç† Article_416\n",
      "âœ”ï¸ å·²è™•ç† Article_417\n",
      "âœ”ï¸ å·²è™•ç† Article_418\n",
      "âœ”ï¸ å·²è™•ç† Article_419\n",
      "âœ”ï¸ å·²è™•ç† Article_420\n",
      "âœ”ï¸ å·²è™•ç† Article_421\n",
      "âœ”ï¸ å·²è™•ç† Article_422\n",
      "âœ”ï¸ å·²è™•ç† Article_423\n",
      "âœ”ï¸ å·²è™•ç† Article_424\n",
      "âœ”ï¸ å·²è™•ç† Article_425\n",
      "âœ”ï¸ å·²è™•ç† Article_426\n",
      "âœ”ï¸ å·²è™•ç† Article_427\n",
      "âœ”ï¸ å·²è™•ç† Article_428\n",
      "âœ”ï¸ å·²è™•ç† Article_429\n",
      "âœ”ï¸ å·²è™•ç† Article_430\n",
      "âœ”ï¸ å·²è™•ç† Article_431\n",
      "âœ”ï¸ å·²è™•ç† Article_433\n",
      "âœ”ï¸ å·²è™•ç† Article_434\n",
      "âœ… step2_batch_8.json å·²å®Œæˆä¸¦å„²å­˜è‡³ step2_batches\\step2_batch_8.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç† step1_batches\\step1_batch_9.json â†’ step2_batches\\step2_batch_9.json\n",
      "âœ”ï¸ å·²è™•ç† Article_435\n",
      "âœ”ï¸ å·²è™•ç† Article_436\n",
      "âœ”ï¸ å·²è™•ç† Article_437\n",
      "âœ”ï¸ å·²è™•ç† Article_438\n",
      "âœ”ï¸ å·²è™•ç† Article_439\n",
      "âœ”ï¸ å·²è™•ç† Article_440\n",
      "âœ”ï¸ å·²è™•ç† Article_441\n",
      "âœ”ï¸ å·²è™•ç† Article_442\n",
      "âœ”ï¸ å·²è™•ç† Article_443\n",
      "âœ”ï¸ å·²è™•ç† Article_445\n",
      "âœ”ï¸ å·²è™•ç† Article_446\n",
      "âœ”ï¸ å·²è™•ç† Article_448\n",
      "âœ”ï¸ å·²è™•ç† Article_449\n",
      "âœ”ï¸ å·²è™•ç† Article_450\n",
      "âœ”ï¸ å·²è™•ç† Article_451\n",
      "âœ”ï¸ å·²è™•ç† Article_452\n",
      "âœ”ï¸ å·²è™•ç† Article_453\n",
      "âœ”ï¸ å·²è™•ç† Article_454\n",
      "âœ”ï¸ å·²è™•ç† Article_455\n",
      "âœ”ï¸ å·²è™•ç† Article_456\n",
      "âœ”ï¸ å·²è™•ç† Article_457\n",
      "âœ”ï¸ å·²è™•ç† Article_458\n",
      "âœ”ï¸ å·²è™•ç† Article_459\n",
      "âœ”ï¸ å·²è™•ç† Article_460\n",
      "âœ”ï¸ å·²è™•ç† Article_461\n",
      "âœ”ï¸ å·²è™•ç† Article_463\n",
      "âœ”ï¸ å·²è™•ç† Article_465\n",
      "âœ”ï¸ å·²è™•ç† Article_466\n",
      "âœ”ï¸ å·²è™•ç† Article_467\n",
      "âœ”ï¸ å·²è™•ç† Article_468\n",
      "âœ”ï¸ å·²è™•ç† Article_469\n",
      "âœ”ï¸ å·²è™•ç† Article_470\n",
      "âœ”ï¸ å·²è™•ç† Article_471\n",
      "âœ”ï¸ å·²è™•ç† Article_473\n",
      "âœ”ï¸ å·²è™•ç† Article_474\n",
      "âœ”ï¸ å·²è™•ç† Article_475\n",
      "âœ”ï¸ å·²è™•ç† Article_476\n",
      "âœ”ï¸ å·²è™•ç† Article_477\n",
      "âœ”ï¸ å·²è™•ç† Article_478\n",
      "âœ”ï¸ å·²è™•ç† Article_479\n",
      "âœ”ï¸ å·²è™•ç† Article_480\n",
      "âœ”ï¸ å·²è™•ç† Article_481\n",
      "âœ”ï¸ å·²è™•ç† Article_483\n",
      "âœ”ï¸ å·²è™•ç† Article_484\n",
      "API èª¿ç”¨éŒ¯èª¤: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "âœ”ï¸ å·²è™•ç† Article_485\n",
      "API èª¿ç”¨éŒ¯èª¤: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "âœ”ï¸ å·²è™•ç† Article_486\n",
      "API èª¿ç”¨éŒ¯èª¤: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "âœ”ï¸ å·²è™•ç† Article_487\n",
      "API èª¿ç”¨éŒ¯èª¤: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "âœ”ï¸ å·²è™•ç† Article_488\n",
      "API èª¿ç”¨éŒ¯èª¤: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "âœ”ï¸ å·²è™•ç† Article_489\n",
      "API èª¿ç”¨éŒ¯èª¤: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "âœ”ï¸ å·²è™•ç† Article_490\n",
      "âœ… step2_batch_9.json å·²å®Œæˆä¸¦å„²å­˜è‡³ step2_batches\\step2_batch_9.json\n",
      "\n",
      "ğŸ‰ Step 2 å…¨éƒ¨æ‰¹æ¬¡è™•ç†å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "step2_prompt = \"\"\"You are a sociology professor with 30 years of experience analyzing Asian racism in the United States.\n",
    "\n",
    "**Step 2: Extract Related Sentences**\n",
    "\n",
    "Based on the named entities identified in **Step 1**, extract **all relevant complete sentences** from the text for each entity.\n",
    "\n",
    "**For each named entity:**\n",
    "1. Use the `\"reference\"` field (the phrase as it appears in the article) to identify relevant sentences.\n",
    "2. Identify **all complete, verbatim sentences** in the text that mention or describe the entityâ€™s **involvement, action, reaction, statement, or experience** related to anti-Asian hate (directly or indirectly).\n",
    "3. Do **not paraphrase** or summarize. Use the **exact wording** from the text.\n",
    "4. If no relevant sentence is found, set `\"relevant_sentences\": []`.\n",
    "5. Return results **grouped by entity name**, exactly matching the names used in Step 1.\n",
    "6. Include the following structured metadata for each entity:\n",
    "   - `\"entity_type\"`:  \n",
    "     - For individuals: the **social role** (e.g., \"politician\", \"celebrity\", \"victim\")  \n",
    "     - For organizations: the **institutional category** (e.g., \"law_enforcement_agency\", \"ngo_or_advocacy_group\")\n",
    "   - `\"asian_status\"`:  \n",
    "     - For individuals: **\"Asian\"**, **\"Non-Asian\"**, or **\"Cannot be inferred\"**  \n",
    "     - For organizations: Always **\"Not applicable\"**\n",
    "\n",
    "**Note:** These sentences will later be used to infer **behavioral reactions** and **emotional responses**, so include **any sentence** that provides context about what the entity did, said, or experienced.\n",
    "\n",
    "### Output format (JSON):\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"Entity Name\": {\n",
    "    \"entity_type\": \"e.g., politician, law_enforcement_agency\",\n",
    "    \"asian_status\": \"Asian / Non-Asian / Cannot be inferred / Not applicable\",\n",
    "    \"relevant_sentences\": [\n",
    "      \"Sentence 1 from the article.\",\n",
    "      \"Sentence 2 from the article.\"\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# å»ºç«‹è¼¸å‡ºè³‡æ–™å¤¾\n",
    "output_dir = \"step2_batches\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# æ‰¾å‡º Step 1 çš„æ‰€æœ‰æ‰¹æ¬¡æª”æ¡ˆ\n",
    "step1_files = sorted(glob.glob(\"step1_batches/step1_batch_*.json\"))\n",
    "\n",
    "for step1_file in step1_files:\n",
    "    batch_name = os.path.basename(step1_file).replace(\"step1_\", \"step2_\")\n",
    "    output_file = os.path.join(output_dir, batch_name)\n",
    "\n",
    "    # å¦‚æœå·²ç¶“æœ‰ Step 2 çš„çµæœï¼Œå°±è·³é\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"â­ï¸ {batch_name} å·²å­˜åœ¨ï¼Œè·³é\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nğŸš€ é–‹å§‹è™•ç† {step1_file} â†’ {output_file}\")\n",
    "\n",
    "    # è®€å– Step 1 çš„çµæœ\n",
    "    with open(step1_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        step1_result = json.load(f)\n",
    "\n",
    "    step2_result = {}\n",
    "\n",
    "    # é€ç¯‡æ–‡ç« è™•ç†\n",
    "    for title, step1_text in step1_result.items():\n",
    "        content = articles.get(title, \"\")  # å¾åŸå§‹æ–‡ç«  dict æ‹¿å…§å®¹\n",
    "\n",
    "        full_prompt = (\n",
    "            step2_prompt +\n",
    "            f\"\\n\\nStep 1 Results:\\n{step1_text}\" +\n",
    "            f\"\\n\\nOriginal Article Text:\\n{content}\"\n",
    "        )\n",
    "\n",
    "        response = get_response(full_prompt)\n",
    "        step2_result[title] = response\n",
    "        print(f\"âœ”ï¸ å·²è™•ç† {title}\")\n",
    "\n",
    "    # å„²å­˜é€™ä¸€æ‰¹çš„ Step 2 çµæœ\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(step2_result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"âœ… {batch_name} å·²å®Œæˆä¸¦å„²å­˜è‡³ {output_file}\")\n",
    "\n",
    "print(\"\\nğŸ‰ Step 2 å…¨éƒ¨æ‰¹æ¬¡è™•ç†å®Œæˆï¼\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc2b10c2-8265-45e8-8c31-e98d7f31ab67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ é‡æ–°è™•ç† Article_485\n",
      "ğŸ”„ é‡æ–°è™•ç† Article_486\n",
      "ğŸ”„ é‡æ–°è™•ç† Article_487\n",
      "ğŸ”„ é‡æ–°è™•ç† Article_488\n",
      "ğŸ”„ é‡æ–°è™•ç† Article_489\n",
      "ğŸ”„ é‡æ–°è™•ç† Article_490\n",
      "âœ… å·²æ›´æ–° step2_batches/step2_batch_9.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def rerun_incomplete_step2(step1_file, step2_file):\n",
    "    # è®€å– Step1 çš„çµæœ\n",
    "    with open(step1_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        step1_result = json.load(f)\n",
    "\n",
    "    # å¦‚æœ step2_file å·²å­˜åœ¨ï¼Œå…ˆè®€å–ï¼›å¦å‰‡æ–°å»º\n",
    "    if os.path.exists(step2_file):\n",
    "        with open(step2_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            step2_result = json.load(f)\n",
    "    else:\n",
    "        step2_result = {}\n",
    "\n",
    "    updated = False\n",
    "\n",
    "    for title, step1_text in step1_result.items():\n",
    "        # åªè™•ç†ç©ºçš„ or ç¼ºå¤±çš„\n",
    "        if not step2_result.get(title):\n",
    "            content = articles.get(title, \"\")  # åŸå§‹æ–‡ç« \n",
    "            full_prompt = (\n",
    "                step2_prompt +\n",
    "                f\"\\n\\nStep 1 Results:\\n{step1_text}\" +\n",
    "                f\"\\n\\nOriginal Article Text:\\n{content}\"\n",
    "            )\n",
    "            response = get_response(full_prompt)\n",
    "            step2_result[title] = response\n",
    "            print(f\"ğŸ”„ é‡æ–°è™•ç† {title}\")\n",
    "            updated = True\n",
    "\n",
    "    if updated:\n",
    "        with open(step2_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(step2_result, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"âœ… å·²æ›´æ–° {step2_file}\")\n",
    "    else:\n",
    "        print(\"ğŸ‘Œ æ²’æœ‰éœ€è¦è£œè·‘çš„æ–‡ç« ï¼Œå…¨æ•¸å®Œæˆ\")\n",
    "\n",
    "# ä½¿ç”¨ç¯„ä¾‹\n",
    "rerun_incomplete_step2(\n",
    "    step1_file=\"step1_batches/step1_batch_9.json\",\n",
    "    step2_file=\"step2_batches/step2_batch_9.json\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e1c2ee9-87b3-42bb-8bef-d271575ff76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 1/12: step2_batches\\step2_batch_1.json\n",
      "âœ… æ‰¹æ¬¡ 1/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step3_batches\\step3_batch_1.json\n",
      "   Gate stats: {'no': 426, 'yes': 174}\n",
      "   Label stats: {'Cannot be inferred': 466, 'Support Asian Americans': 86, 'Government takes actions to stop AAPI hate': 30, 'Advocacy/take actions for changes': 4, 'Attending marches/rallies': 1, 'Politicians initiated anti-Asian hate and racism': 4, 'Color blind/minimizing racism': 1, 'Fostering conversations about anti-Asian hate': 2, 'Useless law enforcement': 2, 'Hiring security guards': 2, 'Educating students': 1, 'Videotaping/confronting harasser/attacker': 1}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 2/12: step2_batches\\step2_batch_10.json\n",
      "âœ… æ‰¹æ¬¡ 2/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step3_batches\\step3_batch_10.json\n",
      "   Gate stats: {'no': 285, 'yes': 131}\n",
      "   Label stats: {'Cannot be inferred': 296, 'Support Asian Americans': 56, 'Government takes actions to stop AAPI hate': 51, 'Attending marches/rallies': 2, 'Advocacy/take actions for changes': 5, 'Politicians initiated anti-Asian hate and racism': 2, 'Color blind/minimizing racism': 1, 'Videotaping/confronting harasser/attacker': 2, 'Educating students': 1}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 3/12: step2_batches\\step2_batch_11.json\n",
      "âœ… æ‰¹æ¬¡ 3/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step3_batches\\step3_batch_11.json\n",
      "   Gate stats: {'yes': 164, 'no': 411}\n",
      "   Label stats: {'Support Asian Americans': 80, 'Cannot be inferred': 431, 'Government takes actions to stop AAPI hate': 46, 'Videotaping/confronting harasser/attacker': 5, 'Hiring security guards': 1, 'Politicians initiated anti-Asian hate and racism': 1, 'Speaking up on social media': 2, 'Advocacy/take actions for changes': 5, 'Attending marches/rallies': 1, 'Useless law enforcement': 1, 'Color blind/minimizing racism': 1, 'Feeling hopeless or support AAPI being not enough': 1}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 4/12: step2_batches\\step2_batch_12.json\n",
      "âœ… æ‰¹æ¬¡ 4/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step3_batches\\step3_batch_12.json\n",
      "   Gate stats: {'yes': 105, 'no': 237}\n",
      "   Label stats: {'Government takes actions to stop AAPI hate': 34, 'Support Asian Americans': 52, 'Cannot be inferred': 246, 'Advocacy/take actions for changes': 7, 'Videotaping/confronting harasser/attacker': 2, 'Politicians initiated anti-Asian hate and racism': 1}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 5/12: step2_batches\\step2_batch_2.json\n",
      "âœ… æ‰¹æ¬¡ 5/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step3_batches\\step3_batch_2.json\n",
      "   Gate stats: {'no': 343, 'yes': 123}\n",
      "   Label stats: {'Cannot be inferred': 369, 'Support Asian Americans': 49, 'Useless law enforcement': 1, 'Government takes actions to stop AAPI hate': 29, 'Politicians initiated anti-Asian hate and racism': 2, 'Attending marches/rallies': 2, 'Advocacy/take actions for changes': 9, 'Feeling hopeless or support AAPI being not enough': 2, 'Educating students': 1, 'Not confronting attacker/harasser or not reporting': 1, 'Color blind/minimizing racism': 1}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 6/12: step2_batches\\step2_batch_3.json\n",
      "âœ… æ‰¹æ¬¡ 6/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step3_batches\\step3_batch_3.json\n",
      "   Gate stats: {'no': 376, 'yes': 129}\n",
      "   Label stats: {'Cannot be inferred': 394, 'Support Asian Americans': 72, 'Advocacy/take actions for changes': 3, 'Government takes actions to stop AAPI hate': 24, 'Educating students': 2, 'Calling for being united': 2, 'Videotaping/confronting harasser/attacker': 1, 'Politicians initiated anti-Asian hate and racism': 4, 'Fostering conversations about anti-Asian hate': 2, 'Color blind/minimizing racism': 1}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 7/12: step2_batches\\step2_batch_4.json\n",
      "âœ… æ‰¹æ¬¡ 7/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step3_batches\\step3_batch_4.json\n",
      "   Gate stats: {'no': 329, 'yes': 154}\n",
      "   Label stats: {'Cannot be inferred': 361, 'Support Asian Americans': 79, 'Fostering conversations about anti-Asian hate': 2, 'Government takes actions to stop AAPI hate': 26, 'Color blind/minimizing racism': 2, 'Advocacy/take actions for changes': 3, 'Speaking up on social media': 2, 'Useless law enforcement': 1, 'Politicians initiated anti-Asian hate and racism': 4, 'Educating students': 3}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 8/12: step2_batches\\step2_batch_5.json\n",
      "âœ… æ‰¹æ¬¡ 8/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step3_batches\\step3_batch_5.json\n",
      "   Gate stats: {'yes': 139, 'no': 335}\n",
      "   Label stats: {'Support Asian Americans': 77, 'Cannot be inferred': 350, 'Useless law enforcement': 1, 'Government takes actions to stop AAPI hate': 20, 'Videotaping/confronting harasser/attacker': 3, 'Advocacy/take actions for changes': 10, 'Attending marches/rallies': 5, 'Educating students': 2, 'Politicians initiated anti-Asian hate and racism': 2, 'Hiring security guards': 1, 'Fostering conversations about anti-Asian hate': 2, 'Feeling hopeless or support AAPI being not enough': 1}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 9/12: step2_batches\\step2_batch_6.json\n",
      "âœ… æ‰¹æ¬¡ 9/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step3_batches\\step3_batch_6.json\n",
      "   Gate stats: {'yes': 141, 'no': 368}\n",
      "   Label stats: {'Support Asian Americans': 90, 'Cannot be inferred': 397, 'Attending marches/rallies': 2, 'Advocacy/take actions for changes': 2, 'Government takes actions to stop AAPI hate': 14, 'Politicians initiated anti-Asian hate and racism': 1, 'Useless law enforcement': 2, 'Fostering conversations about anti-Asian hate': 1}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 10/12: step2_batches\\step2_batch_7.json\n",
      "âœ… æ‰¹æ¬¡ 10/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step3_batches\\step3_batch_7.json\n",
      "   Gate stats: {'yes': 87, 'no': 272}\n",
      "   Label stats: {'Cannot be inferred': 291, 'Support Asian Americans': 31, 'Government takes actions to stop AAPI hate': 28, 'Politicians initiated anti-Asian hate and racism': 2, 'Advocacy/take actions for changes': 6, 'Fostering conversations about anti-Asian hate': 1}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 11/12: step2_batches\\step2_batch_8.json\n",
      "âœ… æ‰¹æ¬¡ 11/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step3_batches\\step3_batch_8.json\n",
      "   Gate stats: {'no': 344, 'yes': 113}\n",
      "   Label stats: {'Cannot be inferred': 376, 'Advocacy/take actions for changes': 11, 'Government takes actions to stop AAPI hate': 27, 'Support Asian Americans': 36, 'Educating students': 3, 'Useless law enforcement': 1, 'Politicians initiated anti-Asian hate and racism': 1, 'Videotaping/confronting harasser/attacker': 2}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 12/12: step2_batches\\step2_batch_9.json\n",
      "âœ… æ‰¹æ¬¡ 12/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step3_batches\\step3_batch_9.json\n",
      "   Gate stats: {'no': 392, 'yes': 115}\n",
      "   Label stats: {'Cannot be inferred': 422, 'Support Asian Americans': 35, 'Government takes actions to stop AAPI hate': 34, 'Advocacy/take actions for changes': 6, 'Useless law enforcement': 4, 'Educating students': 3, 'Fostering conversations about anti-Asian hate': 1, 'Attending marches/rallies': 1, 'Videotaping/confronting harasser/attacker': 1}\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "from collections import Counter\n",
    "from typing import Dict, Any\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# å·¥å…·ï¼šJSON å¯¬é¬†è§£æ + å¥å­æ­£è¦åŒ–\n",
    "# ==============================\n",
    "def strip_code_fences(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return s\n",
    "    s = s.strip()\n",
    "    m = re.search(r\"```(?:json)?\\s*(.*?)```\", s, flags=re.S | re.I)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "    return s\n",
    "\n",
    "def parse_json_loose(s: str):\n",
    "    if not isinstance(s, str):\n",
    "        return None\n",
    "    s = strip_code_fences(s).strip()\n",
    "    if not s:\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        pass\n",
    "    m = re.search(r\"\\{.*\\}\", s, flags=re.S)\n",
    "    if m:\n",
    "        try:\n",
    "            return json.loads(m.group(0))\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def to_text(x: Any) -> str:\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    if isinstance(x, str):\n",
    "        return x\n",
    "    if isinstance(x, list):\n",
    "        return \"\\n\".join(str(item) for item in x)\n",
    "    if isinstance(x, dict):\n",
    "        if \"relevant_sentences\" in x:\n",
    "            return to_text(x[\"relevant_sentences\"])\n",
    "        return \"\\n\".join(to_text(v) for v in x.values())\n",
    "    return str(x)\n",
    "\n",
    "def parse_model_json(s: str, default: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return default\n",
    "    s2 = strip_code_fences(s)\n",
    "    try:\n",
    "        return json.loads(s2)\n",
    "    except Exception:\n",
    "        m = re.search(r\"\\{.*\\}\", s2, flags=re.S)\n",
    "        if m:\n",
    "            try:\n",
    "                return json.loads(m.group(0))\n",
    "            except Exception:\n",
    "                return default\n",
    "        return default\n",
    "\n",
    "# ==============================\n",
    "# Step2 çµæœæ­£è¦åŒ–\n",
    "# ==============================\n",
    "def normalize_step2_result(title: str, raw_obj) -> Dict[str, Dict[str, str]]:\n",
    "    if isinstance(raw_obj, dict):\n",
    "        if all(isinstance(v, dict) for v in raw_obj.values()):\n",
    "            out = {}\n",
    "            for k, v in raw_obj.items():\n",
    "                out[k] = {\n",
    "                    \"entity_type\": v.get(\"entity_type\", \"\"),\n",
    "                    \"asian_status\": v.get(\"asian_status\", \"\"),\n",
    "                    \"relevant_sentences\": to_text(v.get(\"relevant_sentences\"))\n",
    "                }\n",
    "            return out\n",
    "    if isinstance(raw_obj, str):\n",
    "        parsed = parse_json_loose(raw_obj)\n",
    "        if parsed:\n",
    "            return normalize_step2_result(title, parsed)\n",
    "    return {}\n",
    "\n",
    "# ==============================\n",
    "# Prompts\n",
    "# ==============================\n",
    "def build_gate_prompt(step2_sentences: str) -> str:\n",
    "    return f\"\"\"You are a sociology professor analyzing reactions to anti-Asian hate.\n",
    "\n",
    "Task: Decide if the extracted sentences show any OBSERVABLE reaction (action or inaction) to anti-Asian hate by the entity.\n",
    "\n",
    "Rules:\n",
    "- Observable = concrete action/inaction or explicit public stance (e.g., speaking up, condemning, organizing, reporting, policy ask, government action, refusing to act).\n",
    "- Pure emotions/concerns are NOT reactions.\n",
    "- Pure incident descriptions are NOT reactions.\n",
    "- Use ONLY the exact `relevant_sentences`.\n",
    "\n",
    "Output JSON:\n",
    "{{\n",
    "  \"has_reaction\": \"yes\" | \"no\",\n",
    "  \"evidence\": \"Exact sentence(s) from relevant_sentences (or empty if no).\"\n",
    "}}\n",
    "\n",
    "relevant_sentences:\n",
    "{step2_sentences}\n",
    "\"\"\"\n",
    "\n",
    "def build_classifier_prompt(entity_type: str, asian_status: str, step2_sentences: str) -> str:\n",
    "    return f\"\"\"You are a sociology professor with 30 years of experience analyzing anti-Asian racism.\n",
    "\n",
    "Task: Classify the entityâ€™s REACTION strictly using the Reaction Concept Tree. Use ONLY `relevant_sentences` as evidence. Do NOT infer emotions. Do NOT paraphrase.\n",
    "\n",
    "Reaction Concept Tree:\n",
    "- Support Asian Americans\n",
    "  - Attending marches/rallies\n",
    "  - Speaking up on social media\n",
    "  - Calling for being united\n",
    "  - Educating students\n",
    "  - Fostering conversations about anti-Asian hate\n",
    "  - Hiring security guards\n",
    "  - Providing shopkeepers with air horns\n",
    "  - Rewarding the public to report the info about the suspects\n",
    "- Advocacy/take actions for changes\n",
    "- Politicians initiated anti-Asian hate and racism\n",
    "- Undermining human rights\n",
    "- Color blind/minimizing racism\n",
    "- Youth as not an excuse\n",
    "- Videotaping/confronting harasser/attacker\n",
    "- Sex (sexual) addiction\n",
    "- Religion as a reason\n",
    "- Feeling hopeless or support AAPI being not enough\n",
    "- Not confronting attacker/harasser or not reporting\n",
    "- Useless law enforcement\n",
    "  - Did not take a report on Anti-Asian hate crime\n",
    "  - Did not often patrol the streets\n",
    "- Government takes actions to stop AAPI hate\n",
    "  - Installing hotlines\n",
    "  - Launching a hate-crime task force\n",
    "  - Increasing patrols\n",
    "  - Organizing a town hall\n",
    "\n",
    "Strict Rules:\n",
    "1) Pure concerns/worries â‰  reaction; return \"Cannot be inferred\".\n",
    "2) Arrests/charges/prosecutions â‡’ â€œGovernment takes actionsâ€¦â€.\n",
    "3) Explicit condemnation â‡’ â€œSupport Asian Americansâ€.\n",
    "4) If no clear reaction, return \"Cannot be inferred\".\n",
    "5) Do NOT invent labels.\n",
    "6) Always choose the most specific subcategory.\n",
    "\n",
    "entity_type: {entity_type}\n",
    "asian_status: {asian_status}\n",
    "\n",
    "relevant_sentences:\n",
    "{step2_sentences}\n",
    "\n",
    "Output JSON:\n",
    "{{\n",
    "  \"reaction\": \"<one label from the tree or 'Cannot be inferred'>\",\n",
    "  \"reaction_reason\": \"Exact sentence(s) from relevant_sentences\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "# ==============================\n",
    "# ä¸»æµç¨‹ï¼šå¾ Step2 â†’ Step3\n",
    "# ==============================\n",
    "def run_step3_from_step2(step2_dir=\"step2_batches\", step3_dir=\"step3_batches\"):\n",
    "    os.makedirs(step3_dir, exist_ok=True)\n",
    "    step2_files = sorted(glob.glob(os.path.join(step2_dir, \"step2_batch_*.json\")))\n",
    "    total_batches = len(step2_files)\n",
    "\n",
    "    for idx, step2_file in enumerate(step2_files, start=1):\n",
    "        batch_name = os.path.basename(step2_file).replace(\"step2_\", \"step3_\")\n",
    "        out_path = os.path.join(step3_dir, batch_name)\n",
    "\n",
    "        # æ–·é»çºŒè·‘\n",
    "        if os.path.exists(out_path):\n",
    "            print(f\"â­ï¸ æ‰¹æ¬¡ {idx}/{total_batches} {batch_name} å·²å­˜åœ¨ï¼Œè·³é\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ {idx}/{total_batches}: {step2_file}\")\n",
    "\n",
    "        with open(step2_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            step2_batch_result = json.load(f)\n",
    "\n",
    "        step3_batch_result = {}\n",
    "        gate_stats = Counter()\n",
    "        label_stats = Counter()\n",
    "\n",
    "        for title, raw in step2_batch_result.items():\n",
    "            entities = normalize_step2_result(title, raw)\n",
    "            entity_outputs = {}\n",
    "\n",
    "            if not entities:\n",
    "                step3_batch_result[title] = entity_outputs\n",
    "                continue\n",
    "\n",
    "            for entity, meta in entities.items():\n",
    "                entity_type = meta.get(\"entity_type\", \"\")\n",
    "                asian_status = meta.get(\"asian_status\", \"\")\n",
    "                relevant_sentences = to_text(meta.get(\"relevant_sentences\", \"\")).strip()\n",
    "\n",
    "                # --- 3A: Gate ---\n",
    "                gate_prompt = build_gate_prompt(relevant_sentences)\n",
    "                gate_resp = get_response(gate_prompt, temperature=0.0)\n",
    "                gate_json = parse_model_json(gate_resp, default={\"has_reaction\": \"no\", \"evidence\": \"\"})\n",
    "\n",
    "                has_reaction = str(gate_json.get(\"has_reaction\", \"no\")).lower()\n",
    "                gate_stats[has_reaction] += 1\n",
    "\n",
    "                if has_reaction != \"yes\":\n",
    "                    out = {\n",
    "                        \"entity_type\": entity_type,\n",
    "                        \"asian_status\": asian_status,\n",
    "                        \"reaction\": \"Cannot be inferred\",\n",
    "                        \"reaction_reason\": \"\"\n",
    "                    }\n",
    "                    entity_outputs[entity] = out\n",
    "                    label_stats[\"Cannot be inferred\"] += 1\n",
    "                    continue\n",
    "\n",
    "                # --- 3B: Classifier ---\n",
    "                cls_prompt = build_classifier_prompt(entity_type, asian_status, relevant_sentences)\n",
    "                cls_resp = get_response(cls_prompt, temperature=0.0)\n",
    "                cls_json = parse_model_json(cls_resp, default={\"reaction\": \"Cannot be inferred\", \"reaction_reason\": \"\"})\n",
    "\n",
    "                label = cls_json.get(\"reaction\", \"Cannot be inferred\")\n",
    "                reason = cls_json.get(\"reaction_reason\", \"\")\n",
    "\n",
    "                out = {\n",
    "                    \"entity_type\": entity_type,\n",
    "                    \"asian_status\": asian_status,\n",
    "                    \"reaction\": label,\n",
    "                    \"reaction_reason\": reason\n",
    "                }\n",
    "                entity_outputs[entity] = out\n",
    "                label_stats[label] += 1\n",
    "\n",
    "            step3_batch_result[title] = entity_outputs\n",
    "\n",
    "        # å„²å­˜\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(step3_batch_result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"âœ… æ‰¹æ¬¡ {idx}/{total_batches} å·²å®Œæˆä¸¦å„²å­˜è‡³ {out_path}\")\n",
    "        print(\"   Gate stats:\", dict(gate_stats))\n",
    "        print(\"   Label stats:\", dict(label_stats))\n",
    "\n",
    "# ==============================\n",
    "# åŸ·è¡Œ\n",
    "# ==============================\n",
    "if __name__ == \"__main__\":\n",
    "    run_step3_from_step2(\"step2_batches\", \"step3_batches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "235f9159-8986-4aaa-8c91-8533ad327d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 1/12: step2_batches\\step2_batch_1.json\n",
      "âœ… æ‰¹æ¬¡ 1/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step4_batches\\step4_batch_1.json\n",
      "   Stats: {'done': 50}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 2/12: step2_batches\\step2_batch_10.json\n",
      "âœ… æ‰¹æ¬¡ 2/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step4_batches\\step4_batch_10.json\n",
      "   Stats: {'done': 50}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 3/12: step2_batches\\step2_batch_11.json\n",
      "âœ… æ‰¹æ¬¡ 3/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step4_batches\\step4_batch_11.json\n",
      "   Stats: {'done': 50}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 4/12: step2_batches\\step2_batch_12.json\n",
      "âœ… æ‰¹æ¬¡ 4/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step4_batches\\step4_batch_12.json\n",
      "   Stats: {'done': 34}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 5/12: step2_batches\\step2_batch_2.json\n",
      "âœ… æ‰¹æ¬¡ 5/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step4_batches\\step4_batch_2.json\n",
      "   Stats: {'done': 50}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 6/12: step2_batches\\step2_batch_3.json\n",
      "âœ… æ‰¹æ¬¡ 6/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step4_batches\\step4_batch_3.json\n",
      "   Stats: {'done': 50}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 7/12: step2_batches\\step2_batch_4.json\n",
      "âœ… æ‰¹æ¬¡ 7/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step4_batches\\step4_batch_4.json\n",
      "   Stats: {'done': 50}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 8/12: step2_batches\\step2_batch_5.json\n",
      "âœ… æ‰¹æ¬¡ 8/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step4_batches\\step4_batch_5.json\n",
      "   Stats: {'done': 50}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 9/12: step2_batches\\step2_batch_6.json\n",
      "âœ… æ‰¹æ¬¡ 9/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step4_batches\\step4_batch_6.json\n",
      "   Stats: {'done': 50}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 10/12: step2_batches\\step2_batch_7.json\n",
      "âœ… æ‰¹æ¬¡ 10/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step4_batches\\step4_batch_7.json\n",
      "   Stats: {'done': 50}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 11/12: step2_batches\\step2_batch_8.json\n",
      "âœ… æ‰¹æ¬¡ 11/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step4_batches\\step4_batch_8.json\n",
      "   Stats: {'done': 50}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 12/12: step2_batches\\step2_batch_9.json\n",
      "âœ… æ‰¹æ¬¡ 12/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step4_batches\\step4_batch_9.json\n",
      "   Stats: {'done': 50}\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Step 4 Prompt\n",
    "# ==============================\n",
    "step4_prompt = \"\"\"You are a sociology professor with 30 years of experience analyzing racial dynamics and anti-Asian racism in the United States.\n",
    "\n",
    "### Step 4: Infer **Emotions and Their Intensity**\n",
    "\n",
    "Your task is to analyze the **extracted sentences** from **Step 2** and infer each entity's **emotional stance** toward anti-Asian hate.\n",
    "\n",
    "You will also receive metadata from Step 2, including:\n",
    "- `entity_type`: for individuals use their social role; for organizations use institutional category.\n",
    "- `asian_status`: \"Asian\", \"Non-Asian\", \"Cannot be inferred\", or \"Not applicable\"\n",
    "\n",
    "Use only the exact `relevant_sentences` from Step 2 as your source â€” do NOT paraphrase or add your own wording.\n",
    "\n",
    "---\n",
    "\n",
    "## Emotion Concept Tree\n",
    "- Love \n",
    "- Joy \n",
    "- Anger \n",
    "- Sadness \n",
    "- Fear \n",
    "- Surprise \n",
    "\n",
    "---\n",
    "\n",
    "## Instructions\n",
    "1. Focus only on emotions â€” do NOT infer actions.\n",
    "2. If no emotion is expressed, output `\"emotion\": \"Cannot be inferred\".\n",
    "3. If multiple emotions appear, list multiple objects.\n",
    "4. Use the exact sentence(s) as `\"emotion_reason\"`.\n",
    "\n",
    "---\n",
    "\n",
    "## Output format\n",
    "{\n",
    "  \"Entity Name\": {\n",
    "    \"entity_type\": \"...\",\n",
    "    \"asian_status\": \"...\",\n",
    "    \"emotions\": [\n",
    "      {\n",
    "        \"emotion\": \"deepest matched term or Cannot be inferred\",\n",
    "        \"emotion_reason\": \"Exact sentence(s)\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# ==============================\n",
    "# ä¸»æµç¨‹ Step2 â†’ Step4\n",
    "# ==============================\n",
    "def run_step4_from_step2(step2_dir=\"step2_batches\", step4_dir=\"step4_batches\"):\n",
    "    os.makedirs(step4_dir, exist_ok=True)\n",
    "    step2_files = sorted(glob.glob(os.path.join(step2_dir, \"step2_batch_*.json\")))\n",
    "    total_batches = len(step2_files)\n",
    "\n",
    "    for idx, step2_file in enumerate(step2_files, start=1):\n",
    "        batch_name = os.path.basename(step2_file).replace(\"step2_\", \"step4_\")\n",
    "        out_path = os.path.join(step4_dir, batch_name)\n",
    "\n",
    "        # æ–·é»çºŒè·‘\n",
    "        if os.path.exists(out_path):\n",
    "            print(f\"â­ï¸ æ‰¹æ¬¡ {idx}/{total_batches} {batch_name} å·²å­˜åœ¨ï¼Œè·³é\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ {idx}/{total_batches}: {step2_file}\")\n",
    "\n",
    "        with open(step2_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            step2_batch_result = json.load(f)\n",
    "\n",
    "        step4_batch_result = {}\n",
    "        emo_stats = Counter()\n",
    "\n",
    "        for title, step2_text in step2_batch_result.items():\n",
    "            full_prompt = (\n",
    "                step4_prompt +\n",
    "                f\"\\n\\nStep 2 Results (Extracted Sentences):\\n{step2_text}\"\n",
    "            )\n",
    "\n",
    "            response = get_response(full_prompt)\n",
    "            step4_batch_result[title] = response\n",
    "            emo_stats[\"done\"] += 1\n",
    "\n",
    "        # å„²å­˜\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(step4_batch_result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"âœ… æ‰¹æ¬¡ {idx}/{total_batches} å·²å®Œæˆä¸¦å„²å­˜è‡³ {out_path}\")\n",
    "        print(\"   Stats:\", dict(emo_stats))\n",
    "\n",
    "# ==============================\n",
    "# åŸ·è¡Œ\n",
    "# ==============================\n",
    "if __name__ == \"__main__\":\n",
    "    run_step4_from_step2(\"step2_batches\", \"step4_batches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8509778e-2d88-4fd6-9130-661ad83aaa9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Step 3 å·²åˆä½µ 12 å€‹æ‰¹æ¬¡æª” â†’ step3_all.csvï¼Œå…± 5693 ç­†\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def merge_step3_with_id(input_dir=\"step3_batches\", prefix=\"step3_batch_\", \n",
    "                        output_json=\"step3_all.json\", output_csv=\"step3_all.csv\"):\n",
    "    batch_files = sorted(glob.glob(os.path.join(input_dir, f\"{prefix}*.json\")))\n",
    "\n",
    "    merged_result = {}\n",
    "    for file in batch_files:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            merged_result.update(data)\n",
    "\n",
    "    with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(merged_result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    rows = []\n",
    "    idx = 1\n",
    "    for article_id, entities in merged_result.items():\n",
    "        for entity, meta in entities.items():\n",
    "            rows.append({\n",
    "                \"reaction_id\": f\"reaction_{idx}\",\n",
    "                \"article_id\": article_id,\n",
    "                \"entity\": entity,\n",
    "                \"entity_type\": meta.get(\"entity_type\", \"\"),\n",
    "                \"asian_status\": meta.get(\"asian_status\", \"\"),\n",
    "                \"reaction\": meta.get(\"reaction\", \"\"),\n",
    "                \"reaction_reason\": meta.get(\"reaction_reason\", \"\")\n",
    "            })\n",
    "            idx += 1\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"âœ… Step 3 å·²åˆä½µ {len(batch_files)} å€‹æ‰¹æ¬¡æª” â†’ {output_csv}ï¼Œå…± {len(df)} ç­†\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    merge_step3_with_id(\n",
    "        input_dir=\"step3_batches\",\n",
    "        prefix=\"step3_batch_\",\n",
    "        output_json=\"step3_all.json\",\n",
    "        output_csv=\"step3_all.csv\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "094b81c0-66dd-4e5b-8e48-f663b5550e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Step 4 å·²åˆä½µ 12 å€‹æ‰¹æ¬¡æª” â†’ step4_all.csvï¼Œå…± 8011 ç­†\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def parse_json_loose(s: str):\n",
    "    \"\"\"å˜—è©¦å¾å­—ä¸²è£¡è§£æ JSON\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return None\n",
    "    s = s.strip()\n",
    "    if not s:\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        # å˜—è©¦æŠ“ç¬¬ä¸€å€‹ {...}\n",
    "        m = re.search(r\"\\{.*\\}\", s, flags=re.S)\n",
    "        if m:\n",
    "            try:\n",
    "                return json.loads(m.group(0))\n",
    "            except Exception:\n",
    "                return None\n",
    "    return None\n",
    "\n",
    "def merge_step4_with_id(input_dir=\"step4_batches\", prefix=\"step4_batch_\", \n",
    "                        output_json=\"step4_all.json\", output_csv=\"step4_all.csv\"):\n",
    "    batch_files = sorted(glob.glob(os.path.join(input_dir, f\"{prefix}*.json\")))\n",
    "    all_articles = {}\n",
    "\n",
    "    for file in batch_files:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # âš ï¸ ä¸ç”¨ updateï¼Œé€ç¯‡å±•é–‹\n",
    "        for article_id, raw in data.items():\n",
    "            if isinstance(raw, dict):\n",
    "                all_articles[article_id] = raw\n",
    "            elif isinstance(raw, str):\n",
    "                parsed = parse_json_loose(raw)\n",
    "                if isinstance(parsed, dict):\n",
    "                    all_articles[article_id] = parsed\n",
    "\n",
    "    # å­˜ JSON\n",
    "    with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_articles, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # è½‰æˆ CSV\n",
    "    rows = []\n",
    "    idx = 1\n",
    "    for article_id, entities in all_articles.items():\n",
    "        for entity, meta in entities.items():\n",
    "            emotions = meta.get(\"emotions\", [])\n",
    "            \n",
    "            # çµ±ä¸€è™•ç†æ ¼å¼\n",
    "            if isinstance(emotions, str):\n",
    "                emotions = [{\n",
    "                    \"emotion\": emotions,\n",
    "                    \"emotion_path\": None,\n",
    "                    \"emotion_reason\": \"\"\n",
    "                }]\n",
    "            elif not isinstance(emotions, list):\n",
    "                emotions = []\n",
    "    \n",
    "            for emo in emotions:\n",
    "                if not isinstance(emo, dict):  # å†ä¿éšªä¸€æ¬¡\n",
    "                    emo = {\"emotion\": str(emo), \"emotion_path\": None, \"emotion_reason\": \"\"}\n",
    "                rows.append({\n",
    "                    \"emotion_id\": f\"emotion_{idx}\",\n",
    "                    \"article_id\": article_id,\n",
    "                    \"entity\": entity,\n",
    "                    \"entity_type\": meta.get(\"entity_type\", \"\"),\n",
    "                    \"asian_status\": meta.get(\"asian_status\", \"\"),\n",
    "                    \"emotion\": emo.get(\"emotion\", \"\"),\n",
    "                    \"emotion_path\": emo.get(\"emotion_path\", \"\"),\n",
    "                    \"emotion_reason\": emo.get(\"emotion_reason\", \"\")\n",
    "                })\n",
    "                idx += 1\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"âœ… Step 4 å·²åˆä½µ {len(batch_files)} å€‹æ‰¹æ¬¡æª” â†’ {output_csv}ï¼Œå…± {len(df)} ç­†\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    merge_step4_with_id(\n",
    "        input_dir=\"step4_batches\",\n",
    "        prefix=\"step4_batch_\",\n",
    "        output_json=\"step4_all.json\",\n",
    "        output_csv=\"step4_all.csv\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02abf62-6b50-4158-96e5-e868a510cf72",
   "metadata": {},
   "source": [
    "# é‡åˆ† entity_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dbb82d-0a39-4fe6-a614-2b3379ab7f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === 1. è®€å–åŸå§‹è³‡æ–™ ===\n",
    "df = pd.read_csv(\"step4_all_with_date.csv\")  # æ›æˆä½ çš„æª”æ¡ˆåç¨±\n",
    "\n",
    "# === 2. å®šç¾©æ¨™æº–åŒ–å°æ‡‰è¡¨ ===\n",
    "entity_type_mapping = {\n",
    "    # Individuals\n",
    "    \"victim\": \"victims\",\n",
    "    \"victims\": \"victims\",\n",
    "    \"perpetrator\": \"perpetrators\",\n",
    "    \"perpetrators\": \"perpetrators\",\n",
    "    \"politician\": \"politicians\",\n",
    "    \"politicians\": \"politicians\",\n",
    "    \"professional\": \"professionals\",\n",
    "    \"professionals\": \"professionals\",\n",
    "    \"celebrity\": \"celebrities\",\n",
    "    \"musician\": \"celebrities\",\n",
    "    \"actor\": \"celebrities\",\n",
    "    \"actress\": \"celebrities\",\n",
    "    \"journalist\": \"professionals\",\n",
    "    \"reporter\": \"professionals\",\n",
    "    \"professor\": \"professionals\",\n",
    "    \"student\": \"professionals\",\n",
    "    \"educator\": \"professionals\",\n",
    "    \"scholar\": \"professionals\",\n",
    "    \"adjunct instructor\": \"professionals\",\n",
    "    \"attorney\": \"professionals\",\n",
    "    \"director\": \"professionals\",\n",
    "    \"city_manager\": \"professionals\",\n",
    "    \"sociology professor\": \"professionals\",\n",
    "    \"editor\": \"professionals\",\n",
    "    \"deputy inspector\": \"law_enforcement_agencies\",\n",
    "    \"police_officer\": \"law_enforcement_agencies\",\n",
    "    \"police spokesperson\": \"law_enforcement_agencies\",\n",
    "    \"family_member\": \"other_individuals\",\n",
    "    \"friend\": \"other_individuals\",\n",
    "    \"witness\": \"other_individuals\",\n",
    "    \"individual\": \"other_individuals\",\n",
    "    \"individuals\": \"other_individuals\",\n",
    "    \"general public\": \"other_individuals\",\n",
    "    \"general_public\": \"other_individuals\",\n",
    "    \"social_circle\": \"other_individuals\",\n",
    "    \"community_activist\": \"other_individuals\",\n",
    "    \"community_leader\": \"other_individuals\",\n",
    "    \"organizer\": \"other_individuals\",\n",
    "    \"community organizer\": \"other_individuals\",\n",
    "    \"other individual\": \"other_individuals\",\n",
    "    \"rally organizer\": \"other_individuals\",\n",
    "    \"activist\": \"other_individuals\",\n",
    "    \"supporter\": \"other_individuals\",\n",
    "    \"co-host\": \"celebrities\",\n",
    "    \"artist\": \"celebrities\",\n",
    "    \"former assistant district attorney\": \"professionals\",\n",
    "    \"official\": \"professionals\",\n",
    "    \"non-Asian\": \"other_individuals\",\n",
    "    \"youth coordinator\": \"professionals\",\n",
    "    \"school_board_member\": \"professionals\",\n",
    "    \"Dean\": \"professionals\",\n",
    "    \"community leader\": \"other_individuals\",\n",
    "    \"government body\": \"government_bodies\",\n",
    "\n",
    "    # Organizations\n",
    "    \"law_enforcement_agency\": \"law_enforcement_agencies\",\n",
    "    \"law_enforcement_agencies\": \"law_enforcement_agencies\",\n",
    "    \"government_body\": \"government_bodies\",\n",
    "    \"government_bodies\": \"government_bodies\",\n",
    "    \"ngo_or_advocacy_group\": \"ngo_or_advocacy_groups\",\n",
    "    \"ngo_or_advocacy_groups\": \"ngo_or_advocacy_groups\",\n",
    "    \"business_entity\": \"business_entities\",\n",
    "    \"business_entities\": \"business_entities\",\n",
    "    \"community_group\": \"community_groups\",\n",
    "    \"community_groups\": \"community_groups\",\n",
    "    \"educational_institution\": \"government_bodies\",  # å‡è¨­ç‚ºæ­£å¼æ©Ÿæ§‹\n",
    "\n",
    "    # Fallback\n",
    "    \"other\": \"other_individuals\",\n",
    "    \"other_individual\": \"other_individuals\",\n",
    "    \"other_individuals\": \"other_individuals\",\n",
    "    \"group\": \"unknown\",\n",
    "    # \"Cannot be inferred\": \"unknown\",\n",
    "}\n",
    "\n",
    "# === 3. æ›¿æ› entity_type æ¬„ä½ï¼ˆç›´æ¥è¦†è“‹ï¼‰===\n",
    "df[\"entity_type\"] = df[\"entity_type\"].map(entity_type_mapping).fillna(df[\"entity_type\"])\n",
    "\n",
    "# === 4. è¼¸å‡ºæˆæ–°æª”æ¡ˆ ===\n",
    "df.to_csv(\"step4_all_with_date.csv\", index=False)\n",
    "print(\"âœ… finish\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a569e0-82bd-437f-97cd-a4ea32796158",
   "metadata": {},
   "source": [
    "# é‡åˆ† emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c67d52-5974-4ae1-a4ed-0e8ec62ccaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# è¼‰å…¥ CSV\n",
    "df = pd.read_csv(\"step4_all_with_date.csv\")\n",
    "\n",
    "# å®šç¾© emotion å°ç…§è¡¨ï¼ˆç´°åˆ†é¡ â†’ å…­å¤§æƒ…ç·’ï¼Œå…¨å°å¯«ï¼‰\n",
    "emotion_map = {\n",
    "    # love\n",
    "    \"love\": \"love\",\n",
    "    \"support\": \"love\", \"solidarity\": \"love\",\n",
    "    \"empathy\": \"love\", \"compassion\": \"love\",\n",
    "    \"recognition\": \"love\", \"gratitude\": \"love\",\n",
    "    \"appreciation\": \"love\", \"encouragement\": \"love\",\n",
    "    \"affection\": \"love\", \"lust\": \"love\", \"longing\": \"love\",\n",
    "\n",
    "    # joy\n",
    "    \"joy\": \"joy\",\n",
    "    \"confidence\": \"joy\", \"optimism\": \"joy\", \"empowerment\": \"joy\",\n",
    "    \"cheerfulness\": \"joy\", \"zest\": \"joy\", \"contentment\": \"joy\",\n",
    "    \"pride\": \"joy\", \"relief\": \"joy\",\n",
    "\n",
    "    # anger\n",
    "    \"anger\": \"anger\",\n",
    "    \"outrage\": \"anger\", \"defiance\": \"anger\", \"responsibility\": \"anger\",\n",
    "    \"irritation\": \"anger\", \"exasperation\": \"anger\", \"rage\": \"anger\",\n",
    "    \"disgust\": \"anger\", \"envy\": \"anger\", \"determination\": \"anger\",\n",
    "    \"urgency\": \"anger\", \"frustration\": \"anger\",\n",
    "\n",
    "    # sadness\n",
    "    \"sadness\": \"sadness\",\n",
    "    \"worry\": \"sadness\", \"resignation\": \"sadness\", \"regret\": \"sadness\",\n",
    "    \"mixed emotions\": \"sadness\", \"dismay\": \"sadness\",\n",
    "    \"disquiet\": \"sadness\", \"disturbance\": \"sadness\",\n",
    "    \"guilt\": \"sadness\",\n",
    "    \"suffering\": \"sadness\", \"disappointment\": \"sadness\", \"shame\": \"sadness\",\n",
    "    \"neglect\": \"sadness\", \"sympathy\": \"sadness\", \"heartbreak\": \"sadness\",\n",
    "    \"pain\": \"sadness\", \"grief\": \"sadness\", \"grieving\": \"sadness\",\n",
    "    \"hurt\": \"sadness\", \"loneliness\": \"sadness\", \"despondency\": \"sadness\",\n",
    "    \"helplessness\": \"sadness\", \"exhaustion\": \"sadness\",\n",
    "\n",
    "    # fear\n",
    "    \"fear\": \"fear\",\n",
    "    \"terror\": \"fear\", \"doubt\": \"fear\",\n",
    "    \"alarm\": \"fear\", \"anxiety\": \"fear\", \"insecurity\": \"fear\",\n",
    "    \"panic\": \"fear\", \"dread\": \"fear\", \"overwhelming\": \"fear\",\n",
    "    \"overwhelmed\": \"fear\", \"horror\": \"fear\", \"shock\": \"fear\",\n",
    "\n",
    "    # surprise\n",
    "    \"surprise\": \"surprise\",\n",
    "    \"confusion\": \"surprise\", \"lightbulb moment\": \"surprise\",\n",
    "    \"amazement\": \"surprise\", \"wonder\": \"surprise\"\n",
    "}\n",
    "\n",
    "def map_emotions(emotion_str):\n",
    "    \"\"\"æŠŠæƒ…ç·’æ”¶æ–‚æˆå…­å¤§åŸºæœ¬æƒ…ç·’ï¼Œå…¶ä»–æ­¸ç‚º cannot be inferredï¼Œå…¨å°å¯«\"\"\"\n",
    "    if pd.isna(emotion_str):\n",
    "        return \"cannot be inferred\"\n",
    "    emotions = [e.strip().lower() for e in emotion_str.split(\"|\")]\n",
    "    mapped = [emotion_map.get(e, \"cannot be inferred\") for e in emotions]\n",
    "    mapped = list(dict.fromkeys(mapped))  # å»é‡ä½†ä¿ç•™é †åº\n",
    "    return \" | \".join(mapped)\n",
    "\n",
    "# å»ºç«‹æ–°çš„æ¬„ä½\n",
    "df[\"emotion\"] = df[\"emotion\"].apply(map_emotions)\n",
    "\n",
    "# è¼¸å‡ºçµæœ\n",
    "df.to_csv(\"step4_all_with_date.csv\", index=False)\n",
    "print(\"âœ… å·²å®Œæˆï¼šemotion å…¨éƒ¨è½‰æˆå°å¯« (love, joy, anger, sadness, fear, surprise, cannot be inferred)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
