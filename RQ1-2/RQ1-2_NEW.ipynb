{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73242df9-24a6-408b-ac20-ca77f832645e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "925aefd2-80c7-4d84-9617-3b2f1f26cc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ CSV ä¸­çš„æ–‡ç« ç·¨è™Ÿï¼š\n",
      "['Article_1', 'Article_2', 'Article_3', 'Article_4', 'Article_5', 'Article_6', 'Article_8', 'Article_9', 'Article_10', 'Article_11', 'Article_12', 'Article_13', 'Article_14', 'Article_15', 'Article_16', 'Article_17', 'Article_18', 'Article_19', 'Article_20', 'Article_21', 'Article_22', 'Article_23', 'Article_24', 'Article_25', 'Article_26', 'Article_27', 'Article_28', 'Article_30', 'Article_31', 'Article_32', 'Article_33', 'Article_34', 'Article_35', 'Article_36', 'Article_37', 'Article_38', 'Article_39', 'Article_40', 'Article_42', 'Article_43', 'Article_44', 'Article_45', 'Article_46', 'Article_47', 'Article_48', 'Article_49', 'Article_50', 'Article_52', 'Article_53', 'Article_56', 'Article_57', 'Article_58', 'Article_59', 'Article_60', 'Article_61', 'Article_62', 'Article_65', 'Article_66', 'Article_67', 'Article_68', 'Article_69', 'Article_70', 'Article_71', 'Article_72', 'Article_73', 'Article_74', 'Article_75', 'Article_76', 'Article_79', 'Article_80', 'Article_81', 'Article_82', 'Article_83', 'Article_84', 'Article_85', 'Article_86', 'Article_87', 'Article_88', 'Article_89', 'Article_90', 'Article_91', 'Article_92', 'Article_93', 'Article_94', 'Article_96', 'Article_97', 'Article_98', 'Article_99', 'Article_100', 'Article_101', 'Article_102', 'Article_103', 'Article_104', 'Article_105', 'Article_107', 'Article_108', 'Article_109', 'Article_110', 'Article_112', 'Article_113', 'Article_114', 'Article_115', 'Article_116', 'Article_117', 'Article_118', 'Article_119', 'Article_120', 'Article_121', 'Article_122', 'Article_123', 'Article_124', 'Article_125', 'Article_126', 'Article_127', 'Article_128', 'Article_129', 'Article_130', 'Article_131', 'Article_132', 'Article_133', 'Article_134', 'Article_136', 'Article_137', 'Article_139', 'Article_140', 'Article_141', 'Article_142', 'Article_143', 'Article_144', 'Article_145', 'Article_147', 'Article_148', 'Article_149', 'Article_150', 'Article_151', 'Article_152', 'Article_153', 'Article_154', 'Article_155', 'Article_156', 'Article_157', 'Article_158', 'Article_159', 'Article_160', 'Article_161', 'Article_162', 'Article_163', 'Article_164', 'Article_165', 'Article_166', 'Article_167', 'Article_168', 'Article_169', 'Article_170', 'Article_171', 'Article_172', 'Article_173', 'Article_174', 'Article_175', 'Article_176', 'Article_177', 'Article_178', 'Article_179', 'Article_180', 'Article_181', 'Article_182', 'Article_183', 'Article_184', 'Article_185', 'Article_186', 'Article_187', 'Article_188', 'Article_189', 'Article_190', 'Article_191', 'Article_192', 'Article_193', 'Article_194', 'Article_195', 'Article_196', 'Article_197', 'Article_198', 'Article_199', 'Article_200', 'Article_201', 'Article_202', 'Article_203', 'Article_204', 'Article_205', 'Article_206', 'Article_207', 'Article_208', 'Article_209', 'Article_210', 'Article_211', 'Article_212', 'Article_213', 'Article_214', 'Article_216', 'Article_217', 'Article_218', 'Article_219', 'Article_220', 'Article_221', 'Article_222', 'Article_223', 'Article_224', 'Article_225', 'Article_226', 'Article_227', 'Article_228', 'Article_230', 'Article_231', 'Article_232', 'Article_233', 'Article_235', 'Article_236', 'Article_237', 'Article_238', 'Article_239', 'Article_240', 'Article_241', 'Article_242', 'Article_243', 'Article_244', 'Article_245', 'Article_246', 'Article_247', 'Article_248', 'Article_249', 'Article_250', 'Article_251', 'Article_252', 'Article_253', 'Article_255', 'Article_256', 'Article_257', 'Article_258', 'Article_259', 'Article_260', 'Article_262', 'Article_263', 'Article_264', 'Article_265', 'Article_266', 'Article_267', 'Article_268', 'Article_270', 'Article_272', 'Article_273', 'Article_274', 'Article_275', 'Article_276', 'Article_277', 'Article_278', 'Article_279', 'Article_280', 'Article_281', 'Article_282', 'Article_283', 'Article_284', 'Article_285', 'Article_286', 'Article_288', 'Article_290', 'Article_291', 'Article_292', 'Article_293', 'Article_294', 'Article_296', 'Article_297', 'Article_298', 'Article_299', 'Article_300', 'Article_301', 'Article_302', 'Article_303', 'Article_304', 'Article_305', 'Article_306', 'Article_307', 'Article_308', 'Article_309', 'Article_310', 'Article_311', 'Article_312', 'Article_313', 'Article_314', 'Article_315', 'Article_316', 'Article_317', 'Article_318', 'Article_319', 'Article_320', 'Article_321', 'Article_322', 'Article_323', 'Article_324', 'Article_325', 'Article_326', 'Article_327', 'Article_328', 'Article_329', 'Article_330', 'Article_331', 'Article_332', 'Article_333', 'Article_334', 'Article_335', 'Article_336', 'Article_337', 'Article_338', 'Article_339', 'Article_340', 'Article_341', 'Article_342', 'Article_343', 'Article_344', 'Article_345', 'Article_346', 'Article_347', 'Article_348', 'Article_349', 'Article_350', 'Article_351', 'Article_352', 'Article_355', 'Article_356', 'Article_357', 'Article_358', 'Article_359', 'Article_360', 'Article_361', 'Article_363', 'Article_364', 'Article_365', 'Article_366', 'Article_367', 'Article_368', 'Article_369', 'Article_370', 'Article_371', 'Article_373', 'Article_374', 'Article_375', 'Article_376', 'Article_377', 'Article_379', 'Article_380', 'Article_381', 'Article_382', 'Article_383', 'Article_384', 'Article_385', 'Article_386', 'Article_387', 'Article_388', 'Article_389', 'Article_390', 'Article_391', 'Article_392', 'Article_393', 'Article_394', 'Article_395', 'Article_396', 'Article_397', 'Article_398', 'Article_399', 'Article_400', 'Article_401', 'Article_402', 'Article_403', 'Article_404', 'Article_405', 'Article_406', 'Article_409', 'Article_410', 'Article_411', 'Article_412', 'Article_413', 'Article_414', 'Article_415', 'Article_416', 'Article_417', 'Article_418', 'Article_419', 'Article_420', 'Article_421', 'Article_422', 'Article_423', 'Article_424', 'Article_425', 'Article_426', 'Article_427', 'Article_428', 'Article_429', 'Article_430', 'Article_431', 'Article_433', 'Article_434', 'Article_435', 'Article_436', 'Article_437', 'Article_438', 'Article_439', 'Article_440', 'Article_441', 'Article_442', 'Article_443', 'Article_445', 'Article_446', 'Article_448', 'Article_449', 'Article_450', 'Article_451', 'Article_452', 'Article_453', 'Article_454', 'Article_455', 'Article_456', 'Article_457', 'Article_458', 'Article_459', 'Article_460', 'Article_461', 'Article_463', 'Article_465', 'Article_466', 'Article_467', 'Article_468', 'Article_469', 'Article_470', 'Article_471', 'Article_473', 'Article_474', 'Article_475', 'Article_476', 'Article_477', 'Article_478', 'Article_479', 'Article_480', 'Article_481', 'Article_483', 'Article_484', 'Article_485', 'Article_486', 'Article_487', 'Article_488', 'Article_489', 'Article_490', 'Article_492', 'Article_493', 'Article_494', 'Article_495', 'Article_496', 'Article_499', 'Article_500', 'Article_501', 'Article_503', 'Article_504', 'Article_505', 'Article_506', 'Article_507', 'Article_508', 'Article_510', 'Article_511', 'Article_512', 'Article_513', 'Article_514', 'Article_515', 'Article_516', 'Article_517', 'Article_518', 'Article_519', 'Article_520', 'Article_521', 'Article_522', 'Article_523', 'Article_524', 'Article_525', 'Article_526', 'Article_527', 'Article_528', 'Article_529', 'Article_530', 'Article_531', 'Article_532', 'Article_533', 'Article_534', 'Article_535', 'Article_536', 'Article_537', 'Article_538', 'Article_539', 'Article_540', 'Article_541', 'Article_542', 'Article_543', 'Article_544', 'Article_545', 'Article_546', 'Article_547', 'Article_548', 'Article_549', 'Article_550', 'Article_551', 'Article_552', 'Article_553', 'Article_554', 'Article_555', 'Article_556', 'Article_557', 'Article_558', 'Article_559', 'Article_560', 'Article_561', 'Article_562', 'Article_563', 'Article_564', 'Article_565', 'Article_566', 'Article_567', 'Article_568', 'Article_569', 'Article_570', 'Article_571', 'Article_572', 'Article_573', 'Article_574', 'Article_575', 'Article_576', 'Article_577', 'Article_578', 'Article_579', 'Article_580', 'Article_581', 'Article_582', 'Article_583', 'Article_584', 'Article_585', 'Article_586', 'Article_587', 'Article_588', 'Article_589', 'Article_591', 'Article_592', 'Article_593', 'Article_594', 'Article_596', 'Article_597', 'Article_598', 'Article_599', 'Article_600', 'Article_601', 'Article_602', 'Article_603', 'Article_604', 'Article_605', 'Article_606', 'Article_607', 'Article_609', 'Article_610', 'Article_611', 'Article_613', 'Article_614', 'Article_616', 'Article_617', 'Article_618', 'Article_619', 'Article_620', 'Article_621', 'Article_622', 'Article_623', 'Article_624', 'Article_625', 'Article_626', 'Article_627', 'Article_628', 'Article_629', 'Article_630', 'Article_631', 'Article_632', 'Article_633', 'Article_634']\n",
      "âœ… ç¸½å…±æœ‰ 584 ç¯‡æ–‡ç« \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from typing import Dict, Any\n",
    "from openai import OpenAI\n",
    "\n",
    "# ==============================\n",
    "# OpenAI è¨­å®šèˆ‡å‘¼å«\n",
    "# ==============================\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "def get_response(prompt: str, temperature: float = 0.0) -> str:\n",
    "    \"\"\"å‘¼å« OpenAI APIï¼Œå›å‚³æ¨¡å‹è¼¸å‡ºï¼ˆæ‡‰è©²æ˜¯ JSON å­—ä¸²ï¼‰\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperature,\n",
    "            stream=False,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"API èª¿ç”¨éŒ¯èª¤: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# ==============================\n",
    "# è®€å–æ–‡ç« ï¼ˆåªç‚ºäº†ä¿ç•™é †åºï¼Œä¸ä¸Ÿé€²æ¨¡å‹ï¼‰\n",
    "# ==============================\n",
    "import pandas as pd\n",
    "\n",
    "# è®€å– CSV\n",
    "df = pd.read_csv(\"./articles_584.csv\", encoding=\"ISO-8859-1\")\n",
    "\n",
    "# åªä¿ç•™æœ‰æ–‡ç« çš„è³‡æ–™\n",
    "df = df.dropna(subset=[\"ARTICLE_TEXT\"])\n",
    "\n",
    "# ç¢ºä¿ id æ¬„ä½æ˜¯æ•´æ•¸å‹æ…‹ï¼ˆé¿å…å¾Œé¢æ‹¼æ¥å‡ºå•é¡Œï¼‰\n",
    "df[\"id\"] = df[\"id\"].astype(int)\n",
    "\n",
    "# å»ºç«‹ dictï¼šç”¨ CSV è£¡çš„ id ç•¶ç·¨è™Ÿ\n",
    "articles = {f\"Article_{row['id']}\": row[\"ARTICLE_TEXT\"] for _, row in df.iterrows()}\n",
    "\n",
    "# åˆ—å‡ºå…¨éƒ¨æ–‡ç« ç·¨è™Ÿ\n",
    "all_articles = list(articles.keys())\n",
    "print(\"ğŸ“„ CSV ä¸­çš„æ–‡ç« ç·¨è™Ÿï¼š\")\n",
    "print(all_articles)  # å…ˆåªå°å‰ 20 ç­†\n",
    "print(f\"âœ… ç¸½å…±æœ‰ {len(all_articles)} ç¯‡æ–‡ç« \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c453f1f-c7bb-4338-8446-6a9e374f52db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å¯¦éš›æ–‡ç« æ•¸é‡: 584 / 584\n",
      "âŒ ç¼ºå°‘çš„ Article ç·¨è™Ÿï¼ˆå…± 45 ç¯‡ï¼‰:\n",
      "Article_7, Article_29, Article_41, Article_51, Article_54, Article_55, Article_63, Article_64, Article_77, Article_78, Article_95, Article_106, Article_111, Article_135, Article_138, Article_146, Article_215, Article_229, Article_234, Article_254, Article_261, Article_269, Article_271, Article_287, Article_289, Article_295, Article_353, Article_354, Article_362, Article_372, Article_378, Article_407, Article_408, Article_432, Article_444, Article_447, Article_462, Article_464, Article_472, Article_482, Article_491, Article_497, Article_498, Article_502, Article_509\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# è¼¸å…¥ä½ çš„ CSV æª”æ¡ˆåç¨±\n",
    "csv_file = \"articles_584.csv\"\n",
    "\n",
    "# å‡è¨­ä½ çŸ¥é“ç¸½å…±æ‡‰è©²æœ‰å¤šå°‘ç¯‡ï¼ˆä¾‹ï¼š584ï¼‰\n",
    "expected_total = 584\n",
    "\n",
    "# è®€å– CSV\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# å–å¾—å¯¦éš›å‡ºç¾éçš„ idï¼ˆè½‰æˆ intï¼‰\n",
    "existing_numbers = set(df[\"id\"].dropna().astype(int))\n",
    "\n",
    "# å»ºç«‹å®Œæ•´æ‡‰æœ‰çš„ ID é›†åˆ\n",
    "# âš ï¸ å‡è¨­ id å¾ 1 é–‹å§‹ç·¨è™Ÿ\n",
    "expected_numbers = set(range(1, expected_total + 1))\n",
    "\n",
    "# æ‰¾å‡ºç¼ºå°‘çš„ç·¨è™Ÿ\n",
    "missing = sorted(expected_numbers - existing_numbers)\n",
    "\n",
    "print(f\"âœ… å¯¦éš›æ–‡ç« æ•¸é‡: {len(existing_numbers)} / {expected_total}\")\n",
    "print(f\"âŒ ç¼ºå°‘çš„ Article ç·¨è™Ÿï¼ˆå…± {len(missing)} ç¯‡ï¼‰:\")\n",
    "print(\", \".join(f\"Article_{i}\" for i in missing))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4936bcaa-d529-457a-92ad-55356de3f848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 1/12...\n",
      "âœ… æ‰¹æ¬¡ 1/12 å·²å®Œæˆï¼Œå„²å­˜è‡³ step1_batches\\step1_batch_1.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 2/12...\n",
      "âœ… æ‰¹æ¬¡ 2/12 å·²å®Œæˆï¼Œå„²å­˜è‡³ step1_batches\\step1_batch_2.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 3/12...\n",
      "âœ… æ‰¹æ¬¡ 3/12 å·²å®Œæˆï¼Œå„²å­˜è‡³ step1_batches\\step1_batch_3.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 4/12...\n",
      "âœ… æ‰¹æ¬¡ 4/12 å·²å®Œæˆï¼Œå„²å­˜è‡³ step1_batches\\step1_batch_4.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 5/12...\n",
      "âœ… æ‰¹æ¬¡ 5/12 å·²å®Œæˆï¼Œå„²å­˜è‡³ step1_batches\\step1_batch_5.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 6/12...\n",
      "âœ… æ‰¹æ¬¡ 6/12 å·²å®Œæˆï¼Œå„²å­˜è‡³ step1_batches\\step1_batch_6.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 7/12...\n",
      "âœ… æ‰¹æ¬¡ 7/12 å·²å®Œæˆï¼Œå„²å­˜è‡³ step1_batches\\step1_batch_7.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 8/12...\n",
      "âœ… æ‰¹æ¬¡ 8/12 å·²å®Œæˆï¼Œå„²å­˜è‡³ step1_batches\\step1_batch_8.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 9/12...\n",
      "âœ… æ‰¹æ¬¡ 9/12 å·²å®Œæˆï¼Œå„²å­˜è‡³ step1_batches\\step1_batch_9.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 10/12...\n",
      "âœ… æ‰¹æ¬¡ 10/12 å·²å®Œæˆï¼Œå„²å­˜è‡³ step1_batches\\step1_batch_10.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 11/12...\n",
      "âœ… æ‰¹æ¬¡ 11/12 å·²å®Œæˆï¼Œå„²å­˜è‡³ step1_batches\\step1_batch_11.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 12/12...\n",
      "âœ… æ‰¹æ¬¡ 12/12 å·²å®Œæˆï¼Œå„²å­˜è‡³ step1_batches\\step1_batch_12.json\n",
      "\n",
      "ğŸ‰ æ‰€æœ‰æ‰¹æ¬¡è™•ç†å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Colab ç¨‹å¼ç¢¼å€å¡Š 1: åˆå§‹åŒ–å’Œæ­¥é©Ÿ1\n",
    "# ==========================================\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Step 1 çš„æç¤ºè©\n",
    "step1_prompt = \"\"\"You are a sociology professor with 30 years of experience analyzing Asian racism in the United States.\n",
    "Your task is to analyze the following article by identifying named entities and classifying them into the appropriate social roles and institutional categories. Focus especially on people or groups reacting to or affected by anti-Asian incidents.\n",
    "\n",
    "Step 1: Named Entity Recognition and Categorization\n",
    "\n",
    "1. Identify named entities.\n",
    "2. Classify them into appropriate social roles or institutional categories.\n",
    "3. Determine whether each **individual** is **Asian**, **Non-Asian**, or **Cannot be inferred** based on the text.\n",
    "4. For each entity, include a `\"reference\"` field that reflects **the exact phrase or wording** used in the article to refer to them.\n",
    "\n",
    "Please extract all named entities from the text and categorize them according to the following schema:\n",
    "\n",
    "---\n",
    "\n",
    "**INDIVIDUALS** (Specific persons or actors representing individual agency)\n",
    "\n",
    "1. politicians\n",
    "   - Elected officials acting in an individual capacity.\n",
    "   - Examples: senators, representatives, mayors, governors\n",
    "\n",
    "2. professionals\n",
    "   - Individuals recognized by their expertise or institutional role.\n",
    "   - Examples: professors, doctors, lawyers, foundation presidents\n",
    "\n",
    "3. celebrities\n",
    "   - Public figures in entertainment or sports (e.g., actors, athletes) unless clearly acting in a professional or political role.\n",
    "   - If overlapping with another role, assign to the more institutionally grounded category.\n",
    "\n",
    "4. perpetrators\n",
    "   - Individuals directly identified as committing or responsible for anti-Asian actions.\n",
    "   - Do not include vague or generalized public unless clearly specified.\n",
    "\n",
    "5. victims\n",
    "   - Individuals or racial/ethnic groups explicitly targeted by anti-Asian acts.\n",
    "   - Examples: â€œa woman attacked on the subway,â€ â€œJapanese Americans during WWIIâ€\n",
    "\n",
    "6. other_individuals\n",
    "   - All other named or unnamed individuals who do not fall into the above categories.\n",
    "   - Includes the general public, community members, business owners, or relatives (e.g., â€œmy mom,â€ â€œa neighborâ€).\n",
    "\n",
    "---\n",
    "\n",
    "**ORGANIZATIONS** (Named institutions or collectives)\n",
    "\n",
    "1. law_enforcement_agencies\n",
    "   - Official police or investigative institutions.\n",
    "   - Examples: Chicago Police Department, FBI, local sheriffâ€™s office\n",
    "\n",
    "2. government_bodies\n",
    "   - Government agencies, departments, or offices at any level (local/state/federal).\n",
    "   - Examples: CDC, Department of Justice, City Council\n",
    "\n",
    "3. ngo_or_advocacy_groups\n",
    "   - Civil rights organizations, foundations, or advocacy nonprofits.\n",
    "   - Examples: Stop AAPI Hate, Robert Wood Johnson Foundation\n",
    "\n",
    "4. business_entities\n",
    "   - Named companies, hotels, restaurants, or stores.\n",
    "   - Examples: Wrap-on Tools, Edgewater Beach Hotel\n",
    "\n",
    "5. community_groups\n",
    "   - Named cultural, ethnic, or neighborhood associations.\n",
    "   - Examples: Chinatown Association, Asian-American Coalition\n",
    "\n",
    "---\n",
    "\n",
    "**ETHNICITY INFERENCE RULES:**\n",
    "\n",
    "- For each **individual**, determine whether they are **Asian**, **Non-Asian**, or **Cannot be inferred**.\n",
    "- Use contextual clues such as ethnicity indicators, names, or explicit mentions.\n",
    "- If ethnicity is ambiguous or not stated, return `\"Cannot be inferred\"`.\n",
    "\n",
    "---\n",
    "\n",
    "**ADDITIONAL INSTRUCTIONS:**\n",
    "\n",
    "- Use `\"reference\"` to capture how the person/group was referred to in the original article (e.g., `\"an 80-year-old woman\"`, `\"Lee\"`, `\"the attacker\"`).\n",
    "- Normalize all name variants to a canonical form (e.g., â€œDr. Church,â€ â€œJ. Church,â€ and â€œChurchâ€ â†’ â€œJacqueline Churchâ€).\n",
    "- If an individual belongs to multiple roles, assign them to the most institutionally specific one (e.g., categorize a lawyer-celebrity as a professional).\n",
    "- Include only individuals explicitly involved in specific incidents under â€œvictimsâ€ and â€œperpetrators.â€\n",
    "- Do not classify individual police officers or sheriffs as individualsâ€”assign them under law_enforcement_agencies.\n",
    "- Classify individual owners under â€œbusiness_actorsâ€ and company names under â€œbusiness_entities.â€\n",
    "\n",
    "---\n",
    "\n",
    "**Output format (in JSON):**\n",
    "\n",
    "For all individuals, return an object with \"name\" and \"asian_status\" fields.\n",
    "For all organizations, return an object with \"name\" and \"asian_status\": \"Not applicable\".\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"individuals\": {\n",
    "    \"politicians\": [\n",
    "      {\n",
    "        \"name\": \"Tammy Duckworth\",\n",
    "        \"reference\": \"Senator Tammy Duckworth\",\n",
    "        \"asian_status\": \"Asian\"\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"Joe Biden\",\n",
    "        \"reference\": \"President Joe Biden\",\n",
    "        \"asian_status\": \"Non-Asian\"\n",
    "      }\n",
    "    ],\n",
    "    \"professionals\": [\n",
    "      {\n",
    "        \"name\": \"Julie Morita\",\n",
    "        \"reference\": \"Julie Morita\",\n",
    "        \"asian_status\": \"Asian\"\n",
    "      }\n",
    "    ],\n",
    "    \"celebrities\": [\n",
    "      {\n",
    "        \"name\": \"Awkwafina\",\n",
    "        \"reference\": \"Awkwafina\",\n",
    "        \"asian_status\": \"Asian\"\n",
    "      }\n",
    "    ],\n",
    "    \"perpetrators\": [\n",
    "      {\n",
    "        \"name\": \"Unknown Attacker\",\n",
    "        \"reference\": \"the attacker\",\n",
    "        \"asian_status\": \"Non-Asian\"\n",
    "      }\n",
    "    ],\n",
    "    \"victims\": [\n",
    "      {\n",
    "        \"name\": \"Asian Elderly Woman\",\n",
    "        \"reference\": \"an 80-year-old woman\",\n",
    "        \"asian_status\": \"Asian\"\n",
    "      }\n",
    "    ],\n",
    "    \"other_individuals\": [\n",
    "      {\n",
    "        \"name\": \"my mom\",\n",
    "        \"reference\": \"my mom\",\n",
    "        \"asian_status\": \"Asian\"\n",
    "      }\n",
    "    ]\n",
    "    \n",
    "  },\n",
    "  \"organizations\": {\n",
    "    \"law_enforcement_agencies\": [\n",
    "      {\n",
    "        \"name\": \"Chicago Police Department\",\n",
    "        \"reference\": \"Chicago Police Department\",\n",
    "        \"asian_status\": \"Not applicable\"\n",
    "      }\n",
    "    ],\n",
    "    \"government_bodies\": [\n",
    "      {\n",
    "        \"name\": \"City Council\",\n",
    "        \"reference\": \"City Council\",\n",
    "        \"asian_status\": \"Not applicable\"\n",
    "      }\n",
    "    ],\n",
    "    \"ngo_or_advocacy_groups\": [\n",
    "      {\n",
    "        \"name\": \"Stop AAPI Hate\",\n",
    "        \"reference\": \"Stop AAPI Hate\",\n",
    "        \"asian_status\": \"Not applicable\"\n",
    "      }\n",
    "    ],\n",
    "    \"business_entities\": [\n",
    "      {\n",
    "        \"name\": \"Edgewater Beach Hotel\",\n",
    "        \"reference\": \"Edgewater Beach Hotel\",\n",
    "        \"asian_status\": \"Not applicable\"\n",
    "      }\n",
    "    ],\n",
    "    \"community_groups\": [\n",
    "      {\n",
    "        \"name\": \"Chinatown Association\",\n",
    "        \"reference\": \"Chinatown Association\",\n",
    "        \"asian_status\": \"Not applicable\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# å‘¼å« Step 1\n",
    "\n",
    "# å»ºç«‹è¼¸å‡ºè³‡æ–™å¤¾\n",
    "output_dir = \"step1_batches\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# æ¯æ‰¹æ–‡ç« æ•¸é‡\n",
    "batch_size = 50\n",
    "article_items = list(articles.items())\n",
    "total_articles = len(article_items)\n",
    "total_batches = (total_articles + batch_size - 1) // batch_size  # ç„¡æ¢ä»¶é€²ä½\n",
    "\n",
    "# é€æ‰¹è™•ç†\n",
    "for batch_idx in range(0, total_articles, batch_size):\n",
    "    batch_number = batch_idx // batch_size + 1\n",
    "    filename = os.path.join(output_dir, f\"step1_batch_{batch_number}.json\")\n",
    "\n",
    "    # å¦‚æœæª”æ¡ˆå·²ç¶“å­˜åœ¨ï¼Œå°±è·³éé€™æ‰¹\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"â­ï¸ æ‰¹æ¬¡ {batch_number}/{total_batches} å·²å­˜åœ¨ï¼Œè·³é\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ {batch_number}/{total_batches}...\")\n",
    "\n",
    "    step1_result = {}\n",
    "    batch = article_items[batch_idx: batch_idx + batch_size]\n",
    "\n",
    "    for title, content in batch:\n",
    "        full_prompt = step1_prompt + \"\\n\\nArticle Text:\\n\" + content\n",
    "        response = get_response(full_prompt)\n",
    "        step1_result[title] = response\n",
    "\n",
    "    # å„²å­˜é€™ä¸€æ‰¹çµæœ\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(step1_result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"âœ… æ‰¹æ¬¡ {batch_number}/{total_batches} å·²å®Œæˆï¼Œå„²å­˜è‡³ {filename}\")\n",
    "\n",
    "print(\"\\nğŸ‰ æ‰€æœ‰æ‰¹æ¬¡è™•ç†å®Œæˆï¼\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97f46f96-40d0-4d23-a71d-fdf5e7931277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ é–‹å§‹è™•ç† step1_batches\\step1_batch_1.json â†’ step2_batches\\step2_batch_1.json\n",
      "âœ”ï¸ å·²è™•ç† Article_1\n",
      "âœ”ï¸ å·²è™•ç† Article_2\n",
      "âœ”ï¸ å·²è™•ç† Article_3\n",
      "âœ”ï¸ å·²è™•ç† Article_4\n",
      "âœ”ï¸ å·²è™•ç† Article_5\n",
      "âœ”ï¸ å·²è™•ç† Article_6\n",
      "âœ”ï¸ å·²è™•ç† Article_8\n",
      "âœ”ï¸ å·²è™•ç† Article_9\n",
      "âœ”ï¸ å·²è™•ç† Article_10\n",
      "âœ”ï¸ å·²è™•ç† Article_11\n",
      "âœ”ï¸ å·²è™•ç† Article_12\n",
      "âœ”ï¸ å·²è™•ç† Article_13\n",
      "âœ”ï¸ å·²è™•ç† Article_14\n",
      "âœ”ï¸ å·²è™•ç† Article_15\n",
      "âœ”ï¸ å·²è™•ç† Article_16\n",
      "âœ”ï¸ å·²è™•ç† Article_17\n",
      "âœ”ï¸ å·²è™•ç† Article_18\n",
      "âœ”ï¸ å·²è™•ç† Article_19\n",
      "âœ”ï¸ å·²è™•ç† Article_20\n",
      "âœ”ï¸ å·²è™•ç† Article_21\n",
      "âœ”ï¸ å·²è™•ç† Article_22\n",
      "âœ”ï¸ å·²è™•ç† Article_23\n",
      "âœ”ï¸ å·²è™•ç† Article_24\n",
      "âœ”ï¸ å·²è™•ç† Article_25\n",
      "âœ”ï¸ å·²è™•ç† Article_26\n",
      "âœ”ï¸ å·²è™•ç† Article_27\n",
      "âœ”ï¸ å·²è™•ç† Article_28\n",
      "âœ”ï¸ å·²è™•ç† Article_30\n",
      "âœ”ï¸ å·²è™•ç† Article_31\n",
      "âœ”ï¸ å·²è™•ç† Article_32\n",
      "âœ”ï¸ å·²è™•ç† Article_33\n",
      "âœ”ï¸ å·²è™•ç† Article_34\n",
      "âœ”ï¸ å·²è™•ç† Article_35\n",
      "âœ”ï¸ å·²è™•ç† Article_36\n",
      "âœ”ï¸ å·²è™•ç† Article_37\n",
      "âœ”ï¸ å·²è™•ç† Article_38\n",
      "âœ”ï¸ å·²è™•ç† Article_39\n",
      "âœ”ï¸ å·²è™•ç† Article_40\n",
      "âœ”ï¸ å·²è™•ç† Article_42\n",
      "âœ”ï¸ å·²è™•ç† Article_43\n",
      "âœ”ï¸ å·²è™•ç† Article_44\n",
      "âœ”ï¸ å·²è™•ç† Article_45\n",
      "âœ”ï¸ å·²è™•ç† Article_46\n",
      "âœ”ï¸ å·²è™•ç† Article_47\n",
      "âœ”ï¸ å·²è™•ç† Article_48\n",
      "âœ”ï¸ å·²è™•ç† Article_49\n",
      "âœ”ï¸ å·²è™•ç† Article_50\n",
      "âœ”ï¸ å·²è™•ç† Article_52\n",
      "âœ”ï¸ å·²è™•ç† Article_53\n",
      "âœ”ï¸ å·²è™•ç† Article_56\n",
      "âœ… step2_batch_1.json å·²å®Œæˆä¸¦å„²å­˜è‡³ step2_batches\\step2_batch_1.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç† step1_batches\\step1_batch_10.json â†’ step2_batches\\step2_batch_10.json\n",
      "âœ”ï¸ å·²è™•ç† Article_492\n",
      "âœ”ï¸ å·²è™•ç† Article_493\n",
      "âœ”ï¸ å·²è™•ç† Article_494\n",
      "âœ”ï¸ å·²è™•ç† Article_495\n",
      "âœ”ï¸ å·²è™•ç† Article_496\n",
      "âœ”ï¸ å·²è™•ç† Article_499\n",
      "âœ”ï¸ å·²è™•ç† Article_500\n",
      "âœ”ï¸ å·²è™•ç† Article_501\n",
      "âœ”ï¸ å·²è™•ç† Article_503\n",
      "âœ”ï¸ å·²è™•ç† Article_504\n",
      "âœ”ï¸ å·²è™•ç† Article_505\n",
      "âœ”ï¸ å·²è™•ç† Article_506\n",
      "âœ”ï¸ å·²è™•ç† Article_507\n",
      "âœ”ï¸ å·²è™•ç† Article_508\n",
      "âœ”ï¸ å·²è™•ç† Article_510\n",
      "âœ”ï¸ å·²è™•ç† Article_511\n",
      "âœ”ï¸ å·²è™•ç† Article_512\n",
      "âœ”ï¸ å·²è™•ç† Article_513\n",
      "âœ”ï¸ å·²è™•ç† Article_514\n",
      "âœ”ï¸ å·²è™•ç† Article_515\n",
      "âœ”ï¸ å·²è™•ç† Article_516\n",
      "âœ”ï¸ å·²è™•ç† Article_517\n",
      "âœ”ï¸ å·²è™•ç† Article_518\n",
      "âœ”ï¸ å·²è™•ç† Article_519\n",
      "âœ”ï¸ å·²è™•ç† Article_520\n",
      "âœ”ï¸ å·²è™•ç† Article_521\n",
      "âœ”ï¸ å·²è™•ç† Article_522\n",
      "âœ”ï¸ å·²è™•ç† Article_523\n",
      "âœ”ï¸ å·²è™•ç† Article_524\n",
      "âœ”ï¸ å·²è™•ç† Article_525\n",
      "âœ”ï¸ å·²è™•ç† Article_526\n",
      "âœ”ï¸ å·²è™•ç† Article_527\n",
      "âœ”ï¸ å·²è™•ç† Article_528\n",
      "âœ”ï¸ å·²è™•ç† Article_529\n",
      "âœ”ï¸ å·²è™•ç† Article_530\n",
      "âœ”ï¸ å·²è™•ç† Article_531\n",
      "âœ”ï¸ å·²è™•ç† Article_532\n",
      "âœ”ï¸ å·²è™•ç† Article_533\n",
      "âœ”ï¸ å·²è™•ç† Article_534\n",
      "âœ”ï¸ å·²è™•ç† Article_535\n",
      "âœ”ï¸ å·²è™•ç† Article_536\n",
      "âœ”ï¸ å·²è™•ç† Article_537\n",
      "âœ”ï¸ å·²è™•ç† Article_538\n",
      "âœ”ï¸ å·²è™•ç† Article_539\n",
      "âœ”ï¸ å·²è™•ç† Article_540\n",
      "âœ”ï¸ å·²è™•ç† Article_541\n",
      "âœ”ï¸ å·²è™•ç† Article_542\n",
      "âœ”ï¸ å·²è™•ç† Article_543\n",
      "âœ”ï¸ å·²è™•ç† Article_544\n",
      "âœ”ï¸ å·²è™•ç† Article_545\n",
      "âœ… step2_batch_10.json å·²å®Œæˆä¸¦å„²å­˜è‡³ step2_batches\\step2_batch_10.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç† step1_batches\\step1_batch_11.json â†’ step2_batches\\step2_batch_11.json\n",
      "âœ”ï¸ å·²è™•ç† Article_546\n",
      "âœ”ï¸ å·²è™•ç† Article_547\n",
      "âœ”ï¸ å·²è™•ç† Article_548\n",
      "âœ”ï¸ å·²è™•ç† Article_549\n",
      "âœ”ï¸ å·²è™•ç† Article_550\n",
      "âœ”ï¸ å·²è™•ç† Article_551\n",
      "âœ”ï¸ å·²è™•ç† Article_552\n",
      "âœ”ï¸ å·²è™•ç† Article_553\n",
      "âœ”ï¸ å·²è™•ç† Article_554\n",
      "âœ”ï¸ å·²è™•ç† Article_555\n",
      "âœ”ï¸ å·²è™•ç† Article_556\n",
      "âœ”ï¸ å·²è™•ç† Article_557\n",
      "âœ”ï¸ å·²è™•ç† Article_558\n",
      "âœ”ï¸ å·²è™•ç† Article_559\n",
      "âœ”ï¸ å·²è™•ç† Article_560\n",
      "âœ”ï¸ å·²è™•ç† Article_561\n",
      "âœ”ï¸ å·²è™•ç† Article_562\n",
      "âœ”ï¸ å·²è™•ç† Article_563\n",
      "âœ”ï¸ å·²è™•ç† Article_564\n",
      "âœ”ï¸ å·²è™•ç† Article_565\n",
      "âœ”ï¸ å·²è™•ç† Article_566\n",
      "âœ”ï¸ å·²è™•ç† Article_567\n",
      "âœ”ï¸ å·²è™•ç† Article_568\n",
      "âœ”ï¸ å·²è™•ç† Article_569\n",
      "âœ”ï¸ å·²è™•ç† Article_570\n",
      "âœ”ï¸ å·²è™•ç† Article_571\n",
      "âœ”ï¸ å·²è™•ç† Article_572\n",
      "âœ”ï¸ å·²è™•ç† Article_573\n",
      "âœ”ï¸ å·²è™•ç† Article_574\n",
      "âœ”ï¸ å·²è™•ç† Article_575\n",
      "âœ”ï¸ å·²è™•ç† Article_576\n",
      "âœ”ï¸ å·²è™•ç† Article_577\n",
      "âœ”ï¸ å·²è™•ç† Article_578\n",
      "âœ”ï¸ å·²è™•ç† Article_579\n",
      "âœ”ï¸ å·²è™•ç† Article_580\n",
      "âœ”ï¸ å·²è™•ç† Article_581\n",
      "âœ”ï¸ å·²è™•ç† Article_582\n",
      "âœ”ï¸ å·²è™•ç† Article_583\n",
      "âœ”ï¸ å·²è™•ç† Article_584\n",
      "âœ”ï¸ å·²è™•ç† Article_585\n",
      "âœ”ï¸ å·²è™•ç† Article_586\n",
      "âœ”ï¸ å·²è™•ç† Article_587\n",
      "âœ”ï¸ å·²è™•ç† Article_588\n",
      "âœ”ï¸ å·²è™•ç† Article_589\n",
      "âœ”ï¸ å·²è™•ç† Article_591\n",
      "âœ”ï¸ å·²è™•ç† Article_592\n",
      "âœ”ï¸ å·²è™•ç† Article_593\n",
      "âœ”ï¸ å·²è™•ç† Article_594\n",
      "âœ”ï¸ å·²è™•ç† Article_596\n",
      "âœ”ï¸ å·²è™•ç† Article_597\n",
      "âœ… step2_batch_11.json å·²å®Œæˆä¸¦å„²å­˜è‡³ step2_batches\\step2_batch_11.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç† step1_batches\\step1_batch_12.json â†’ step2_batches\\step2_batch_12.json\n",
      "âœ”ï¸ å·²è™•ç† Article_598\n",
      "âœ”ï¸ å·²è™•ç† Article_599\n",
      "âœ”ï¸ å·²è™•ç† Article_600\n",
      "âœ”ï¸ å·²è™•ç† Article_601\n",
      "âœ”ï¸ å·²è™•ç† Article_602\n",
      "âœ”ï¸ å·²è™•ç† Article_603\n",
      "âœ”ï¸ å·²è™•ç† Article_604\n",
      "âœ”ï¸ å·²è™•ç† Article_605\n",
      "âœ”ï¸ å·²è™•ç† Article_606\n",
      "âœ”ï¸ å·²è™•ç† Article_607\n",
      "âœ”ï¸ å·²è™•ç† Article_609\n",
      "âœ”ï¸ å·²è™•ç† Article_610\n",
      "âœ”ï¸ å·²è™•ç† Article_611\n",
      "âœ”ï¸ å·²è™•ç† Article_613\n",
      "âœ”ï¸ å·²è™•ç† Article_614\n",
      "âœ”ï¸ å·²è™•ç† Article_616\n",
      "âœ”ï¸ å·²è™•ç† Article_617\n",
      "âœ”ï¸ å·²è™•ç† Article_618\n",
      "âœ”ï¸ å·²è™•ç† Article_619\n",
      "âœ”ï¸ å·²è™•ç† Article_620\n",
      "âœ”ï¸ å·²è™•ç† Article_621\n",
      "âœ”ï¸ å·²è™•ç† Article_622\n",
      "âœ”ï¸ å·²è™•ç† Article_623\n",
      "âœ”ï¸ å·²è™•ç† Article_624\n",
      "âœ”ï¸ å·²è™•ç† Article_625\n",
      "âœ”ï¸ å·²è™•ç† Article_626\n",
      "âœ”ï¸ å·²è™•ç† Article_627\n",
      "âœ”ï¸ å·²è™•ç† Article_628\n",
      "âœ”ï¸ å·²è™•ç† Article_629\n",
      "âœ”ï¸ å·²è™•ç† Article_630\n",
      "âœ”ï¸ å·²è™•ç† Article_631\n",
      "âœ”ï¸ å·²è™•ç† Article_632\n",
      "âœ”ï¸ å·²è™•ç† Article_633\n",
      "âœ”ï¸ å·²è™•ç† Article_634\n",
      "âœ… step2_batch_12.json å·²å®Œæˆä¸¦å„²å­˜è‡³ step2_batches\\step2_batch_12.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç† step1_batches\\step1_batch_2.json â†’ step2_batches\\step2_batch_2.json\n",
      "âœ”ï¸ å·²è™•ç† Article_57\n",
      "âœ”ï¸ å·²è™•ç† Article_58\n",
      "âœ”ï¸ å·²è™•ç† Article_59\n",
      "âœ”ï¸ å·²è™•ç† Article_60\n",
      "âœ”ï¸ å·²è™•ç† Article_61\n",
      "âœ”ï¸ å·²è™•ç† Article_62\n",
      "âœ”ï¸ å·²è™•ç† Article_65\n",
      "âœ”ï¸ å·²è™•ç† Article_66\n",
      "âœ”ï¸ å·²è™•ç† Article_67\n",
      "âœ”ï¸ å·²è™•ç† Article_68\n",
      "âœ”ï¸ å·²è™•ç† Article_69\n",
      "âœ”ï¸ å·²è™•ç† Article_70\n",
      "âœ”ï¸ å·²è™•ç† Article_71\n",
      "âœ”ï¸ å·²è™•ç† Article_72\n",
      "âœ”ï¸ å·²è™•ç† Article_73\n",
      "âœ”ï¸ å·²è™•ç† Article_74\n",
      "âœ”ï¸ å·²è™•ç† Article_75\n",
      "âœ”ï¸ å·²è™•ç† Article_76\n",
      "âœ”ï¸ å·²è™•ç† Article_79\n",
      "âœ”ï¸ å·²è™•ç† Article_80\n",
      "âœ”ï¸ å·²è™•ç† Article_81\n",
      "âœ”ï¸ å·²è™•ç† Article_82\n",
      "âœ”ï¸ å·²è™•ç† Article_83\n",
      "âœ”ï¸ å·²è™•ç† Article_84\n",
      "âœ”ï¸ å·²è™•ç† Article_85\n",
      "âœ”ï¸ å·²è™•ç† Article_86\n",
      "âœ”ï¸ å·²è™•ç† Article_87\n",
      "âœ”ï¸ å·²è™•ç† Article_88\n",
      "âœ”ï¸ å·²è™•ç† Article_89\n",
      "âœ”ï¸ å·²è™•ç† Article_90\n",
      "âœ”ï¸ å·²è™•ç† Article_91\n",
      "âœ”ï¸ å·²è™•ç† Article_92\n",
      "âœ”ï¸ å·²è™•ç† Article_93\n",
      "âœ”ï¸ å·²è™•ç† Article_94\n",
      "âœ”ï¸ å·²è™•ç† Article_96\n",
      "âœ”ï¸ å·²è™•ç† Article_97\n",
      "âœ”ï¸ å·²è™•ç† Article_98\n",
      "âœ”ï¸ å·²è™•ç† Article_99\n",
      "âœ”ï¸ å·²è™•ç† Article_100\n",
      "âœ”ï¸ å·²è™•ç† Article_101\n",
      "âœ”ï¸ å·²è™•ç† Article_102\n",
      "âœ”ï¸ å·²è™•ç† Article_103\n",
      "âœ”ï¸ å·²è™•ç† Article_104\n",
      "âœ”ï¸ å·²è™•ç† Article_105\n",
      "âœ”ï¸ å·²è™•ç† Article_107\n",
      "âœ”ï¸ å·²è™•ç† Article_108\n",
      "âœ”ï¸ å·²è™•ç† Article_109\n",
      "âœ”ï¸ å·²è™•ç† Article_110\n",
      "âœ”ï¸ å·²è™•ç† Article_112\n",
      "âœ”ï¸ å·²è™•ç† Article_113\n",
      "âœ… step2_batch_2.json å·²å®Œæˆä¸¦å„²å­˜è‡³ step2_batches\\step2_batch_2.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç† step1_batches\\step1_batch_3.json â†’ step2_batches\\step2_batch_3.json\n",
      "âœ”ï¸ å·²è™•ç† Article_114\n",
      "âœ”ï¸ å·²è™•ç† Article_115\n",
      "âœ”ï¸ å·²è™•ç† Article_116\n",
      "âœ”ï¸ å·²è™•ç† Article_117\n",
      "âœ”ï¸ å·²è™•ç† Article_118\n",
      "âœ”ï¸ å·²è™•ç† Article_119\n",
      "âœ”ï¸ å·²è™•ç† Article_120\n",
      "âœ”ï¸ å·²è™•ç† Article_121\n",
      "âœ”ï¸ å·²è™•ç† Article_122\n",
      "âœ”ï¸ å·²è™•ç† Article_123\n",
      "âœ”ï¸ å·²è™•ç† Article_124\n",
      "âœ”ï¸ å·²è™•ç† Article_125\n",
      "âœ”ï¸ å·²è™•ç† Article_126\n",
      "âœ”ï¸ å·²è™•ç† Article_127\n",
      "âœ”ï¸ å·²è™•ç† Article_128\n",
      "âœ”ï¸ å·²è™•ç† Article_129\n",
      "âœ”ï¸ å·²è™•ç† Article_130\n",
      "âœ”ï¸ å·²è™•ç† Article_131\n",
      "âœ”ï¸ å·²è™•ç† Article_132\n",
      "âœ”ï¸ å·²è™•ç† Article_133\n",
      "âœ”ï¸ å·²è™•ç† Article_134\n",
      "âœ”ï¸ å·²è™•ç† Article_136\n",
      "âœ”ï¸ å·²è™•ç† Article_137\n",
      "âœ”ï¸ å·²è™•ç† Article_139\n",
      "âœ”ï¸ å·²è™•ç† Article_140\n",
      "âœ”ï¸ å·²è™•ç† Article_141\n",
      "âœ”ï¸ å·²è™•ç† Article_142\n",
      "âœ”ï¸ å·²è™•ç† Article_143\n",
      "âœ”ï¸ å·²è™•ç† Article_144\n",
      "âœ”ï¸ å·²è™•ç† Article_145\n",
      "âœ”ï¸ å·²è™•ç† Article_147\n",
      "âœ”ï¸ å·²è™•ç† Article_148\n",
      "âœ”ï¸ å·²è™•ç† Article_149\n",
      "âœ”ï¸ å·²è™•ç† Article_150\n",
      "âœ”ï¸ å·²è™•ç† Article_151\n",
      "âœ”ï¸ å·²è™•ç† Article_152\n",
      "âœ”ï¸ å·²è™•ç† Article_153\n",
      "âœ”ï¸ å·²è™•ç† Article_154\n",
      "âœ”ï¸ å·²è™•ç† Article_155\n",
      "âœ”ï¸ å·²è™•ç† Article_156\n",
      "âœ”ï¸ å·²è™•ç† Article_157\n",
      "âœ”ï¸ å·²è™•ç† Article_158\n",
      "âœ”ï¸ å·²è™•ç† Article_159\n",
      "âœ”ï¸ å·²è™•ç† Article_160\n",
      "âœ”ï¸ å·²è™•ç† Article_161\n",
      "âœ”ï¸ å·²è™•ç† Article_162\n",
      "âœ”ï¸ å·²è™•ç† Article_163\n",
      "âœ”ï¸ å·²è™•ç† Article_164\n",
      "âœ”ï¸ å·²è™•ç† Article_165\n",
      "âœ”ï¸ å·²è™•ç† Article_166\n",
      "âœ… step2_batch_3.json å·²å®Œæˆä¸¦å„²å­˜è‡³ step2_batches\\step2_batch_3.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç† step1_batches\\step1_batch_4.json â†’ step2_batches\\step2_batch_4.json\n",
      "âœ”ï¸ å·²è™•ç† Article_167\n",
      "âœ”ï¸ å·²è™•ç† Article_168\n",
      "âœ”ï¸ å·²è™•ç† Article_169\n",
      "âœ”ï¸ å·²è™•ç† Article_170\n",
      "âœ”ï¸ å·²è™•ç† Article_171\n",
      "âœ”ï¸ å·²è™•ç† Article_172\n",
      "âœ”ï¸ å·²è™•ç† Article_173\n",
      "âœ”ï¸ å·²è™•ç† Article_174\n",
      "âœ”ï¸ å·²è™•ç† Article_175\n",
      "âœ”ï¸ å·²è™•ç† Article_176\n",
      "âœ”ï¸ å·²è™•ç† Article_177\n",
      "âœ”ï¸ å·²è™•ç† Article_178\n",
      "âœ”ï¸ å·²è™•ç† Article_179\n",
      "âœ”ï¸ å·²è™•ç† Article_180\n",
      "âœ”ï¸ å·²è™•ç† Article_181\n",
      "âœ”ï¸ å·²è™•ç† Article_182\n",
      "âœ”ï¸ å·²è™•ç† Article_183\n",
      "âœ”ï¸ å·²è™•ç† Article_184\n",
      "âœ”ï¸ å·²è™•ç† Article_185\n",
      "âœ”ï¸ å·²è™•ç† Article_186\n",
      "âœ”ï¸ å·²è™•ç† Article_187\n",
      "âœ”ï¸ å·²è™•ç† Article_188\n",
      "âœ”ï¸ å·²è™•ç† Article_189\n",
      "âœ”ï¸ å·²è™•ç† Article_190\n",
      "âœ”ï¸ å·²è™•ç† Article_191\n",
      "âœ”ï¸ å·²è™•ç† Article_192\n",
      "âœ”ï¸ å·²è™•ç† Article_193\n",
      "âœ”ï¸ å·²è™•ç† Article_194\n",
      "âœ”ï¸ å·²è™•ç† Article_195\n",
      "âœ”ï¸ å·²è™•ç† Article_196\n",
      "âœ”ï¸ å·²è™•ç† Article_197\n",
      "âœ”ï¸ å·²è™•ç† Article_198\n",
      "âœ”ï¸ å·²è™•ç† Article_199\n",
      "âœ”ï¸ å·²è™•ç† Article_200\n",
      "âœ”ï¸ å·²è™•ç† Article_201\n",
      "âœ”ï¸ å·²è™•ç† Article_202\n",
      "âœ”ï¸ å·²è™•ç† Article_203\n",
      "âœ”ï¸ å·²è™•ç† Article_204\n",
      "âœ”ï¸ å·²è™•ç† Article_205\n",
      "âœ”ï¸ å·²è™•ç† Article_206\n",
      "âœ”ï¸ å·²è™•ç† Article_207\n",
      "âœ”ï¸ å·²è™•ç† Article_208\n",
      "âœ”ï¸ å·²è™•ç† Article_209\n",
      "âœ”ï¸ å·²è™•ç† Article_210\n",
      "âœ”ï¸ å·²è™•ç† Article_211\n",
      "âœ”ï¸ å·²è™•ç† Article_212\n",
      "âœ”ï¸ å·²è™•ç† Article_213\n",
      "âœ”ï¸ å·²è™•ç† Article_214\n",
      "âœ”ï¸ å·²è™•ç† Article_216\n",
      "âœ”ï¸ å·²è™•ç† Article_217\n",
      "âœ… step2_batch_4.json å·²å®Œæˆä¸¦å„²å­˜è‡³ step2_batches\\step2_batch_4.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç† step1_batches\\step1_batch_5.json â†’ step2_batches\\step2_batch_5.json\n",
      "âœ”ï¸ å·²è™•ç† Article_218\n",
      "âœ”ï¸ å·²è™•ç† Article_219\n",
      "âœ”ï¸ å·²è™•ç† Article_220\n",
      "âœ”ï¸ å·²è™•ç† Article_221\n",
      "âœ”ï¸ å·²è™•ç† Article_222\n",
      "âœ”ï¸ å·²è™•ç† Article_223\n",
      "âœ”ï¸ å·²è™•ç† Article_224\n",
      "âœ”ï¸ å·²è™•ç† Article_225\n",
      "âœ”ï¸ å·²è™•ç† Article_226\n",
      "âœ”ï¸ å·²è™•ç† Article_227\n",
      "âœ”ï¸ å·²è™•ç† Article_228\n",
      "âœ”ï¸ å·²è™•ç† Article_230\n",
      "âœ”ï¸ å·²è™•ç† Article_231\n",
      "âœ”ï¸ å·²è™•ç† Article_232\n",
      "âœ”ï¸ å·²è™•ç† Article_233\n",
      "âœ”ï¸ å·²è™•ç† Article_235\n",
      "âœ”ï¸ å·²è™•ç† Article_236\n",
      "âœ”ï¸ å·²è™•ç† Article_237\n",
      "âœ”ï¸ å·²è™•ç† Article_238\n",
      "âœ”ï¸ å·²è™•ç† Article_239\n",
      "âœ”ï¸ å·²è™•ç† Article_240\n",
      "âœ”ï¸ å·²è™•ç† Article_241\n",
      "âœ”ï¸ å·²è™•ç† Article_242\n",
      "âœ”ï¸ å·²è™•ç† Article_243\n",
      "âœ”ï¸ å·²è™•ç† Article_244\n",
      "âœ”ï¸ å·²è™•ç† Article_245\n",
      "âœ”ï¸ å·²è™•ç† Article_246\n",
      "âœ”ï¸ å·²è™•ç† Article_247\n",
      "âœ”ï¸ å·²è™•ç† Article_248\n",
      "âœ”ï¸ å·²è™•ç† Article_249\n",
      "âœ”ï¸ å·²è™•ç† Article_250\n",
      "âœ”ï¸ å·²è™•ç† Article_251\n",
      "âœ”ï¸ å·²è™•ç† Article_252\n",
      "âœ”ï¸ å·²è™•ç† Article_253\n",
      "âœ”ï¸ å·²è™•ç† Article_255\n",
      "âœ”ï¸ å·²è™•ç† Article_256\n",
      "âœ”ï¸ å·²è™•ç† Article_257\n",
      "âœ”ï¸ å·²è™•ç† Article_258\n",
      "âœ”ï¸ å·²è™•ç† Article_259\n",
      "âœ”ï¸ å·²è™•ç† Article_260\n",
      "âœ”ï¸ å·²è™•ç† Article_262\n",
      "âœ”ï¸ å·²è™•ç† Article_263\n",
      "âœ”ï¸ å·²è™•ç† Article_264\n",
      "âœ”ï¸ å·²è™•ç† Article_265\n",
      "âœ”ï¸ å·²è™•ç† Article_266\n",
      "âœ”ï¸ å·²è™•ç† Article_267\n",
      "âœ”ï¸ å·²è™•ç† Article_268\n",
      "âœ”ï¸ å·²è™•ç† Article_270\n",
      "âœ”ï¸ å·²è™•ç† Article_272\n",
      "âœ”ï¸ å·²è™•ç† Article_273\n",
      "âœ… step2_batch_5.json å·²å®Œæˆä¸¦å„²å­˜è‡³ step2_batches\\step2_batch_5.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç† step1_batches\\step1_batch_6.json â†’ step2_batches\\step2_batch_6.json\n",
      "âœ”ï¸ å·²è™•ç† Article_274\n",
      "âœ”ï¸ å·²è™•ç† Article_275\n",
      "âœ”ï¸ å·²è™•ç† Article_276\n",
      "âœ”ï¸ å·²è™•ç† Article_277\n",
      "âœ”ï¸ å·²è™•ç† Article_278\n",
      "âœ”ï¸ å·²è™•ç† Article_279\n",
      "âœ”ï¸ å·²è™•ç† Article_280\n",
      "âœ”ï¸ å·²è™•ç† Article_281\n",
      "âœ”ï¸ å·²è™•ç† Article_282\n",
      "âœ”ï¸ å·²è™•ç† Article_283\n",
      "âœ”ï¸ å·²è™•ç† Article_284\n",
      "âœ”ï¸ å·²è™•ç† Article_285\n",
      "âœ”ï¸ å·²è™•ç† Article_286\n",
      "âœ”ï¸ å·²è™•ç† Article_288\n",
      "âœ”ï¸ å·²è™•ç† Article_290\n",
      "âœ”ï¸ å·²è™•ç† Article_291\n",
      "âœ”ï¸ å·²è™•ç† Article_292\n",
      "âœ”ï¸ å·²è™•ç† Article_293\n",
      "âœ”ï¸ å·²è™•ç† Article_294\n",
      "âœ”ï¸ å·²è™•ç† Article_296\n",
      "âœ”ï¸ å·²è™•ç† Article_297\n",
      "âœ”ï¸ å·²è™•ç† Article_298\n",
      "âœ”ï¸ å·²è™•ç† Article_299\n",
      "âœ”ï¸ å·²è™•ç† Article_300\n",
      "âœ”ï¸ å·²è™•ç† Article_301\n",
      "âœ”ï¸ å·²è™•ç† Article_302\n",
      "âœ”ï¸ å·²è™•ç† Article_303\n",
      "âœ”ï¸ å·²è™•ç† Article_304\n",
      "âœ”ï¸ å·²è™•ç† Article_305\n",
      "âœ”ï¸ å·²è™•ç† Article_306\n",
      "âœ”ï¸ å·²è™•ç† Article_307\n",
      "âœ”ï¸ å·²è™•ç† Article_308\n",
      "âœ”ï¸ å·²è™•ç† Article_309\n",
      "âœ”ï¸ å·²è™•ç† Article_310\n",
      "âœ”ï¸ å·²è™•ç† Article_311\n",
      "âœ”ï¸ å·²è™•ç† Article_312\n",
      "âœ”ï¸ å·²è™•ç† Article_313\n",
      "âœ”ï¸ å·²è™•ç† Article_314\n",
      "âœ”ï¸ å·²è™•ç† Article_315\n",
      "âœ”ï¸ å·²è™•ç† Article_316\n",
      "âœ”ï¸ å·²è™•ç† Article_317\n",
      "âœ”ï¸ å·²è™•ç† Article_318\n",
      "âœ”ï¸ å·²è™•ç† Article_319\n",
      "âœ”ï¸ å·²è™•ç† Article_320\n",
      "âœ”ï¸ å·²è™•ç† Article_321\n",
      "âœ”ï¸ å·²è™•ç† Article_322\n",
      "âœ”ï¸ å·²è™•ç† Article_323\n",
      "âœ”ï¸ å·²è™•ç† Article_324\n",
      "âœ”ï¸ å·²è™•ç† Article_325\n",
      "âœ”ï¸ å·²è™•ç† Article_326\n",
      "âœ… step2_batch_6.json å·²å®Œæˆä¸¦å„²å­˜è‡³ step2_batches\\step2_batch_6.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç† step1_batches\\step1_batch_7.json â†’ step2_batches\\step2_batch_7.json\n",
      "âœ”ï¸ å·²è™•ç† Article_327\n",
      "âœ”ï¸ å·²è™•ç† Article_328\n",
      "âœ”ï¸ å·²è™•ç† Article_329\n",
      "âœ”ï¸ å·²è™•ç† Article_330\n",
      "âœ”ï¸ å·²è™•ç† Article_331\n",
      "âœ”ï¸ å·²è™•ç† Article_332\n",
      "âœ”ï¸ å·²è™•ç† Article_333\n",
      "âœ”ï¸ å·²è™•ç† Article_334\n",
      "âœ”ï¸ å·²è™•ç† Article_335\n",
      "âœ”ï¸ å·²è™•ç† Article_336\n",
      "âœ”ï¸ å·²è™•ç† Article_337\n",
      "âœ”ï¸ å·²è™•ç† Article_338\n",
      "âœ”ï¸ å·²è™•ç† Article_339\n",
      "âœ”ï¸ å·²è™•ç† Article_340\n",
      "âœ”ï¸ å·²è™•ç† Article_341\n",
      "âœ”ï¸ å·²è™•ç† Article_342\n",
      "âœ”ï¸ å·²è™•ç† Article_343\n",
      "âœ”ï¸ å·²è™•ç† Article_344\n",
      "âœ”ï¸ å·²è™•ç† Article_345\n",
      "âœ”ï¸ å·²è™•ç† Article_346\n",
      "âœ”ï¸ å·²è™•ç† Article_347\n",
      "âœ”ï¸ å·²è™•ç† Article_348\n",
      "âœ”ï¸ å·²è™•ç† Article_349\n",
      "âœ”ï¸ å·²è™•ç† Article_350\n",
      "âœ”ï¸ å·²è™•ç† Article_351\n",
      "âœ”ï¸ å·²è™•ç† Article_352\n",
      "âœ”ï¸ å·²è™•ç† Article_355\n",
      "âœ”ï¸ å·²è™•ç† Article_356\n",
      "âœ”ï¸ å·²è™•ç† Article_357\n",
      "âœ”ï¸ å·²è™•ç† Article_358\n",
      "âœ”ï¸ å·²è™•ç† Article_359\n",
      "âœ”ï¸ å·²è™•ç† Article_360\n",
      "âœ”ï¸ å·²è™•ç† Article_361\n",
      "âœ”ï¸ å·²è™•ç† Article_363\n",
      "âœ”ï¸ å·²è™•ç† Article_364\n",
      "âœ”ï¸ å·²è™•ç† Article_365\n",
      "âœ”ï¸ å·²è™•ç† Article_366\n",
      "âœ”ï¸ å·²è™•ç† Article_367\n",
      "âœ”ï¸ å·²è™•ç† Article_368\n",
      "âœ”ï¸ å·²è™•ç† Article_369\n",
      "âœ”ï¸ å·²è™•ç† Article_370\n",
      "âœ”ï¸ å·²è™•ç† Article_371\n",
      "âœ”ï¸ å·²è™•ç† Article_373\n",
      "âœ”ï¸ å·²è™•ç† Article_374\n",
      "âœ”ï¸ å·²è™•ç† Article_375\n",
      "âœ”ï¸ å·²è™•ç† Article_376\n",
      "âœ”ï¸ å·²è™•ç† Article_377\n",
      "âœ”ï¸ å·²è™•ç† Article_379\n",
      "âœ”ï¸ å·²è™•ç† Article_380\n",
      "âœ”ï¸ å·²è™•ç† Article_381\n",
      "âœ… step2_batch_7.json å·²å®Œæˆä¸¦å„²å­˜è‡³ step2_batches\\step2_batch_7.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç† step1_batches\\step1_batch_8.json â†’ step2_batches\\step2_batch_8.json\n",
      "âœ”ï¸ å·²è™•ç† Article_382\n",
      "âœ”ï¸ å·²è™•ç† Article_383\n",
      "âœ”ï¸ å·²è™•ç† Article_384\n",
      "âœ”ï¸ å·²è™•ç† Article_385\n",
      "âœ”ï¸ å·²è™•ç† Article_386\n",
      "âœ”ï¸ å·²è™•ç† Article_387\n",
      "âœ”ï¸ å·²è™•ç† Article_388\n",
      "âœ”ï¸ å·²è™•ç† Article_389\n",
      "âœ”ï¸ å·²è™•ç† Article_390\n",
      "âœ”ï¸ å·²è™•ç† Article_391\n",
      "âœ”ï¸ å·²è™•ç† Article_392\n",
      "âœ”ï¸ å·²è™•ç† Article_393\n",
      "âœ”ï¸ å·²è™•ç† Article_394\n",
      "âœ”ï¸ å·²è™•ç† Article_395\n",
      "âœ”ï¸ å·²è™•ç† Article_396\n",
      "âœ”ï¸ å·²è™•ç† Article_397\n",
      "âœ”ï¸ å·²è™•ç† Article_398\n",
      "âœ”ï¸ å·²è™•ç† Article_399\n",
      "âœ”ï¸ å·²è™•ç† Article_400\n",
      "âœ”ï¸ å·²è™•ç† Article_401\n",
      "âœ”ï¸ å·²è™•ç† Article_402\n",
      "âœ”ï¸ å·²è™•ç† Article_403\n",
      "âœ”ï¸ å·²è™•ç† Article_404\n",
      "âœ”ï¸ å·²è™•ç† Article_405\n",
      "âœ”ï¸ å·²è™•ç† Article_406\n",
      "âœ”ï¸ å·²è™•ç† Article_409\n",
      "âœ”ï¸ å·²è™•ç† Article_410\n",
      "âœ”ï¸ å·²è™•ç† Article_411\n",
      "âœ”ï¸ å·²è™•ç† Article_412\n",
      "âœ”ï¸ å·²è™•ç† Article_413\n",
      "âœ”ï¸ å·²è™•ç† Article_414\n",
      "âœ”ï¸ å·²è™•ç† Article_415\n",
      "âœ”ï¸ å·²è™•ç† Article_416\n",
      "âœ”ï¸ å·²è™•ç† Article_417\n",
      "âœ”ï¸ å·²è™•ç† Article_418\n",
      "âœ”ï¸ å·²è™•ç† Article_419\n",
      "âœ”ï¸ å·²è™•ç† Article_420\n",
      "âœ”ï¸ å·²è™•ç† Article_421\n",
      "âœ”ï¸ å·²è™•ç† Article_422\n",
      "âœ”ï¸ å·²è™•ç† Article_423\n",
      "âœ”ï¸ å·²è™•ç† Article_424\n",
      "âœ”ï¸ å·²è™•ç† Article_425\n",
      "âœ”ï¸ å·²è™•ç† Article_426\n",
      "âœ”ï¸ å·²è™•ç† Article_427\n",
      "âœ”ï¸ å·²è™•ç† Article_428\n",
      "âœ”ï¸ å·²è™•ç† Article_429\n",
      "âœ”ï¸ å·²è™•ç† Article_430\n",
      "âœ”ï¸ å·²è™•ç† Article_431\n",
      "âœ”ï¸ å·²è™•ç† Article_433\n",
      "âœ”ï¸ å·²è™•ç† Article_434\n",
      "âœ… step2_batch_8.json å·²å®Œæˆä¸¦å„²å­˜è‡³ step2_batches\\step2_batch_8.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç† step1_batches\\step1_batch_9.json â†’ step2_batches\\step2_batch_9.json\n",
      "âœ”ï¸ å·²è™•ç† Article_435\n",
      "âœ”ï¸ å·²è™•ç† Article_436\n",
      "âœ”ï¸ å·²è™•ç† Article_437\n",
      "âœ”ï¸ å·²è™•ç† Article_438\n",
      "âœ”ï¸ å·²è™•ç† Article_439\n",
      "âœ”ï¸ å·²è™•ç† Article_440\n",
      "âœ”ï¸ å·²è™•ç† Article_441\n",
      "âœ”ï¸ å·²è™•ç† Article_442\n",
      "âœ”ï¸ å·²è™•ç† Article_443\n",
      "âœ”ï¸ å·²è™•ç† Article_445\n",
      "âœ”ï¸ å·²è™•ç† Article_446\n",
      "âœ”ï¸ å·²è™•ç† Article_448\n",
      "âœ”ï¸ å·²è™•ç† Article_449\n",
      "âœ”ï¸ å·²è™•ç† Article_450\n",
      "âœ”ï¸ å·²è™•ç† Article_451\n",
      "âœ”ï¸ å·²è™•ç† Article_452\n",
      "âœ”ï¸ å·²è™•ç† Article_453\n",
      "âœ”ï¸ å·²è™•ç† Article_454\n",
      "âœ”ï¸ å·²è™•ç† Article_455\n",
      "âœ”ï¸ å·²è™•ç† Article_456\n",
      "âœ”ï¸ å·²è™•ç† Article_457\n",
      "âœ”ï¸ å·²è™•ç† Article_458\n",
      "âœ”ï¸ å·²è™•ç† Article_459\n",
      "âœ”ï¸ å·²è™•ç† Article_460\n",
      "âœ”ï¸ å·²è™•ç† Article_461\n",
      "âœ”ï¸ å·²è™•ç† Article_463\n",
      "âœ”ï¸ å·²è™•ç† Article_465\n",
      "âœ”ï¸ å·²è™•ç† Article_466\n",
      "âœ”ï¸ å·²è™•ç† Article_467\n",
      "âœ”ï¸ å·²è™•ç† Article_468\n",
      "âœ”ï¸ å·²è™•ç† Article_469\n",
      "âœ”ï¸ å·²è™•ç† Article_470\n",
      "âœ”ï¸ å·²è™•ç† Article_471\n",
      "âœ”ï¸ å·²è™•ç† Article_473\n",
      "âœ”ï¸ å·²è™•ç† Article_474\n",
      "âœ”ï¸ å·²è™•ç† Article_475\n",
      "âœ”ï¸ å·²è™•ç† Article_476\n",
      "âœ”ï¸ å·²è™•ç† Article_477\n",
      "âœ”ï¸ å·²è™•ç† Article_478\n",
      "âœ”ï¸ å·²è™•ç† Article_479\n",
      "âœ”ï¸ å·²è™•ç† Article_480\n",
      "âœ”ï¸ å·²è™•ç† Article_481\n",
      "âœ”ï¸ å·²è™•ç† Article_483\n",
      "âœ”ï¸ å·²è™•ç† Article_484\n",
      "API èª¿ç”¨éŒ¯èª¤: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "âœ”ï¸ å·²è™•ç† Article_485\n",
      "API èª¿ç”¨éŒ¯èª¤: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "âœ”ï¸ å·²è™•ç† Article_486\n",
      "API èª¿ç”¨éŒ¯èª¤: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "âœ”ï¸ å·²è™•ç† Article_487\n",
      "API èª¿ç”¨éŒ¯èª¤: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "âœ”ï¸ å·²è™•ç† Article_488\n",
      "API èª¿ç”¨éŒ¯èª¤: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "âœ”ï¸ å·²è™•ç† Article_489\n",
      "API èª¿ç”¨éŒ¯èª¤: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "âœ”ï¸ å·²è™•ç† Article_490\n",
      "âœ… step2_batch_9.json å·²å®Œæˆä¸¦å„²å­˜è‡³ step2_batches\\step2_batch_9.json\n",
      "\n",
      "ğŸ‰ Step 2 å…¨éƒ¨æ‰¹æ¬¡è™•ç†å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "step2_prompt = \"\"\"You are a sociology professor with 30 years of experience analyzing Asian racism in the United States.\n",
    "\n",
    "**Step 2: Extract Related Sentences**\n",
    "\n",
    "Based on the named entities identified in **Step 1**, extract **all relevant complete sentences** from the text for each entity.\n",
    "\n",
    "**For each named entity:**\n",
    "1. Use the `\"reference\"` field (the phrase as it appears in the article) to identify relevant sentences.\n",
    "2. Identify **all complete, verbatim sentences** in the text that mention or describe the entityâ€™s **involvement, action, reaction, statement, or experience** related to anti-Asian hate (directly or indirectly).\n",
    "3. Do **not paraphrase** or summarize. Use the **exact wording** from the text.\n",
    "4. If no relevant sentence is found, set `\"relevant_sentences\": []`.\n",
    "5. Return results **grouped by entity name**, exactly matching the names used in Step 1.\n",
    "6. Include the following structured metadata for each entity:\n",
    "   - `\"entity_type\"`:  \n",
    "     - For individuals: the **social role** (e.g., \"politician\", \"celebrity\", \"victim\")  \n",
    "     - For organizations: the **institutional category** (e.g., \"law_enforcement_agency\", \"ngo_or_advocacy_group\")\n",
    "   - `\"asian_status\"`:  \n",
    "     - For individuals: **\"Asian\"**, **\"Non-Asian\"**, or **\"Cannot be inferred\"**  \n",
    "     - For organizations: Always **\"Not applicable\"**\n",
    "\n",
    "**Note:** These sentences will later be used to infer **behavioral reactions** and **emotional responses**, so include **any sentence** that provides context about what the entity did, said, or experienced.\n",
    "\n",
    "### Output format (JSON):\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"Entity Name\": {\n",
    "    \"entity_type\": \"e.g., politician, law_enforcement_agency\",\n",
    "    \"asian_status\": \"Asian / Non-Asian / Cannot be inferred / Not applicable\",\n",
    "    \"relevant_sentences\": [\n",
    "      \"Sentence 1 from the article.\",\n",
    "      \"Sentence 2 from the article.\"\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# å»ºç«‹è¼¸å‡ºè³‡æ–™å¤¾\n",
    "output_dir = \"step2_batches\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# æ‰¾å‡º Step 1 çš„æ‰€æœ‰æ‰¹æ¬¡æª”æ¡ˆ\n",
    "step1_files = sorted(glob.glob(\"step1_batches/step1_batch_*.json\"))\n",
    "\n",
    "for step1_file in step1_files:\n",
    "    batch_name = os.path.basename(step1_file).replace(\"step1_\", \"step2_\")\n",
    "    output_file = os.path.join(output_dir, batch_name)\n",
    "\n",
    "    # å¦‚æœå·²ç¶“æœ‰ Step 2 çš„çµæœï¼Œå°±è·³é\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"â­ï¸ {batch_name} å·²å­˜åœ¨ï¼Œè·³é\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nğŸš€ é–‹å§‹è™•ç† {step1_file} â†’ {output_file}\")\n",
    "\n",
    "    # è®€å– Step 1 çš„çµæœ\n",
    "    with open(step1_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        step1_result = json.load(f)\n",
    "\n",
    "    step2_result = {}\n",
    "\n",
    "    # é€ç¯‡æ–‡ç« è™•ç†\n",
    "    for title, step1_text in step1_result.items():\n",
    "        content = articles.get(title, \"\")  # å¾åŸå§‹æ–‡ç«  dict æ‹¿å…§å®¹\n",
    "\n",
    "        full_prompt = (\n",
    "            step2_prompt +\n",
    "            f\"\\n\\nStep 1 Results:\\n{step1_text}\" +\n",
    "            f\"\\n\\nOriginal Article Text:\\n{content}\"\n",
    "        )\n",
    "\n",
    "        response = get_response(full_prompt)\n",
    "        step2_result[title] = response\n",
    "        print(f\"âœ”ï¸ å·²è™•ç† {title}\")\n",
    "\n",
    "    # å„²å­˜é€™ä¸€æ‰¹çš„ Step 2 çµæœ\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(step2_result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"âœ… {batch_name} å·²å®Œæˆä¸¦å„²å­˜è‡³ {output_file}\")\n",
    "\n",
    "print(\"\\nğŸ‰ Step 2 å…¨éƒ¨æ‰¹æ¬¡è™•ç†å®Œæˆï¼\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc2b10c2-8265-45e8-8c31-e98d7f31ab67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ é‡æ–°è™•ç† Article_485\n",
      "ğŸ”„ é‡æ–°è™•ç† Article_486\n",
      "ğŸ”„ é‡æ–°è™•ç† Article_487\n",
      "ğŸ”„ é‡æ–°è™•ç† Article_488\n",
      "ğŸ”„ é‡æ–°è™•ç† Article_489\n",
      "ğŸ”„ é‡æ–°è™•ç† Article_490\n",
      "âœ… å·²æ›´æ–° step2_batches/step2_batch_9.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def rerun_incomplete_step2(step1_file, step2_file):\n",
    "    # è®€å– Step1 çš„çµæœ\n",
    "    with open(step1_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        step1_result = json.load(f)\n",
    "\n",
    "    # å¦‚æœ step2_file å·²å­˜åœ¨ï¼Œå…ˆè®€å–ï¼›å¦å‰‡æ–°å»º\n",
    "    if os.path.exists(step2_file):\n",
    "        with open(step2_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            step2_result = json.load(f)\n",
    "    else:\n",
    "        step2_result = {}\n",
    "\n",
    "    updated = False\n",
    "\n",
    "    for title, step1_text in step1_result.items():\n",
    "        # åªè™•ç†ç©ºçš„ or ç¼ºå¤±çš„\n",
    "        if not step2_result.get(title):\n",
    "            content = articles.get(title, \"\")  # åŸå§‹æ–‡ç« \n",
    "            full_prompt = (\n",
    "                step2_prompt +\n",
    "                f\"\\n\\nStep 1 Results:\\n{step1_text}\" +\n",
    "                f\"\\n\\nOriginal Article Text:\\n{content}\"\n",
    "            )\n",
    "            response = get_response(full_prompt)\n",
    "            step2_result[title] = response\n",
    "            print(f\"ğŸ”„ é‡æ–°è™•ç† {title}\")\n",
    "            updated = True\n",
    "\n",
    "    if updated:\n",
    "        with open(step2_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(step2_result, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"âœ… å·²æ›´æ–° {step2_file}\")\n",
    "    else:\n",
    "        print(\"ğŸ‘Œ æ²’æœ‰éœ€è¦è£œè·‘çš„æ–‡ç« ï¼Œå…¨æ•¸å®Œæˆ\")\n",
    "\n",
    "# ä½¿ç”¨ç¯„ä¾‹\n",
    "rerun_incomplete_step2(\n",
    "    step1_file=\"step1_batches/step1_batch_9.json\",\n",
    "    step2_file=\"step2_batches/step2_batch_9.json\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e1c2ee9-87b3-42bb-8bef-d271575ff76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 1/12: step2_batches\\step2_batch_1.json\n",
      "âœ… æ‰¹æ¬¡ 1/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step3_batches\\step3_batch_1.json\n",
      "   Gate stats: {'no': 426, 'yes': 174}\n",
      "   Label stats: {'Cannot be inferred': 466, 'Support Asian Americans': 86, 'Government takes actions to stop AAPI hate': 30, 'Advocacy/take actions for changes': 4, 'Attending marches/rallies': 1, 'Politicians initiated anti-Asian hate and racism': 4, 'Color blind/minimizing racism': 1, 'Fostering conversations about anti-Asian hate': 2, 'Useless law enforcement': 2, 'Hiring security guards': 2, 'Educating students': 1, 'Videotaping/confronting harasser/attacker': 1}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 2/12: step2_batches\\step2_batch_10.json\n",
      "âœ… æ‰¹æ¬¡ 2/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step3_batches\\step3_batch_10.json\n",
      "   Gate stats: {'no': 285, 'yes': 131}\n",
      "   Label stats: {'Cannot be inferred': 296, 'Support Asian Americans': 56, 'Government takes actions to stop AAPI hate': 51, 'Attending marches/rallies': 2, 'Advocacy/take actions for changes': 5, 'Politicians initiated anti-Asian hate and racism': 2, 'Color blind/minimizing racism': 1, 'Videotaping/confronting harasser/attacker': 2, 'Educating students': 1}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 3/12: step2_batches\\step2_batch_11.json\n",
      "âœ… æ‰¹æ¬¡ 3/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step3_batches\\step3_batch_11.json\n",
      "   Gate stats: {'yes': 164, 'no': 411}\n",
      "   Label stats: {'Support Asian Americans': 80, 'Cannot be inferred': 431, 'Government takes actions to stop AAPI hate': 46, 'Videotaping/confronting harasser/attacker': 5, 'Hiring security guards': 1, 'Politicians initiated anti-Asian hate and racism': 1, 'Speaking up on social media': 2, 'Advocacy/take actions for changes': 5, 'Attending marches/rallies': 1, 'Useless law enforcement': 1, 'Color blind/minimizing racism': 1, 'Feeling hopeless or support AAPI being not enough': 1}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 4/12: step2_batches\\step2_batch_12.json\n",
      "âœ… æ‰¹æ¬¡ 4/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step3_batches\\step3_batch_12.json\n",
      "   Gate stats: {'yes': 105, 'no': 237}\n",
      "   Label stats: {'Government takes actions to stop AAPI hate': 34, 'Support Asian Americans': 52, 'Cannot be inferred': 246, 'Advocacy/take actions for changes': 7, 'Videotaping/confronting harasser/attacker': 2, 'Politicians initiated anti-Asian hate and racism': 1}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 5/12: step2_batches\\step2_batch_2.json\n",
      "âœ… æ‰¹æ¬¡ 5/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step3_batches\\step3_batch_2.json\n",
      "   Gate stats: {'no': 343, 'yes': 123}\n",
      "   Label stats: {'Cannot be inferred': 369, 'Support Asian Americans': 49, 'Useless law enforcement': 1, 'Government takes actions to stop AAPI hate': 29, 'Politicians initiated anti-Asian hate and racism': 2, 'Attending marches/rallies': 2, 'Advocacy/take actions for changes': 9, 'Feeling hopeless or support AAPI being not enough': 2, 'Educating students': 1, 'Not confronting attacker/harasser or not reporting': 1, 'Color blind/minimizing racism': 1}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 6/12: step2_batches\\step2_batch_3.json\n",
      "âœ… æ‰¹æ¬¡ 6/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step3_batches\\step3_batch_3.json\n",
      "   Gate stats: {'no': 376, 'yes': 129}\n",
      "   Label stats: {'Cannot be inferred': 394, 'Support Asian Americans': 72, 'Advocacy/take actions for changes': 3, 'Government takes actions to stop AAPI hate': 24, 'Educating students': 2, 'Calling for being united': 2, 'Videotaping/confronting harasser/attacker': 1, 'Politicians initiated anti-Asian hate and racism': 4, 'Fostering conversations about anti-Asian hate': 2, 'Color blind/minimizing racism': 1}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 7/12: step2_batches\\step2_batch_4.json\n",
      "âœ… æ‰¹æ¬¡ 7/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step3_batches\\step3_batch_4.json\n",
      "   Gate stats: {'no': 329, 'yes': 154}\n",
      "   Label stats: {'Cannot be inferred': 361, 'Support Asian Americans': 79, 'Fostering conversations about anti-Asian hate': 2, 'Government takes actions to stop AAPI hate': 26, 'Color blind/minimizing racism': 2, 'Advocacy/take actions for changes': 3, 'Speaking up on social media': 2, 'Useless law enforcement': 1, 'Politicians initiated anti-Asian hate and racism': 4, 'Educating students': 3}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 8/12: step2_batches\\step2_batch_5.json\n",
      "âœ… æ‰¹æ¬¡ 8/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step3_batches\\step3_batch_5.json\n",
      "   Gate stats: {'yes': 139, 'no': 335}\n",
      "   Label stats: {'Support Asian Americans': 77, 'Cannot be inferred': 350, 'Useless law enforcement': 1, 'Government takes actions to stop AAPI hate': 20, 'Videotaping/confronting harasser/attacker': 3, 'Advocacy/take actions for changes': 10, 'Attending marches/rallies': 5, 'Educating students': 2, 'Politicians initiated anti-Asian hate and racism': 2, 'Hiring security guards': 1, 'Fostering conversations about anti-Asian hate': 2, 'Feeling hopeless or support AAPI being not enough': 1}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 9/12: step2_batches\\step2_batch_6.json\n",
      "âœ… æ‰¹æ¬¡ 9/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step3_batches\\step3_batch_6.json\n",
      "   Gate stats: {'yes': 141, 'no': 368}\n",
      "   Label stats: {'Support Asian Americans': 90, 'Cannot be inferred': 397, 'Attending marches/rallies': 2, 'Advocacy/take actions for changes': 2, 'Government takes actions to stop AAPI hate': 14, 'Politicians initiated anti-Asian hate and racism': 1, 'Useless law enforcement': 2, 'Fostering conversations about anti-Asian hate': 1}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 10/12: step2_batches\\step2_batch_7.json\n",
      "âœ… æ‰¹æ¬¡ 10/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step3_batches\\step3_batch_7.json\n",
      "   Gate stats: {'yes': 87, 'no': 272}\n",
      "   Label stats: {'Cannot be inferred': 291, 'Support Asian Americans': 31, 'Government takes actions to stop AAPI hate': 28, 'Politicians initiated anti-Asian hate and racism': 2, 'Advocacy/take actions for changes': 6, 'Fostering conversations about anti-Asian hate': 1}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 11/12: step2_batches\\step2_batch_8.json\n",
      "âœ… æ‰¹æ¬¡ 11/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step3_batches\\step3_batch_8.json\n",
      "   Gate stats: {'no': 344, 'yes': 113}\n",
      "   Label stats: {'Cannot be inferred': 376, 'Advocacy/take actions for changes': 11, 'Government takes actions to stop AAPI hate': 27, 'Support Asian Americans': 36, 'Educating students': 3, 'Useless law enforcement': 1, 'Politicians initiated anti-Asian hate and racism': 1, 'Videotaping/confronting harasser/attacker': 2}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 12/12: step2_batches\\step2_batch_9.json\n",
      "âœ… æ‰¹æ¬¡ 12/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step3_batches\\step3_batch_9.json\n",
      "   Gate stats: {'no': 392, 'yes': 115}\n",
      "   Label stats: {'Cannot be inferred': 422, 'Support Asian Americans': 35, 'Government takes actions to stop AAPI hate': 34, 'Advocacy/take actions for changes': 6, 'Useless law enforcement': 4, 'Educating students': 3, 'Fostering conversations about anti-Asian hate': 1, 'Attending marches/rallies': 1, 'Videotaping/confronting harasser/attacker': 1}\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "from collections import Counter\n",
    "from typing import Dict, Any\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# å·¥å…·ï¼šJSON å¯¬é¬†è§£æ + å¥å­æ­£è¦åŒ–\n",
    "# ==============================\n",
    "def strip_code_fences(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return s\n",
    "    s = s.strip()\n",
    "    m = re.search(r\"```(?:json)?\\s*(.*?)```\", s, flags=re.S | re.I)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "    return s\n",
    "\n",
    "def parse_json_loose(s: str):\n",
    "    if not isinstance(s, str):\n",
    "        return None\n",
    "    s = strip_code_fences(s).strip()\n",
    "    if not s:\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        pass\n",
    "    m = re.search(r\"\\{.*\\}\", s, flags=re.S)\n",
    "    if m:\n",
    "        try:\n",
    "            return json.loads(m.group(0))\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def to_text(x: Any) -> str:\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    if isinstance(x, str):\n",
    "        return x\n",
    "    if isinstance(x, list):\n",
    "        return \"\\n\".join(str(item) for item in x)\n",
    "    if isinstance(x, dict):\n",
    "        if \"relevant_sentences\" in x:\n",
    "            return to_text(x[\"relevant_sentences\"])\n",
    "        return \"\\n\".join(to_text(v) for v in x.values())\n",
    "    return str(x)\n",
    "\n",
    "def parse_model_json(s: str, default: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return default\n",
    "    s2 = strip_code_fences(s)\n",
    "    try:\n",
    "        return json.loads(s2)\n",
    "    except Exception:\n",
    "        m = re.search(r\"\\{.*\\}\", s2, flags=re.S)\n",
    "        if m:\n",
    "            try:\n",
    "                return json.loads(m.group(0))\n",
    "            except Exception:\n",
    "                return default\n",
    "        return default\n",
    "\n",
    "# ==============================\n",
    "# Step2 çµæœæ­£è¦åŒ–\n",
    "# ==============================\n",
    "def normalize_step2_result(title: str, raw_obj) -> Dict[str, Dict[str, str]]:\n",
    "    if isinstance(raw_obj, dict):\n",
    "        if all(isinstance(v, dict) for v in raw_obj.values()):\n",
    "            out = {}\n",
    "            for k, v in raw_obj.items():\n",
    "                out[k] = {\n",
    "                    \"entity_type\": v.get(\"entity_type\", \"\"),\n",
    "                    \"asian_status\": v.get(\"asian_status\", \"\"),\n",
    "                    \"relevant_sentences\": to_text(v.get(\"relevant_sentences\"))\n",
    "                }\n",
    "            return out\n",
    "    if isinstance(raw_obj, str):\n",
    "        parsed = parse_json_loose(raw_obj)\n",
    "        if parsed:\n",
    "            return normalize_step2_result(title, parsed)\n",
    "    return {}\n",
    "\n",
    "# ==============================\n",
    "# Prompts\n",
    "# ==============================\n",
    "def build_gate_prompt(step2_sentences: str) -> str:\n",
    "    return f\"\"\"You are a sociology professor analyzing reactions to anti-Asian hate.\n",
    "\n",
    "Task: Decide if the extracted sentences show any OBSERVABLE reaction (action or inaction) to anti-Asian hate by the entity.\n",
    "\n",
    "Rules:\n",
    "- Observable = concrete action/inaction or explicit public stance (e.g., speaking up, condemning, organizing, reporting, policy ask, government action, refusing to act).\n",
    "- Pure emotions/concerns are NOT reactions.\n",
    "- Pure incident descriptions are NOT reactions.\n",
    "- Use ONLY the exact `relevant_sentences`.\n",
    "\n",
    "Output JSON:\n",
    "{{\n",
    "  \"has_reaction\": \"yes\" | \"no\",\n",
    "  \"evidence\": \"Exact sentence(s) from relevant_sentences (or empty if no).\"\n",
    "}}\n",
    "\n",
    "relevant_sentences:\n",
    "{step2_sentences}\n",
    "\"\"\"\n",
    "\n",
    "def build_classifier_prompt(entity_type: str, asian_status: str, step2_sentences: str) -> str:\n",
    "    return f\"\"\"You are a sociology professor with 30 years of experience analyzing anti-Asian racism.\n",
    "\n",
    "Task: Classify the entityâ€™s REACTION strictly using the Reaction Concept Tree. Use ONLY `relevant_sentences` as evidence. Do NOT infer emotions. Do NOT paraphrase.\n",
    "\n",
    "Reaction Concept Tree:\n",
    "- Support Asian Americans\n",
    "  - Attending marches/rallies\n",
    "  - Speaking up on social media\n",
    "  - Calling for being united\n",
    "  - Educating students\n",
    "  - Fostering conversations about anti-Asian hate\n",
    "  - Hiring security guards\n",
    "  - Providing shopkeepers with air horns\n",
    "  - Rewarding the public to report the info about the suspects\n",
    "- Advocacy/take actions for changes\n",
    "- Politicians initiated anti-Asian hate and racism\n",
    "- Undermining human rights\n",
    "- Color blind/minimizing racism\n",
    "- Youth as not an excuse\n",
    "- Videotaping/confronting harasser/attacker\n",
    "- Sex (sexual) addiction\n",
    "- Religion as a reason\n",
    "- Feeling hopeless or support AAPI being not enough\n",
    "- Not confronting attacker/harasser or not reporting\n",
    "- Useless law enforcement\n",
    "  - Did not take a report on Anti-Asian hate crime\n",
    "  - Did not often patrol the streets\n",
    "- Government takes actions to stop AAPI hate\n",
    "  - Installing hotlines\n",
    "  - Launching a hate-crime task force\n",
    "  - Increasing patrols\n",
    "  - Organizing a town hall\n",
    "\n",
    "Strict Rules:\n",
    "1) Pure concerns/worries â‰  reaction; return \"Cannot be inferred\".\n",
    "2) Arrests/charges/prosecutions â‡’ â€œGovernment takes actionsâ€¦â€.\n",
    "3) Explicit condemnation â‡’ â€œSupport Asian Americansâ€.\n",
    "4) If no clear reaction, return \"Cannot be inferred\".\n",
    "5) Do NOT invent labels.\n",
    "6) Always choose the most specific subcategory.\n",
    "\n",
    "entity_type: {entity_type}\n",
    "asian_status: {asian_status}\n",
    "\n",
    "relevant_sentences:\n",
    "{step2_sentences}\n",
    "\n",
    "Output JSON:\n",
    "{{\n",
    "  \"reaction\": \"<one label from the tree or 'Cannot be inferred'>\",\n",
    "  \"reaction_reason\": \"Exact sentence(s) from relevant_sentences\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "# ==============================\n",
    "# ä¸»æµç¨‹ï¼šå¾ Step2 â†’ Step3\n",
    "# ==============================\n",
    "def run_step3_from_step2(step2_dir=\"step2_batches\", step3_dir=\"step3_batches\"):\n",
    "    os.makedirs(step3_dir, exist_ok=True)\n",
    "    step2_files = sorted(glob.glob(os.path.join(step2_dir, \"step2_batch_*.json\")))\n",
    "    total_batches = len(step2_files)\n",
    "\n",
    "    for idx, step2_file in enumerate(step2_files, start=1):\n",
    "        batch_name = os.path.basename(step2_file).replace(\"step2_\", \"step3_\")\n",
    "        out_path = os.path.join(step3_dir, batch_name)\n",
    "\n",
    "        # æ–·é»çºŒè·‘\n",
    "        if os.path.exists(out_path):\n",
    "            print(f\"â­ï¸ æ‰¹æ¬¡ {idx}/{total_batches} {batch_name} å·²å­˜åœ¨ï¼Œè·³é\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ {idx}/{total_batches}: {step2_file}\")\n",
    "\n",
    "        with open(step2_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            step2_batch_result = json.load(f)\n",
    "\n",
    "        step3_batch_result = {}\n",
    "        gate_stats = Counter()\n",
    "        label_stats = Counter()\n",
    "\n",
    "        for title, raw in step2_batch_result.items():\n",
    "            entities = normalize_step2_result(title, raw)\n",
    "            entity_outputs = {}\n",
    "\n",
    "            if not entities:\n",
    "                step3_batch_result[title] = entity_outputs\n",
    "                continue\n",
    "\n",
    "            for entity, meta in entities.items():\n",
    "                entity_type = meta.get(\"entity_type\", \"\")\n",
    "                asian_status = meta.get(\"asian_status\", \"\")\n",
    "                relevant_sentences = to_text(meta.get(\"relevant_sentences\", \"\")).strip()\n",
    "\n",
    "                # --- 3A: Gate ---\n",
    "                gate_prompt = build_gate_prompt(relevant_sentences)\n",
    "                gate_resp = get_response(gate_prompt, temperature=0.0)\n",
    "                gate_json = parse_model_json(gate_resp, default={\"has_reaction\": \"no\", \"evidence\": \"\"})\n",
    "\n",
    "                has_reaction = str(gate_json.get(\"has_reaction\", \"no\")).lower()\n",
    "                gate_stats[has_reaction] += 1\n",
    "\n",
    "                if has_reaction != \"yes\":\n",
    "                    out = {\n",
    "                        \"entity_type\": entity_type,\n",
    "                        \"asian_status\": asian_status,\n",
    "                        \"reaction\": \"Cannot be inferred\",\n",
    "                        \"reaction_reason\": \"\"\n",
    "                    }\n",
    "                    entity_outputs[entity] = out\n",
    "                    label_stats[\"Cannot be inferred\"] += 1\n",
    "                    continue\n",
    "\n",
    "                # --- 3B: Classifier ---\n",
    "                cls_prompt = build_classifier_prompt(entity_type, asian_status, relevant_sentences)\n",
    "                cls_resp = get_response(cls_prompt, temperature=0.0)\n",
    "                cls_json = parse_model_json(cls_resp, default={\"reaction\": \"Cannot be inferred\", \"reaction_reason\": \"\"})\n",
    "\n",
    "                label = cls_json.get(\"reaction\", \"Cannot be inferred\")\n",
    "                reason = cls_json.get(\"reaction_reason\", \"\")\n",
    "\n",
    "                out = {\n",
    "                    \"entity_type\": entity_type,\n",
    "                    \"asian_status\": asian_status,\n",
    "                    \"reaction\": label,\n",
    "                    \"reaction_reason\": reason\n",
    "                }\n",
    "                entity_outputs[entity] = out\n",
    "                label_stats[label] += 1\n",
    "\n",
    "            step3_batch_result[title] = entity_outputs\n",
    "\n",
    "        # å„²å­˜\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(step3_batch_result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"âœ… æ‰¹æ¬¡ {idx}/{total_batches} å·²å®Œæˆä¸¦å„²å­˜è‡³ {out_path}\")\n",
    "        print(\"   Gate stats:\", dict(gate_stats))\n",
    "        print(\"   Label stats:\", dict(label_stats))\n",
    "\n",
    "# ==============================\n",
    "# åŸ·è¡Œ\n",
    "# ==============================\n",
    "if __name__ == \"__main__\":\n",
    "    run_step3_from_step2(\"step2_batches\", \"step3_batches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33bca6c5-678f-4f0b-83b5-4892945dd504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 1/12: step2_batches\\step2_batch_1.json\n",
      "âœ… æ‰¹æ¬¡ 1/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step3_batches_new\\step3_batch_1.json\n",
      "   Gate stats: {'no': 472, 'yes': 185}\n",
      "   Label stats: {'Cannot be inferred': 504, 'Support Asian Americans': 77, 'Fostering conversations about anti-Asian hate': 14, 'Speaking up on social media': 4, 'Government takes actions for changes': 2, 'Attending marches/rallies': 8, 'Calling for being united': 7, 'Increasing patrols': 4, 'Takes actions to stop AAPI hate': 12, 'Politicians initiated anti-Asian hate and racism': 5, 'Feeling hopeless or support AAPI being not enough': 11, 'Videotaping/confronting harasser/attacker': 2, 'Advocacy/take actions for changes': 4, 'Government takes actions to stop AAPI hate': 1, 'Government takes actionsâ€¦': 1, 'Launching a hate-crime task force': 1}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 2/12: step2_batches\\step2_batch_10.json\n",
      "âœ… æ‰¹æ¬¡ 2/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step3_batches_new\\step3_batch_10.json\n",
      "   Gate stats: {'no': 314, 'yes': 136}\n",
      "   Label stats: {'Cannot be inferred': 332, 'Support Asian Americans': 53, 'Takes actions to stop AAPI hate': 26, 'Attending marches/rallies': 6, 'Advocacy/take actions for changes': 6, 'Government takes actionsâ€¦': 3, 'Increasing patrols': 3, 'Government takes actions to stop AAPI hate': 1, 'Fostering conversations about anti-Asian hate': 5, 'Videotaping/confronting harasser/attacker': 1, 'Speaking up on social media': 7, 'Educating students': 2, 'Installing hotlines': 1, 'Calling for being united': 1, 'Making an announcement to condemn anti-Asian hate': 1, 'Politicians initiated anti-Asian hate and racism': 1, 'Feeling hopeless or support AAPI being not enough': 1}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 3/12: step2_batches\\step2_batch_11.json\n",
      "âœ… æ‰¹æ¬¡ 3/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step3_batches_new\\step3_batch_11.json\n",
      "   Gate stats: {'yes': 170, 'no': 428}\n",
      "   Label stats: {'Support Asian Americans': 75, 'Cannot be inferred': 455, 'Takes actions to stop AAPI hate': 15, 'Government takes actionsâ€¦': 6, 'Launching a hate-crime task force': 2, 'Speaking up on social media': 10, 'Feeling hopeless or support AAPI being not enough': 4, 'Advocacy/take actions for changes': 7, 'Videotaping/confronting harasser/attacker': 4, 'Fostering conversations about anti-Asian hate': 4, 'Increasing patrols': 7, 'Hiring security guards': 1, 'Calling for being united': 2, 'Attending marches/rallies': 1, 'Making an announcement to condemn anti-Asian hate': 1, 'Installing hotlines': 1, 'Government takes actions to stop AAPI hate': 2, 'Politicians initiated anti-Asian hate and racism': 1}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 4/12: step2_batches\\step2_batch_12.json\n",
      "âœ… æ‰¹æ¬¡ 4/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step3_batches_new\\step3_batch_12.json\n",
      "   Gate stats: {'yes': 105, 'no': 237}\n",
      "   Label stats: {'Support Asian Americans': 39, 'Takes actions to stop AAPI hate': 16, 'Cannot be inferred': 251, 'Government takes actionsâ€¦': 3, 'Speaking up on social media': 8, 'Videotaping/confronting harasser/attacker': 2, 'Installing hotlines': 1, 'Fostering conversations about anti-Asian hate': 7, 'Advocacy/take actions for changes': 6, 'Feeling hopeless or support AAPI being not enough': 1, 'Politicians initiated anti-Asian hate and racism': 1, 'Attending marches/rallies': 1, 'Rewarding the public to report the info about the suspects': 4, 'Calling for being united': 2}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 5/12: step2_batches\\step2_batch_2.json\n",
      "âœ… æ‰¹æ¬¡ 5/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step3_batches_new\\step3_batch_2.json\n",
      "   Gate stats: {'no': 372, 'yes': 129}\n",
      "   Label stats: {'Cannot be inferred': 402, 'Fostering conversations about anti-Asian hate': 13, 'Support Asian Americans': 43, 'Useless law enforcement': 1, 'Increasing patrols': 1, 'Politicians initiated anti-Asian hate and racism': 1, 'Launching a hate-crime task force': 4, 'Attending marches/rallies': 5, 'Takes actions to stop AAPI hate': 10, 'Advocacy/take actions for changes': 5, 'Speaking up on social media': 1, 'Rewarding the public to report the info about the suspects': 1, 'Feeling hopeless or support AAPI being not enough': 2, 'Calling for being united': 1, 'Government takes actions to stop AAPI hate': 6, 'Government takes actionsâ€¦': 1, 'Installing hotlines': 1, 'Not confronting attacker/harasser or not reporting': 1, 'Organizing a town hall': 1, 'Color blind/minimizing racism': 1}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 6/12: step2_batches\\step2_batch_3.json\n",
      "âœ… æ‰¹æ¬¡ 6/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step3_batches_new\\step3_batch_3.json\n",
      "   Gate stats: {'no': 407, 'yes': 130}\n",
      "   Label stats: {'Cannot be inferred': 426, 'Support Asian Americans': 58, 'Advocacy/take actions for changes': 2, 'Fostering conversations about anti-Asian hate': 16, 'Takes actions to stop AAPI hate': 5, 'Calling for being united': 6, 'Useless law enforcement': 1, 'Did not take a report on Anti-Asian hate crime': 1, 'Speaking up on social media': 8, 'Attending marches/rallies': 2, 'Organizing a town hall': 1, 'Government takes actionsâ€¦': 2, 'Politicians initiated anti-Asian hate and racism': 2, 'Installing hotlines': 6, 'Rewarding the public to report the info about the suspects': 1}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 7/12: step2_batches\\step2_batch_4.json\n",
      "âœ… æ‰¹æ¬¡ 7/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step3_batches_new\\step3_batch_4.json\n",
      "   Gate stats: {'no': 345, 'yes': 152}\n",
      "   Label stats: {'Cannot be inferred': 370, 'Fostering conversations about anti-Asian hate': 9, 'Calling for being united': 3, 'Support Asian Americans': 75, 'Politicians initiated anti-Asian hate and racism': 4, 'Government takes actionsâ€¦': 2, 'Takes actions to stop AAPI hate': 8, 'Speaking up on social media': 12, 'Installing hotlines': 6, 'Attending marches/rallies': 3, 'Feeling hopeless or support AAPI being not enough': 1, 'Educating students': 1, 'Government takes actions to stop AAPI hate': 1, 'Advocacy/take actions for changes': 1, 'Launching a hate-crime task force': 1}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 8/12: step2_batches\\step2_batch_5.json\n",
      "âœ… æ‰¹æ¬¡ 8/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step3_batches_new\\step3_batch_5.json\n",
      "   Gate stats: {'yes': 154, 'no': 376}\n",
      "   Label stats: {'Support Asian Americans': 76, 'Cannot be inferred': 392, 'Takes actions to stop AAPI hate': 10, 'Videotaping/confronting harasser/attacker': 3, 'Advocacy/take actions for changes': 9, 'Calling for being united': 4, 'Attending marches/rallies': 9, 'Fostering conversations about anti-Asian hate': 13, 'Speaking up on social media': 3, 'Increasing patrols': 2, 'Feeling hopeless or support AAPI being not enough': 2, 'Organizing a town hall': 3, 'Launching a hate-crime task force': 1, 'Educating students': 1, 'Rewarding the public to report the info about the suspects': 2}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 9/12: step2_batches\\step2_batch_6.json\n",
      "âœ… æ‰¹æ¬¡ 9/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step3_batches_new\\step3_batch_6.json\n",
      "   Gate stats: {'yes': 156, 'no': 389}\n",
      "   Label stats: {'Attending marches/rallies': 12, 'Fostering conversations about anti-Asian hate': 4, 'Support Asian Americans': 81, 'Cannot be inferred': 416, 'Calling for being united': 7, 'Feeling hopeless or support AAPI being not enough': 2, 'Advocacy/take actions for changes': 2, 'Useless law enforcement': 1, 'Launching a hate-crime task force': 1, 'Increasing patrols': 2, 'Videotaping/confronting harasser/attacker': 1, 'Takes actions to stop AAPI hate': 6, 'Government takes actionsâ€¦': 1, 'Government takes actions to stop AAPI hate': 2, 'Speaking up on social media': 6, 'Color blind/minimizing racism': 1}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 10/12: step2_batches\\step2_batch_7.json\n",
      "âœ… æ‰¹æ¬¡ 10/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step3_batches_new\\step3_batch_7.json\n",
      "   Gate stats: {'yes': 95, 'no': 302}\n",
      "   Label stats: {'Fostering conversations about anti-Asian hate': 9, 'Cannot be inferred': 317, 'Support Asian Americans': 34, 'Increasing patrols': 3, 'Takes actions to stop AAPI hate': 15, 'Politicians initiated anti-Asian hate and racism': 3, 'Installing hotlines': 1, 'Advocacy/take actions for changes': 4, 'Government takes actionsâ€¦': 4, 'Government takes actions to stop AAPI hate': 2, 'Launching a hate-crime task force': 1, 'Attending marches/rallies': 1, 'Feeling hopeless or support AAPI being not enough': 1, 'Videotaping/confronting harasser/attacker': 1, 'Speaking up on social media': 1}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 11/12: step2_batches\\step2_batch_8.json\n",
      "âœ… æ‰¹æ¬¡ 11/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step3_batches_new\\step3_batch_8.json\n",
      "   Gate stats: {'no': 370, 'yes': 121}\n",
      "   Label stats: {'Cannot be inferred': 405, 'Fostering conversations about anti-Asian hate': 12, 'Government takes actions to stop AAPI hate': 2, 'Support Asian Americans': 43, 'Advocacy/take actions for changes': 10, 'Speaking up on social media': 3, 'Government takes actionsâ€¦': 1, 'Launching a hate-crime task force': 1, 'Takes actions to stop AAPI hate': 9, 'Calling for being united': 1, 'Videotaping/confronting harasser/attacker': 2, 'Installing hotlines': 2}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 12/12: step2_batches\\step2_batch_9.json\n",
      "âœ… æ‰¹æ¬¡ 12/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step3_batches_new\\step3_batch_9.json\n",
      "   Gate stats: {'no': 395, 'yes': 123}\n",
      "   Label stats: {'Cannot be inferred': 438, 'Speaking up on social media': 3, 'Advocacy/take actions for changes': 6, 'Support Asian Americans': 31, 'Fostering conversations about anti-Asian hate': 12, 'Takes actions to stop AAPI hate': 15, 'Attending marches/rallies': 5, 'Feeling hopeless or support AAPI being not enough': 1, 'Color blind/minimizing racism': 1, 'Government takes actionsâ€¦': 1, 'Increasing patrols': 2, 'Videotaping/confronting harasser/attacker': 1, 'Useless law enforcement': 2}\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "from collections import Counter\n",
    "from typing import Dict, Any\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# å·¥å…·ï¼šJSON å¯¬é¬†è§£æ + å¥å­æ­£è¦åŒ–\n",
    "# ==============================\n",
    "def strip_code_fences(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return s\n",
    "    s = s.strip()\n",
    "    m = re.search(r\"```(?:json)?\\s*(.*?)```\", s, flags=re.S | re.I)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "    return s\n",
    "\n",
    "def parse_json_loose(s: str):\n",
    "    if not isinstance(s, str):\n",
    "        return None\n",
    "    s = strip_code_fences(s).strip()\n",
    "    if not s:\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        pass\n",
    "    m = re.search(r\"\\{.*\\}\", s, flags=re.S)\n",
    "    if m:\n",
    "        try:\n",
    "            return json.loads(m.group(0))\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def to_text(x: Any) -> str:\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    if isinstance(x, str):\n",
    "        return x\n",
    "    if isinstance(x, list):\n",
    "        return \"\\n\".join(str(item) for item in x)\n",
    "    if isinstance(x, dict):\n",
    "        if \"relevant_sentences\" in x:\n",
    "            return to_text(x[\"relevant_sentences\"])\n",
    "        return \"\\n\".join(to_text(v) for v in x.values())\n",
    "    return str(x)\n",
    "\n",
    "def parse_model_json(s: str, default: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return default\n",
    "    s2 = strip_code_fences(s)\n",
    "    try:\n",
    "        return json.loads(s2)\n",
    "    except Exception:\n",
    "        m = re.search(r\"\\{.*\\}\", s2, flags=re.S)\n",
    "        if m:\n",
    "            try:\n",
    "                return json.loads(m.group(0))\n",
    "            except Exception:\n",
    "                return default\n",
    "        return default\n",
    "\n",
    "# ==============================\n",
    "# Step2 çµæœæ­£è¦åŒ–\n",
    "# ==============================\n",
    "def normalize_step2_result(title: str, raw_obj) -> Dict[str, Dict[str, str]]:\n",
    "    if isinstance(raw_obj, dict):\n",
    "        if all(isinstance(v, dict) for v in raw_obj.values()):\n",
    "            out = {}\n",
    "            for k, v in raw_obj.items():\n",
    "                out[k] = {\n",
    "                    \"entity_type\": v.get(\"entity_type\", \"\"),\n",
    "                    \"asian_status\": v.get(\"asian_status\", \"\"),\n",
    "                    \"relevant_sentences\": to_text(v.get(\"relevant_sentences\"))\n",
    "                }\n",
    "            return out\n",
    "    if isinstance(raw_obj, str):\n",
    "        parsed = parse_json_loose(raw_obj)\n",
    "        if parsed:\n",
    "            return normalize_step2_result(title, parsed)\n",
    "    return {}\n",
    "\n",
    "# ==============================\n",
    "# Prompts\n",
    "# ==============================\n",
    "def build_gate_prompt(step2_sentences: str) -> str:\n",
    "    return f\"\"\"You are a sociology professor analyzing reactions to anti-Asian hate.\n",
    "\n",
    "Task: Decide if the extracted sentences show any OBSERVABLE reaction (action or inaction) to anti-Asian hate by the entity.\n",
    "\n",
    "Rules:\n",
    "- Observable = concrete action/inaction or explicit public stance (e.g., speaking up, condemning, organizing, reporting, policy ask, government action, refusing to act).\n",
    "- Pure emotions/concerns are NOT reactions.\n",
    "- Pure incident descriptions are NOT reactions.\n",
    "- Use ONLY the exact `relevant_sentences`.\n",
    "\n",
    "Output JSON:\n",
    "{{\n",
    "  \"has_reaction\": \"yes\" | \"no\",\n",
    "  \"evidence\": \"Exact sentence(s) from relevant_sentences (or empty if no).\"\n",
    "}}\n",
    "\n",
    "relevant_sentences:\n",
    "{step2_sentences}\n",
    "\"\"\"\n",
    "\n",
    "def build_classifier_prompt(entity_type: str, asian_status: str, step2_sentences: str) -> str:\n",
    "    return f\"\"\"You are a sociology professor with 30 years of experience analyzing anti-Asian racism.\n",
    "\n",
    "Task: Classify the entityâ€™s REACTION strictly using the Reaction Concept Tree. Use ONLY `relevant_sentences` as evidence. Do NOT infer emotions. Do NOT paraphrase.\n",
    "\n",
    "Reaction Concept Tree:\n",
    "- Support Asian Americans:People or organizations condemned/do not want Anti-Asian incidents to happen, including all forms of crimes, attacks, violence, assaults, physical, verbal, and online harassment. This kind of support is at a conscientious (cognitive) level, not yet taking concrete actions to stop AAPI hate. \n",
    "  - Attending marches/rallies:People or organizations actively attended marches/rallies for supporting Asian American communities.\n",
    "  - Speaking up on social media:People or organizations spoke up in public, such as via social media, to condemn Anti-Asian hate.\n",
    "  - Calling for being united:Asians/Asian Americans become united to support each other to overcome Anti-Asian hate.\n",
    "  - Fostering conversations about anti-Asian hate:The public fostered conversations regarding the Anti-Asian hate issues. Some organizations/groups (e.g., Asian American communities) also think itâ€™s important to have conversations to address the root cause behind the Anti-Asian hate incidents so this will help us to make change.\n",
    "  - Providing shopkeepers with air horns:Some stores or groups provide shopkeepers with air horns. If any anti-Asian hate crimes happened, they can use the air horns to draw everyoneâ€™s attention\n",
    "- Advocacy/take actions for changes:Individuals, groups, or organizations want or advocate for changing the current situation where Asians/Asian Americans suffered from Anti-Asian hate, such as physical, verbal, and online harassment, attacks, violence, assaults, and hate crimes. They want cultural shift, open dialogue and listening sessions regarding incidents, practical change in racial stereotypes\n",
    "and perceptions, more awareness about Anti-Asian hate, and human rights.\n",
    "- Politicians initiated anti-Asian hate and racism:Trump and some republican politicians made a lot of comments on the COVID-19 pandemic. One kind of comment is that he used/dubbed Asian-related objects to combine with disease (virus/flu/covid) or directly calling coronavirus/covid (e.g., countryâ€™s or regionâ€™s names or Kung flu[modified by Kung Fu], or races) to verbally attack Asians/Asian Americans, such as  â€œChina/Chinese virusâ€ or â€œKung flu.â€ Another type of comment is that he blamed China for causing the pandemic. Such kinds of comments initiated racism, Anti-Asian bigotry/hate, and Anti-Asian hate incidents in the US society. Such comments also led some Americans to blame Asians/Asian Americans for causing the pandemic.\n",
    "- Undermining human rights:People want to dehumanize and to undermine the fundamental rights, dignity and belonging of those they target.\n",
    "- Color blind/minimizing racism:Some Americans do not believe discrimination, racism, or racist bias/bigotry against Asians/Asian Americans exists in the community. Incidents of anti-Asian hate (including physical, verbal, and online harassment, attacks, violence, assaults, and Anti-Asian crimes) were downplayed, ignored, or perceived as not existing by the public, the law enforcement system (e.g., the police), and governors (e.g., Mr. Donald Trump). When Asians/Asian Americans were attacked, Anti-Asian hate or racism was not perceived as the perpetratorsâ€™ motives/motivations by the police or the perpetrators said their motivations were not triggered by Anti-Asian bigotry or racism.\n",
    "- Youth as not an excuse:Robert Aaron Long murdered eight people in the incident of the 2021 Atlanta Spa Shootings. In news reports, he was called â€œthe 21-year-old.â€ Some comments advocated stopping calling him â€œthe 21-year-oldâ€ as if his youth is an excuse to murder others because of their race, ethnicities, and sex.\n",
    "- Videotaping/confronting harasser/attacker:Asians/Asian Americans or bystanders videotaped/recorded the incidents of physical or verbal harassment; Anti-Asian attacks, assaults, or violence; and Anti-Asian crimes. Asians/Asian Americans who suffered physical harassment, attacks, assaults, violence attacked back to the harassers or attackers. Bystandersâ€™ behaviors aim to defense those who were attacked. Cell phones and survelliance system can be used for videotaping or recording. Additionally, those who experienced verbal harassment speak out to the harassers to let them know their thoughts were biased, offensive, and unjust and tell them to stop. \n",
    "- Sex (sexual) addiction:Excessive sexual thoughts, desires, urges or behaviors that canâ€™t be controlled and cause distress and harm to your relationships, finances and other aspects of life. It is also called hypersexuality or compulsive sexual behavior. It is what the Atlanta shooter claimed as a motivation that led to his senseless killings of the victims.\n",
    "- Religion as a reason:In the 2021 Atlanta Spa Shootings, Robert Aaron Long was the killer who murdered eight people. He told the police that his motive was religious guilt about his sexuality. He said he had sexual desire so he wanted to eliminate it. Thatâ€™s why he went to the spa to skill women of Asian descent. Asian advocacy groups mentioned whether the killerâ€™s motive was religious guilt about his sexuality, no one should ignore the broader context of Anti-Asian violence and hate crimes. Asian advocacy groups tend to attribute the killerâ€™s motive stems from racism or xenophobia, misogyny, and gendered racism\n",
    "- Feeling hopeless or support AAPI being not enough:Asians/Asian Americans felt worried, frustrated, anxious, and afraid that they may experience Anti-Asian hate crimes, attacks, assaults, and violence. But they felt that nothing happened to stop them. Support for Asian American communities is not enough.\n",
    "- Not confronting attacker/harasser or not reporting:Asians/Asian Americans did not want to confront attackers/harassers/bullies who physically or verbally harassed or attacked them. They thought it is not worthy of reporting the incidents. They did not want to confront because they were afraid of their safety. They just wanted to leave from the incidents soon.\n",
    "- Useless law enforcement:Police did not take a police report and denied there was an Anti-Asian hate crime for the incidents of physical, verbal, or online harassment, attacks, assaults, violence, and Anti-Asian crimes. Another situation is that police affirmed there was a crime, but the motivation did not come from Anti-Asian hate or bigotry/prejudice or racism. Additionally, Asian Americans thought if police often patrolled the streets, a lot of Anti-Asian hate crimes, attacks, assaults, and violence would not happen. But in reality, policy did not do so. \n",
    "  - Did not take a report on Anti-Asian hate crime:police did not take a report on Anti-Asian hate crime, including physical, verbal, or online harassment, attacks, assaults, and violence.\n",
    "  - Did not often patrol the streets:police affirmed there was a crime, but police did not often patrol the streets so that there were a lot of Anti-Asian hate crimes (e.g., physical, verbal, or online harassment, attacks, assaults, and violence) happened.\n",
    "- Takes actions to stop AAPI hate:After the incidents of Anti-Asian hate crimes, attacks, assaults, and violence, state or city government or individuals take concrete actions that aim to stop AAPI hate.\n",
    "  - Installing hotlines:This is a type of action to stop AAPI hate. Some organizations (e.g., city and state governments) install hotlines for victims or people who witness Anti-Asian incidents to report.\n",
    "  - Launching a hate-crime task force:This is a type of action to stop AAPI hate. Some organizations (e.g., city and state governments) launched an Asian hate crime task force to develop approaches to stopping anti-Asian hate crimes.\n",
    "  - Making an announcement to condemn anti-Asian hate:This is a type of action to stop AAPI hate. Some organizations (e.g., city and state governments)  made an open announcement to condemn anti-Asian hate.\n",
    "  - Increasing patrols:Some organizations (e.g., city and state governments) increased patrolling the streets to ensure the safety of Asian Americans. \n",
    "  - Organizing a town hall:Some organizations (e.g., city and state governments) organized a town hall meeting to discuss how to stop anti-Asian hate racism.\n",
    "  - Hiring security guards:Some stores or groups hired security guards to increase safety for Asian Americans and prevent anti-Asian hate crimes or racism.\n",
    "  - Educating students:Schoolteachers and university faculty took actions to educate students on current social and political issues on Anti-Asian hate. They aim to use education to change the publicâ€™s view about Asian Americans/Asians and increase the awareness of respecting Asian Americans/Asians\n",
    "  - Rewarding the public to report the info about the suspects:Individuals, groups, or organizations provide rewards to the public when they report any information regarding the suspects who may commit anti-Asian hate crimes.\n",
    "\n",
    "\n",
    "Strict Rules:\n",
    "1) Pure concerns/worries â‰  reaction; return \"Cannot be inferred\".\n",
    "2) Arrests/charges/prosecutions â‡’ â€œGovernment takes actionsâ€¦â€.\n",
    "3) Explicit condemnation â‡’ â€œSupport Asian Americansâ€.\n",
    "4) If no clear reaction, return \"Cannot be inferred\".\n",
    "5) Do NOT invent labels.\n",
    "6) Always choose the most specific subcategory.\n",
    "\n",
    "entity_type: {entity_type}\n",
    "asian_status: {asian_status}\n",
    "\n",
    "relevant_sentences:\n",
    "{step2_sentences}\n",
    "\n",
    "Output JSON:\n",
    "{{\n",
    "  \"reaction\": \"<one label from the tree or 'Cannot be inferred'>\",\n",
    "  \"reaction_reason\": \"Exact sentence(s) from relevant_sentences\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "# ==============================\n",
    "# ä¸»æµç¨‹ï¼šå¾ Step2 â†’ Step3\n",
    "# ==============================\n",
    "def run_step3_from_step2(step2_dir=\"step2_batches\", step3_dir=\"step3_batches\"):\n",
    "    os.makedirs(step3_dir, exist_ok=True)\n",
    "    step2_files = sorted(glob.glob(os.path.join(step2_dir, \"step2_batch_*.json\")))\n",
    "    total_batches = len(step2_files)\n",
    "\n",
    "    for idx, step2_file in enumerate(step2_files, start=1):\n",
    "        batch_name = os.path.basename(step2_file).replace(\"step2_\", \"step3_\")\n",
    "        out_path = os.path.join(step3_dir, batch_name)\n",
    "\n",
    "        # æ–·é»çºŒè·‘\n",
    "        if os.path.exists(out_path):\n",
    "            print(f\"â­ï¸ æ‰¹æ¬¡ {idx}/{total_batches} {batch_name} å·²å­˜åœ¨ï¼Œè·³é\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ {idx}/{total_batches}: {step2_file}\")\n",
    "\n",
    "        with open(step2_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            step2_batch_result = json.load(f)\n",
    "\n",
    "        step3_batch_result = {}\n",
    "        gate_stats = Counter()\n",
    "        label_stats = Counter()\n",
    "\n",
    "        for title, raw in step2_batch_result.items():\n",
    "            entities = normalize_step2_result(title, raw)\n",
    "            entity_outputs = {}\n",
    "\n",
    "            if not entities:\n",
    "                step3_batch_result[title] = entity_outputs\n",
    "                continue\n",
    "\n",
    "            for entity, meta in entities.items():\n",
    "                entity_type = meta.get(\"entity_type\", \"\")\n",
    "                asian_status = meta.get(\"asian_status\", \"\")\n",
    "                relevant_sentences = to_text(meta.get(\"relevant_sentences\", \"\")).strip()\n",
    "\n",
    "                # --- 3A: Gate ---\n",
    "                gate_prompt = build_gate_prompt(relevant_sentences)\n",
    "                gate_resp = get_response(gate_prompt, temperature=0.0)\n",
    "                gate_json = parse_model_json(gate_resp, default={\"has_reaction\": \"no\", \"evidence\": \"\"})\n",
    "\n",
    "                has_reaction = str(gate_json.get(\"has_reaction\", \"no\")).lower()\n",
    "                gate_stats[has_reaction] += 1\n",
    "\n",
    "                if has_reaction != \"yes\":\n",
    "                    out = {\n",
    "                        \"entity_type\": entity_type,\n",
    "                        \"asian_status\": asian_status,\n",
    "                        \"reaction\": \"Cannot be inferred\",\n",
    "                        \"reaction_reason\": \"\"\n",
    "                    }\n",
    "                    entity_outputs[entity] = out\n",
    "                    label_stats[\"Cannot be inferred\"] += 1\n",
    "                    continue\n",
    "\n",
    "                # --- 3B: Classifier ---\n",
    "                cls_prompt = build_classifier_prompt(entity_type, asian_status, relevant_sentences)\n",
    "                cls_resp = get_response(cls_prompt, temperature=0.0)\n",
    "                cls_json = parse_model_json(cls_resp, default={\"reaction\": \"Cannot be inferred\", \"reaction_reason\": \"\"})\n",
    "\n",
    "                label = cls_json.get(\"reaction\", \"Cannot be inferred\")\n",
    "                reason = cls_json.get(\"reaction_reason\", \"\")\n",
    "\n",
    "                out = {\n",
    "                    \"entity_type\": entity_type,\n",
    "                    \"asian_status\": asian_status,\n",
    "                    \"reaction\": label,\n",
    "                    \"reaction_reason\": reason\n",
    "                }\n",
    "                entity_outputs[entity] = out\n",
    "                label_stats[label] += 1\n",
    "\n",
    "            step3_batch_result[title] = entity_outputs\n",
    "\n",
    "        # å„²å­˜\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(step3_batch_result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"âœ… æ‰¹æ¬¡ {idx}/{total_batches} å·²å®Œæˆä¸¦å„²å­˜è‡³ {out_path}\")\n",
    "        print(\"   Gate stats:\", dict(gate_stats))\n",
    "        print(\"   Label stats:\", dict(label_stats))\n",
    "\n",
    "# ==============================\n",
    "# åŸ·è¡Œ\n",
    "# ==============================\n",
    "if __name__ == \"__main__\":\n",
    "    run_step3_from_step2(\"step2_batches\", \"step3_batches_new\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "235f9159-8986-4aaa-8c91-8533ad327d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 1/12: step2_batches\\step2_batch_1.json\n",
      "âœ… æ‰¹æ¬¡ 1/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step4_batches\\step4_batch_1.json\n",
      "   Stats: {'done': 50}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 2/12: step2_batches\\step2_batch_10.json\n",
      "âœ… æ‰¹æ¬¡ 2/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step4_batches\\step4_batch_10.json\n",
      "   Stats: {'done': 50}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 3/12: step2_batches\\step2_batch_11.json\n",
      "âœ… æ‰¹æ¬¡ 3/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step4_batches\\step4_batch_11.json\n",
      "   Stats: {'done': 50}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 4/12: step2_batches\\step2_batch_12.json\n",
      "âœ… æ‰¹æ¬¡ 4/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step4_batches\\step4_batch_12.json\n",
      "   Stats: {'done': 34}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 5/12: step2_batches\\step2_batch_2.json\n",
      "âœ… æ‰¹æ¬¡ 5/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step4_batches\\step4_batch_2.json\n",
      "   Stats: {'done': 50}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 6/12: step2_batches\\step2_batch_3.json\n",
      "âœ… æ‰¹æ¬¡ 6/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step4_batches\\step4_batch_3.json\n",
      "   Stats: {'done': 50}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 7/12: step2_batches\\step2_batch_4.json\n",
      "âœ… æ‰¹æ¬¡ 7/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step4_batches\\step4_batch_4.json\n",
      "   Stats: {'done': 50}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 8/12: step2_batches\\step2_batch_5.json\n",
      "âœ… æ‰¹æ¬¡ 8/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step4_batches\\step4_batch_5.json\n",
      "   Stats: {'done': 50}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 9/12: step2_batches\\step2_batch_6.json\n",
      "âœ… æ‰¹æ¬¡ 9/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step4_batches\\step4_batch_6.json\n",
      "   Stats: {'done': 50}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 10/12: step2_batches\\step2_batch_7.json\n",
      "âœ… æ‰¹æ¬¡ 10/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step4_batches\\step4_batch_7.json\n",
      "   Stats: {'done': 50}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 11/12: step2_batches\\step2_batch_8.json\n",
      "âœ… æ‰¹æ¬¡ 11/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step4_batches\\step4_batch_8.json\n",
      "   Stats: {'done': 50}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 12/12: step2_batches\\step2_batch_9.json\n",
      "âœ… æ‰¹æ¬¡ 12/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step4_batches\\step4_batch_9.json\n",
      "   Stats: {'done': 50}\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Step 4 Prompt\n",
    "# ==============================\n",
    "step4_prompt = \"\"\"You are a sociology professor with 30 years of experience analyzing racial dynamics and anti-Asian racism in the United States.\n",
    "\n",
    "### Step 4: Infer **Emotions and Their Intensity**\n",
    "\n",
    "Your task is to analyze the **extracted sentences** from **Step 2** and infer each entity's **emotional stance** toward anti-Asian hate.\n",
    "\n",
    "You will also receive metadata from Step 2, including:\n",
    "- `entity_type`: for individuals use their social role; for organizations use institutional category.\n",
    "- `asian_status`: \"Asian\", \"Non-Asian\", \"Cannot be inferred\", or \"Not applicable\"\n",
    "\n",
    "Use only the exact `relevant_sentences` from Step 2 as your source â€” do NOT paraphrase or add your own wording.\n",
    "\n",
    "---\n",
    "\n",
    "## Emotion Concept Tree\n",
    "- Love \n",
    "- Joy \n",
    "- Anger \n",
    "- Sadness \n",
    "- Fear \n",
    "- Surprise \n",
    "\n",
    "---\n",
    "\n",
    "## Instructions\n",
    "1. Focus only on emotions â€” do NOT infer actions.\n",
    "2. If no emotion is expressed, output `\"emotion\": \"Cannot be inferred\".\n",
    "3. If multiple emotions appear, list multiple objects.\n",
    "4. Use the exact sentence(s) as `\"emotion_reason\"`.\n",
    "\n",
    "---\n",
    "\n",
    "## Output format\n",
    "{\n",
    "  \"Entity Name\": {\n",
    "    \"entity_type\": \"...\",\n",
    "    \"asian_status\": \"...\",\n",
    "    \"emotions\": [\n",
    "      {\n",
    "        \"emotion\": \"deepest matched term or Cannot be inferred\",\n",
    "        \"emotion_reason\": \"Exact sentence(s)\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# ==============================\n",
    "# ä¸»æµç¨‹ Step2 â†’ Step4\n",
    "# ==============================\n",
    "def run_step4_from_step2(step2_dir=\"step2_batches\", step4_dir=\"step4_batches\"):\n",
    "    os.makedirs(step4_dir, exist_ok=True)\n",
    "    step2_files = sorted(glob.glob(os.path.join(step2_dir, \"step2_batch_*.json\")))\n",
    "    total_batches = len(step2_files)\n",
    "\n",
    "    for idx, step2_file in enumerate(step2_files, start=1):\n",
    "        batch_name = os.path.basename(step2_file).replace(\"step2_\", \"step4_\")\n",
    "        out_path = os.path.join(step4_dir, batch_name)\n",
    "\n",
    "        # æ–·é»çºŒè·‘\n",
    "        if os.path.exists(out_path):\n",
    "            print(f\"â­ï¸ æ‰¹æ¬¡ {idx}/{total_batches} {batch_name} å·²å­˜åœ¨ï¼Œè·³é\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ {idx}/{total_batches}: {step2_file}\")\n",
    "\n",
    "        with open(step2_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            step2_batch_result = json.load(f)\n",
    "\n",
    "        step4_batch_result = {}\n",
    "        emo_stats = Counter()\n",
    "\n",
    "        for title, step2_text in step2_batch_result.items():\n",
    "            full_prompt = (\n",
    "                step4_prompt +\n",
    "                f\"\\n\\nStep 2 Results (Extracted Sentences):\\n{step2_text}\"\n",
    "            )\n",
    "\n",
    "            response = get_response(full_prompt)\n",
    "            step4_batch_result[title] = response\n",
    "            emo_stats[\"done\"] += 1\n",
    "\n",
    "        # å„²å­˜\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(step4_batch_result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"âœ… æ‰¹æ¬¡ {idx}/{total_batches} å·²å®Œæˆä¸¦å„²å­˜è‡³ {out_path}\")\n",
    "        print(\"   Stats:\", dict(emo_stats))\n",
    "\n",
    "# ==============================\n",
    "# åŸ·è¡Œ\n",
    "# ==============================\n",
    "if __name__ == \"__main__\":\n",
    "    run_step4_from_step2(\"step2_batches\", \"step4_batches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8509778e-2d88-4fd6-9130-661ad83aaa9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Step 3 å·²åˆä½µ 12 å€‹æ‰¹æ¬¡æª” â†’ step3_all_new.csvï¼Œå…± 6063 ç­†\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def merge_step3_with_id(input_dir=\"step3_batches\", prefix=\"step3_batch_\", \n",
    "                        output_json=\"step3_all.json\", output_csv=\"step3_all.csv\"):\n",
    "    batch_files = sorted(glob.glob(os.path.join(input_dir, f\"{prefix}*.json\")))\n",
    "\n",
    "    merged_result = {}\n",
    "    for file in batch_files:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            merged_result.update(data)\n",
    "\n",
    "    with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(merged_result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    rows = []\n",
    "    idx = 1\n",
    "    for article_id, entities in merged_result.items():\n",
    "        for entity, meta in entities.items():\n",
    "            rows.append({\n",
    "                \"reaction_id\": f\"reaction_{idx}\",\n",
    "                \"article_id\": article_id,\n",
    "                \"entity\": entity,\n",
    "                \"entity_type\": meta.get(\"entity_type\", \"\"),\n",
    "                \"asian_status\": meta.get(\"asian_status\", \"\"),\n",
    "                \"reaction\": meta.get(\"reaction\", \"\"),\n",
    "                \"reaction_reason\": meta.get(\"reaction_reason\", \"\")\n",
    "            })\n",
    "            idx += 1\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"âœ… Step 3 å·²åˆä½µ {len(batch_files)} å€‹æ‰¹æ¬¡æª” â†’ {output_csv}ï¼Œå…± {len(df)} ç­†\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    merge_step3_with_id(\n",
    "        input_dir=\"step3_batches_new\",\n",
    "        prefix=\"step3_batch_\",\n",
    "        output_json=\"step3_all_new.json\",\n",
    "        output_csv=\"step3_all_new.csv\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "094b81c0-66dd-4e5b-8e48-f663b5550e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Step 4 å·²åˆä½µ 12 å€‹æ‰¹æ¬¡æª” â†’ step4_all.csvï¼Œå…± 8011 ç­†\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def parse_json_loose(s: str):\n",
    "    \"\"\"å˜—è©¦å¾å­—ä¸²è£¡è§£æ JSON\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return None\n",
    "    s = s.strip()\n",
    "    if not s:\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        # å˜—è©¦æŠ“ç¬¬ä¸€å€‹ {...}\n",
    "        m = re.search(r\"\\{.*\\}\", s, flags=re.S)\n",
    "        if m:\n",
    "            try:\n",
    "                return json.loads(m.group(0))\n",
    "            except Exception:\n",
    "                return None\n",
    "    return None\n",
    "\n",
    "def merge_step4_with_id(input_dir=\"step4_batches\", prefix=\"step4_batch_\", \n",
    "                        output_json=\"step4_all.json\", output_csv=\"step4_all.csv\"):\n",
    "    batch_files = sorted(glob.glob(os.path.join(input_dir, f\"{prefix}*.json\")))\n",
    "    all_articles = {}\n",
    "\n",
    "    for file in batch_files:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # âš ï¸ ä¸ç”¨ updateï¼Œé€ç¯‡å±•é–‹\n",
    "        for article_id, raw in data.items():\n",
    "            if isinstance(raw, dict):\n",
    "                all_articles[article_id] = raw\n",
    "            elif isinstance(raw, str):\n",
    "                parsed = parse_json_loose(raw)\n",
    "                if isinstance(parsed, dict):\n",
    "                    all_articles[article_id] = parsed\n",
    "\n",
    "    # å­˜ JSON\n",
    "    with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_articles, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # è½‰æˆ CSV\n",
    "    rows = []\n",
    "    idx = 1\n",
    "    for article_id, entities in all_articles.items():\n",
    "        for entity, meta in entities.items():\n",
    "            emotions = meta.get(\"emotions\", [])\n",
    "            \n",
    "            # çµ±ä¸€è™•ç†æ ¼å¼\n",
    "            if isinstance(emotions, str):\n",
    "                emotions = [{\n",
    "                    \"emotion\": emotions,\n",
    "                    \"emotion_path\": None,\n",
    "                    \"emotion_reason\": \"\"\n",
    "                }]\n",
    "            elif not isinstance(emotions, list):\n",
    "                emotions = []\n",
    "    \n",
    "            for emo in emotions:\n",
    "                if not isinstance(emo, dict):  # å†ä¿éšªä¸€æ¬¡\n",
    "                    emo = {\"emotion\": str(emo), \"emotion_path\": None, \"emotion_reason\": \"\"}\n",
    "                rows.append({\n",
    "                    \"emotion_id\": f\"emotion_{idx}\",\n",
    "                    \"article_id\": article_id,\n",
    "                    \"entity\": entity,\n",
    "                    \"entity_type\": meta.get(\"entity_type\", \"\"),\n",
    "                    \"asian_status\": meta.get(\"asian_status\", \"\"),\n",
    "                    \"emotion\": emo.get(\"emotion\", \"\"),\n",
    "                    \"emotion_path\": emo.get(\"emotion_path\", \"\"),\n",
    "                    \"emotion_reason\": emo.get(\"emotion_reason\", \"\")\n",
    "                })\n",
    "                idx += 1\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"âœ… Step 4 å·²åˆä½µ {len(batch_files)} å€‹æ‰¹æ¬¡æª” â†’ {output_csv}ï¼Œå…± {len(df)} ç­†\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    merge_step4_with_id(\n",
    "        input_dir=\"step4_batches\",\n",
    "        prefix=\"step4_batch_\",\n",
    "        output_json=\"step4_all.json\",\n",
    "        output_csv=\"step4_all.csv\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b719cfd3-826a-4595-a245-3dbb745cf6d6",
   "metadata": {},
   "source": [
    "# è™•ç†éŒ¯èª¤æ–‡ç« "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "7778de67-dfe2-41b7-9c7c-b4ba74fa673f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Debugging Article_440\n",
      "\n",
      "ğŸ” Debugging Article_465\n",
      "âœ… å·²å®Œæˆ ['Article_440', 'Article_465']ï¼Œè¼¸å‡º step3_batches_debug\\step3_batch_9__subset_debug.json\n",
      "   Gate stats: {}\n",
      "   Label stats: {'Politicians initiated anti-Asian hate and racism': 1, 'Color blind/minimizing racism': 2, 'Support Asian Americans': 2, 'Feeling hopeless or support AAPI being not enough': 2, 'Advocacy/take actions for changes': 1, 'Cannot be inferred': 3}\n"
     ]
    }
   ],
   "source": [
    "import os, json, re\n",
    "from collections import Counter\n",
    "from typing import Any, Dict\n",
    "from openai import OpenAI\n",
    "\n",
    "# ==============================\n",
    "# OpenAI client (æ›¿æ›æˆä½ è‡ªå·±çš„ key)\n",
    "# ==============================\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "def get_response(prompt: str, temperature: float = 0.0) -> str:\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        return resp.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(\"âŒ API error:\", e)\n",
    "        return \"\"\n",
    "\n",
    "# ==============================\n",
    "# JSON utilities (å¯¬é¬† parser)\n",
    "# ==============================\n",
    "def strip_code_fences(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return s\n",
    "    s = s.strip()\n",
    "    m = re.search(r\"```(?:json)?\\s*([\\s\\S]*?)\\s*```\", s, flags=re.I)\n",
    "    return m.group(1).strip() if m else s\n",
    "\n",
    "def _sanitize_json_like(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = strip_code_fences(s).lstrip(\"\\ufeff\")\n",
    "    # ç§»é™¤ä¸å¯è¦‹æ§åˆ¶å­—å…ƒï¼ˆä¿ç•™æ›è¡Œ/ç¸®æ’ï¼‰\n",
    "    s = \"\".join(ch for ch in s if ch.isprintable() or ch in \"\\n\\r\\t\")\n",
    "    # å˜—è©¦ä¿®æ­£å¸¸è¦‹ mojibake\n",
    "    if \"Ã¢â‚¬\" in s or \"Ãƒ\" in s:\n",
    "        try:\n",
    "            s = s.encode(\"latin1\").decode(\"utf-8\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    # å–æœ€å¤–å±¤ {...} æˆ– [...]\n",
    "    m = re.search(r\"(\\{[\\s\\S]*\\}|\\[[\\s\\S]*\\])\", s)\n",
    "    if m:\n",
    "        s = m.group(1)\n",
    "    # ç§»é™¤å°¾é€—è™Ÿ\n",
    "    s = re.sub(r\",(\\s*[}\\]])\", r\"\\1\", s)\n",
    "    return s.strip()\n",
    "\n",
    "def parse_json_loose(s: str):\n",
    "    if not isinstance(s, str):\n",
    "        return None\n",
    "    base = _sanitize_json_like(s)\n",
    "    if not base:\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(base)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # å–®å¼•è™Ÿ pseudo-JSON\n",
    "    if base.startswith(\"{\") and base.endswith(\"}\") and '\"' not in base and \"'\" in base:\n",
    "        try:\n",
    "            return json.loads(re.sub(r\"'\", '\"', base))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def parse_model_json(s: str, default: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return default\n",
    "    cleaned = _sanitize_json_like(s)\n",
    "    try:\n",
    "        return json.loads(cleaned)\n",
    "    except Exception:\n",
    "        m = re.search(r\"\\{[\\s\\S]*\\}\", cleaned)\n",
    "        if m:\n",
    "            try:\n",
    "                return json.loads(m.group(0))\n",
    "            except Exception:\n",
    "                return default\n",
    "        return default\n",
    "\n",
    "def to_text(x: Any) -> str:\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    if isinstance(x, str):\n",
    "        return x\n",
    "    if isinstance(x, list):\n",
    "        return \"\\n\".join(str(item) for item in x)\n",
    "    if isinstance(x, dict):\n",
    "        if \"relevant_sentences\" in x:\n",
    "            return to_text(x[\"relevant_sentences\"])\n",
    "        return \"\\n\".join(to_text(v) for v in x.values())\n",
    "    return str(x)\n",
    "\n",
    "def normalize_step2_result(title: str, raw_obj) -> Dict[str, Dict[str, str]]:\n",
    "    if isinstance(raw_obj, dict) and all(isinstance(v, dict) for v in raw_obj.values()):\n",
    "        out = {}\n",
    "        for k, v in raw_obj.items():\n",
    "            out[k] = {\n",
    "                \"entity_type\": v.get(\"entity_type\", \"\"),\n",
    "                \"asian_status\": v.get(\"asian_status\", \"\"),\n",
    "                \"relevant_sentences\": to_text(v.get(\"relevant_sentences\"))\n",
    "            }\n",
    "        return out\n",
    "    if isinstance(raw_obj, str):\n",
    "        parsed = parse_json_loose(raw_obj)\n",
    "        if parsed is not None: \n",
    "            return normalize_step2_result(title, parsed)\n",
    "    return {}\n",
    "\n",
    "def debug_json_failure(s: str, context=2):\n",
    "    cleaned = _sanitize_json_like(s)\n",
    "    try:\n",
    "        json.loads(cleaned)\n",
    "        print(\"âœ… JSON OK\")\n",
    "        return\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"âŒ JSON error @ line {e.lineno}, col {e.colno}: {e.msg}\")\n",
    "        lines = cleaned.splitlines()\n",
    "        i = e.lineno - 1\n",
    "        lo, hi = max(0, i-context), min(len(lines), i+context+1)\n",
    "        for idx in range(lo, hi):\n",
    "            mark = \">>\" if idx == i else \"  \"\n",
    "            print(f\"{mark} {idx+1:4d}: {lines[idx]}\")\n",
    "\n",
    "# ==============================\n",
    "# Prompts\n",
    "# ==============================\n",
    "def build_gate_prompt(step2_sentences: str) -> str:\n",
    "    return f\"\"\"Task: Decide if the extracted sentences show any observable reaction.\n",
    "\n",
    "Rules:\n",
    "- Observable = action/inaction or explicit stance\n",
    "- Pure emotions or incident descriptions â‰  reaction\n",
    "\n",
    "Output JSON:\n",
    "{{\n",
    "  \"has_reaction\": \"yes\" | \"no\",\n",
    "  \"evidence\": \"Exact sentence(s) from relevant_sentences\"\n",
    "}}\n",
    "\n",
    "relevant_sentences:\n",
    "{step2_sentences}\n",
    "\"\"\"\n",
    "\n",
    "def build_classifier_prompt(entity_type: str, asian_status: str, step2_sentences: str) -> str:\n",
    "    return f\"\"\"Task: Classify the entityâ€™s REACTION strictly using the Reaction Concept Tree.\n",
    "\n",
    "Reaction Concept Tree:\n",
    "- Support Asian Americans\n",
    "  - Attending marches/rallies\n",
    "  - Speaking up on social media\n",
    "  - Calling for being united\n",
    "  - Educating students\n",
    "  - Fostering conversations about anti-Asian hate\n",
    "  - Hiring security guards\n",
    "  - Providing shopkeepers with air horns\n",
    "  - Rewarding the public to report suspects\n",
    "- Advocacy/take actions for changes\n",
    "- Politicians initiated anti-Asian hate and racism\n",
    "- Undermining human rights\n",
    "- Color blind/minimizing racism\n",
    "- Youth as not an excuse\n",
    "- Videotaping/confronting harasser/attacker\n",
    "- Sex (sexual) addiction\n",
    "- Religion as a reason\n",
    "- Feeling hopeless or support AAPI being not enough\n",
    "- Not confronting attacker/harasser or not reporting\n",
    "- Useless law enforcement\n",
    "  - Did not take a report\n",
    "  - Did not patrol the streets\n",
    "- Government takes actions to stop AAPI hate\n",
    "  - Installing hotlines\n",
    "  - Launching a hate-crime task force\n",
    "  - Increasing patrols\n",
    "  - Organizing a town hall\n",
    "\n",
    "Rules:\n",
    "- If no clear reaction â†’ \"Cannot be inferred\"\n",
    "- Always choose the most specific subcategory\n",
    "\n",
    "entity_type: {entity_type}\n",
    "asian_status: {asian_status}\n",
    "\n",
    "relevant_sentences:\n",
    "{step2_sentences}\n",
    "\n",
    "Output JSON:\n",
    "{{\n",
    "  \"reaction\": \"<one label or 'Cannot be inferred'>\",\n",
    "  \"reaction_reason\": \"Exact sentence(s)\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "# ==============================\n",
    "# Step3 ä¸»æµç¨‹ + Debug\n",
    "# ==============================\n",
    "def run_step3_for_titles(step2_file=\"step2_batches/step2_batch_1.json\",\n",
    "                         titles=None,\n",
    "                         out_dir=\"step3_batches_debug\",\n",
    "                         out_suffix=\"__subset_debug\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    with open(step2_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        step2_batch_result = json.load(f)\n",
    "\n",
    "    if titles is None:\n",
    "        titles = [next(iter(step2_batch_result.keys()), None)]\n",
    "\n",
    "    filtered = {t: step2_batch_result[t] for t in titles if t in step2_batch_result}\n",
    "    if not filtered:\n",
    "        print(\"æ‰¾ä¸åˆ°æŒ‡å®šçš„ title\")\n",
    "        return\n",
    "\n",
    "    gate_stats = Counter()\n",
    "    label_stats = Counter()\n",
    "    step3_batch_result = {}\n",
    "\n",
    "    for title, raw in filtered.items():\n",
    "        print(f\"\\nğŸ” Debugging {title}\")\n",
    "        entities = normalize_step2_result(title, raw)\n",
    "        if not entities:\n",
    "            print(\"âš ï¸ ç„¡æ³•è§£æï¼Œå‘¼å« debug_json_failure\")\n",
    "            debug_json_failure(raw)\n",
    "            continue\n",
    "\n",
    "        entity_outputs = {}\n",
    "        for entity, meta in entities.items():\n",
    "            entity_type = meta.get(\"entity_type\", \"\")\n",
    "            asian_status = meta.get(\"asian_status\", \"\")\n",
    "            relevant_sentences = to_text(meta.get(\"relevant_sentences\", \"\")).strip()\n",
    "\n",
    "            # --- Gate\n",
    "            gate_resp = get_response(build_gate_prompt(relevant_sentences))\n",
    "            gate_json = parse_model_json(gate_resp, default={\"has_reaction\": \"no\", \"evidence\": \"\"})\n",
    "            if str(gate_json.get(\"has_reaction\", \"no\")).lower() != \"yes\":\n",
    "                out = {\n",
    "                    \"entity_type\": entity_type,\n",
    "                    \"asian_status\": asian_status,\n",
    "                    \"reaction\": \"Cannot be inferred\",\n",
    "                    \"reaction_reason\": \"\"\n",
    "                }\n",
    "                entity_outputs[entity] = out\n",
    "                label_stats[\"Cannot be inferred\"] += 1\n",
    "                continue\n",
    "\n",
    "            # --- Classifier\n",
    "            cls_resp = get_response(build_classifier_prompt(entity_type, asian_status, relevant_sentences))\n",
    "            cls_json = parse_model_json(cls_resp, default={\"reaction\": \"Cannot be inferred\", \"reaction_reason\": \"\"})\n",
    "            out = {\n",
    "                \"entity_type\": entity_type,\n",
    "                \"asian_status\": asian_status,\n",
    "                \"reaction\": cls_json.get(\"reaction\", \"Cannot be inferred\"),\n",
    "                \"reaction_reason\": cls_json.get(\"reaction_reason\", \"\")\n",
    "            }\n",
    "            entity_outputs[entity] = out\n",
    "            label_stats[out[\"reaction\"]] += 1\n",
    "\n",
    "        step3_batch_result[title] = entity_outputs\n",
    "\n",
    "    base = os.path.basename(step2_file).replace(\"step2_\", \"step3_\").replace(\".json\", f\"{out_suffix}.json\")\n",
    "    out_path = os.path.join(out_dir, base)\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(step3_batch_result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"âœ… å·²å®Œæˆ {list(step3_batch_result.keys())}ï¼Œè¼¸å‡º {out_path}\")\n",
    "    print(\"   Gate stats:\", dict(gate_stats))\n",
    "    print(\"   Label stats:\", dict(label_stats))\n",
    "\n",
    "\n",
    "# æˆ–è·‘å¤šç¯‡\n",
    "run_step3_for_titles(\"step2_batches/step2_batch_9.json\", titles=[\"Article_440\",\n",
    "    \"Article_465\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "b9f70092-e754-4bdb-95e5-c5116e53a868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Debugging Article_40\n",
      "âœ… å·²å®Œæˆ ['Article_40']ï¼Œè¼¸å‡º step4_batches_debug\\step4_batch_1__subset_debug.json\n",
      "   Stats: {'done': 1}\n",
      "\n",
      "ğŸ” Debugging Article_255\n",
      "âœ… å·²å®Œæˆ ['Article_255']ï¼Œè¼¸å‡º step4_batches_debug\\step4_batch_5__subset_debug.json\n",
      "   Stats: {'done': 1}\n",
      "\n",
      "ğŸ” Debugging Article_349\n",
      "\n",
      "ğŸ” Debugging Article_350\n",
      "\n",
      "ğŸ” Debugging Article_379\n",
      "âœ… å·²å®Œæˆ ['Article_349', 'Article_350', 'Article_379']ï¼Œè¼¸å‡º step4_batches_debug\\step4_batch_7__subset_debug.json\n",
      "   Stats: {'done': 3}\n",
      "\n",
      "ğŸ” Debugging Article_429\n",
      "âœ… å·²å®Œæˆ ['Article_429']ï¼Œè¼¸å‡º step4_batches_debug\\step4_batch_8__subset_debug.json\n",
      "   Stats: {'done': 1}\n",
      "\n",
      "ğŸ” Debugging Article_524\n",
      "âœ… å·²å®Œæˆ ['Article_524']ï¼Œè¼¸å‡º step4_batches_debug\\step4_batch_10__subset_debug.json\n",
      "   Stats: {'done': 1}\n",
      "\n",
      "ğŸ” Debugging Article_553\n",
      "âœ… å·²å®Œæˆ ['Article_553']ï¼Œè¼¸å‡º step4_batches_debug\\step4_batch_11__subset_debug.json\n",
      "   Stats: {'done': 1}\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os, json, re\n",
    "from collections import Counter\n",
    "from typing import Any, Dict\n",
    "from openai import OpenAI\n",
    "\n",
    "# ==============================\n",
    "# OpenAI client\n",
    "# ==============================\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "def get_response(prompt: str, temperature: float = 0.0) -> str:\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        return resp.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(\"âŒ API error:\", e)\n",
    "        return \"{}\"\n",
    "\n",
    "# ==============================\n",
    "# JSON utilities\n",
    "# ==============================\n",
    "def strip_code_fences(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return s\n",
    "    s = s.strip()\n",
    "    m = re.search(r\"```(?:json)?\\s*([\\s\\S]*?)\\s*```\", s, flags=re.I)\n",
    "    return m.group(1).strip() if m else s\n",
    "\n",
    "def _sanitize_json_like(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = strip_code_fences(s).lstrip(\"\\ufeff\")\n",
    "    s = \"\".join(ch for ch in s if ch.isprintable() or ch in \"\\n\\r\\t\")\n",
    "    m = re.search(r\"(\\{[\\s\\S]*\\}|\\[[\\s\\S]*\\])\", s)\n",
    "    if m:\n",
    "        s = m.group(1)\n",
    "    s = re.sub(r\",(\\s*[}\\]])\", r\"\\1\", s)\n",
    "    return s.strip()\n",
    "\n",
    "def parse_model_json(s: str, default: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return default\n",
    "    cleaned = _sanitize_json_like(s)\n",
    "    try:\n",
    "        return json.loads(cleaned)\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "def to_text(x: Any) -> str:\n",
    "    if x is None: return \"\"\n",
    "    if isinstance(x, str): return x\n",
    "    if isinstance(x, list): return \"\\n\".join(str(item) for item in x)\n",
    "    if isinstance(x, dict):\n",
    "        if \"relevant_sentences\" in x:\n",
    "            return to_text(x[\"relevant_sentences\"])\n",
    "        return \"\\n\".join(to_text(v) for v in x.values())\n",
    "    return str(x)\n",
    "\n",
    "def normalize_step2_result(title: str, raw_obj) -> Dict[str, Dict[str, str]]:\n",
    "    if isinstance(raw_obj, dict) and all(isinstance(v, dict) for v in raw_obj.values()):\n",
    "        out = {}\n",
    "        for k, v in raw_obj.items():\n",
    "            out[k] = {\n",
    "                \"entity_type\": v.get(\"entity_type\", \"\"),\n",
    "                \"asian_status\": v.get(\"asian_status\", \"\"),\n",
    "                \"relevant_sentences\": to_text(v.get(\"relevant_sentences\"))\n",
    "            }\n",
    "        return out\n",
    "    return {}\n",
    "\n",
    "# ==============================\n",
    "# Step4 Prompt\n",
    "# ==============================\n",
    "step4_prompt = \"\"\"You are a sociology professor with 30 years of experience analyzing racial dynamics and anti-Asian racism in the United States.\n",
    "\n",
    "### Step 4: Infer **Emotions and Their Intensity**\n",
    "\n",
    "Your task is to analyze the **extracted sentences** from **Step 2** and infer each entity's **emotional stance** toward anti-Asian hate.\n",
    "\n",
    "You will also receive metadata from Step 2, including:\n",
    "- `entity_type`\n",
    "- `asian_status`\n",
    "\n",
    "Use only the exact `relevant_sentences` from Step 2 as your source.\n",
    "\n",
    "---\n",
    "\n",
    "## Emotion Concept Tree\n",
    "- Love \n",
    "- Joy \n",
    "- Anger \n",
    "- Sadness \n",
    "- Fear \n",
    "- Surprise \n",
    "\n",
    "---\n",
    "\n",
    "## Instructions\n",
    "1. Focus only on emotions â€” do NOT infer actions.\n",
    "2. If no emotion is expressed, output `\"emotion\": \"Cannot be inferred\"`.\n",
    "3. If multiple emotions appear, list multiple objects.\n",
    "4. Use the exact sentence(s) as `\"emotion_reason\"`.\n",
    "\n",
    "---\n",
    "\n",
    "## Output format\n",
    "{\n",
    "  \"Entity Name\": {\n",
    "    \"entity_type\": \"...\",\n",
    "    \"asian_status\": \"...\",\n",
    "    \"emotions\": [\n",
    "      {\n",
    "        \"emotion\": \"deepest matched term or Cannot be inferred\",\n",
    "        \"emotion_reason\": \"Exact sentence(s)\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# ==============================\n",
    "# Step4 ä¸»æµç¨‹ + Debug\n",
    "# ==============================\n",
    "def run_step4_for_titles(step2_file=\"step2_batches/step2_batch_1.json\",\n",
    "                         titles=None,\n",
    "                         out_dir=\"step4_batches_debug\",\n",
    "                         out_suffix=\"__subset_debug\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    with open(step2_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        step2_batch_result = json.load(f)\n",
    "\n",
    "    if titles is None:\n",
    "        titles = [next(iter(step2_batch_result.keys()), None)]\n",
    "\n",
    "    filtered = {t: step2_batch_result[t] for t in titles if t in step2_batch_result}\n",
    "    if not filtered:\n",
    "        print(\"æ‰¾ä¸åˆ°æŒ‡å®šçš„ title\")\n",
    "        return\n",
    "\n",
    "    step4_batch_result = {}\n",
    "    emo_stats = Counter()\n",
    "\n",
    "    for title, raw in filtered.items():\n",
    "        print(f\"\\nğŸ” Debugging {title}\")\n",
    "        entities = normalize_step2_result(title, raw)\n",
    "        if not entities:\n",
    "            print(f\"âš ï¸ {title} ç„¡æ³•è§£æ step2 çµæœ\")\n",
    "            continue\n",
    "\n",
    "        entity_outputs = {}\n",
    "        for entity, meta in entities.items():\n",
    "            entity_type = meta.get(\"entity_type\", \"\")\n",
    "            asian_status = meta.get(\"asian_status\", \"\")\n",
    "            relevant_sentences = to_text(meta.get(\"relevant_sentences\", \"\")).strip()\n",
    "\n",
    "            full_prompt = (\n",
    "                step4_prompt +\n",
    "                f\"\\n\\nEntity: {entity}\\nentity_type: {entity_type}\\nasian_status: {asian_status}\\nrelevant_sentences:\\n{relevant_sentences}\"\n",
    "            )\n",
    "\n",
    "            resp = get_response(full_prompt)\n",
    "            parsed = parse_model_json(resp, default={\n",
    "                entity: {\n",
    "                    \"entity_type\": entity_type,\n",
    "                    \"asian_status\": asian_status,\n",
    "                    \"emotions\": [{\"emotion\": \"Cannot be inferred\", \"emotion_reason\": \"\"}]\n",
    "                }\n",
    "            })\n",
    "            entity_outputs[entity] = parsed.get(entity, parsed)\n",
    "\n",
    "        step4_batch_result[title] = entity_outputs\n",
    "        emo_stats[\"done\"] += 1\n",
    "\n",
    "    # === è¼¸å‡º ===\n",
    "    base = os.path.basename(step2_file).replace(\"step2_\", \"step4_\").replace(\".json\", f\"{out_suffix}.json\")\n",
    "    out_path = os.path.join(out_dir, base)\n",
    "\n",
    "    # å¦‚æœæª”æ¡ˆå·²ç¶“å­˜åœ¨ â†’ åˆä½µèˆŠçµæœ\n",
    "    if os.path.exists(out_path):\n",
    "        with open(out_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            old_data = json.load(f)\n",
    "        old_data.update(step4_batch_result)\n",
    "        step4_batch_result = old_data\n",
    "\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(step4_batch_result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"âœ… å·²å®Œæˆ {list(filtered.keys())}ï¼Œè¼¸å‡º {out_path}\")\n",
    "    print(\"   Stats:\", dict(emo_stats))\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# åŸ·è¡Œç¯„ä¾‹\n",
    "# ==============================\n",
    "run_step4_for_titles(\"step2_batches/step2_batch_1.json\", titles=[\"Article_40\"])\n",
    "run_step4_for_titles(\"step2_batches/step2_batch_5.json\", titles=[\"Article_255\"])\n",
    "run_step4_for_titles(\"step2_batches/step2_batch_7.json\", titles=[\"Article_349\", \"Article_350\", \"Article_379\"])\n",
    "run_step4_for_titles(\"step2_batches/step2_batch_8.json\", titles=[\"Article_429\"])\n",
    "run_step4_for_titles(\"step2_batches/step2_batch_10.json\", titles=[\"Article_524\"])\n",
    "run_step4_for_titles(\"step2_batches/step2_batch_11.json\", titles=[\"Article_553\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "7495c32c-97c5-4f61-aa7b-fb9db37cc517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Re-running Article_220\n",
      "âœ… Article_220 å·²å®Œæˆ\n",
      "ğŸ’¾ å·²å¯«å› step3_batches_debug\\step3_batch_5__subset_debug.jsonï¼Œç›®å‰å…± 7 ç¯‡\n"
     ]
    }
   ],
   "source": [
    "def rerun_step3_to_subset(step2_file, titles, out_dir=\"step3_batches_debug\", out_suffix=\"__subset_debug\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # è®€ step2\n",
    "    with open(step2_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        step2_batch_result = json.load(f)\n",
    "\n",
    "    # éæ¿¾å‡ºè¦è·‘çš„æ–‡ç« \n",
    "    filtered = {t: step2_batch_result[t] for t in titles if t in step2_batch_result}\n",
    "    if not filtered:\n",
    "        print(\"âš ï¸ æ‰¾ä¸åˆ°æŒ‡å®šçš„ titles\")\n",
    "        return\n",
    "\n",
    "    step3_batch_result = {}\n",
    "\n",
    "    for title, raw in filtered.items():\n",
    "        print(f\"\\nğŸ” Re-running {title}\")\n",
    "        entities = normalize_step2_result(title, raw)\n",
    "        if not entities:\n",
    "            print(f\"âš ï¸ {title} step2 ç„¡æ³•è§£æ\")\n",
    "            debug_json_failure(raw)\n",
    "            continue\n",
    "\n",
    "        entity_outputs = {}\n",
    "        for entity, meta in entities.items():\n",
    "            entity_type = meta.get(\"entity_type\", \"\")\n",
    "            asian_status = meta.get(\"asian_status\", \"\")\n",
    "            relevant_sentences = to_text(meta.get(\"relevant_sentences\", \"\")).strip()\n",
    "\n",
    "            # --- Gate\n",
    "            gate_resp = get_response(build_gate_prompt(relevant_sentences))\n",
    "            gate_json = parse_model_json(gate_resp, default={\"has_reaction\": \"no\", \"evidence\": \"\"})\n",
    "            if str(gate_json.get(\"has_reaction\", \"no\")).lower() != \"yes\":\n",
    "                entity_outputs[entity] = {\n",
    "                    \"entity_type\": entity_type,\n",
    "                    \"asian_status\": asian_status,\n",
    "                    \"reaction\": \"Cannot be inferred\",\n",
    "                    \"reaction_reason\": \"\"\n",
    "                }\n",
    "                continue\n",
    "\n",
    "            # --- Classifier\n",
    "            cls_resp = get_response(build_classifier_prompt(entity_type, asian_status, relevant_sentences))\n",
    "            cls_json = parse_model_json(cls_resp, default={\"reaction\": \"Cannot be inferred\", \"reaction_reason\": \"\"})\n",
    "            entity_outputs[entity] = {\n",
    "                \"entity_type\": entity_type,\n",
    "                \"asian_status\": asian_status,\n",
    "                \"reaction\": cls_json.get(\"reaction\", \"Cannot be inferred\"),\n",
    "                \"reaction_reason\": cls_json.get(\"reaction_reason\", \"\")\n",
    "            }\n",
    "\n",
    "        step3_batch_result[title] = entity_outputs\n",
    "        print(f\"âœ… {title} å·²å®Œæˆ\")\n",
    "\n",
    "    # è¼¸å‡ºåˆ° subset æª”æ¡ˆ\n",
    "    base = os.path.basename(step2_file).replace(\"step2_\", \"step3_\").replace(\".json\", f\"{out_suffix}.json\")\n",
    "    out_path = os.path.join(out_dir, base)\n",
    "\n",
    "    # å¦‚æœ subset æª”æ¡ˆå·²å­˜åœ¨ â†’ åˆä½µ\n",
    "    if os.path.exists(out_path):\n",
    "        with open(out_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            existing = json.load(f)\n",
    "    else:\n",
    "        existing = {}\n",
    "\n",
    "    existing.update(step3_batch_result)\n",
    "\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(existing, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"ğŸ’¾ å·²å¯«å› {out_path}ï¼Œç›®å‰å…± {len(existing)} ç¯‡\")\n",
    "\n",
    "rerun_step3_to_subset(\n",
    "    step2_file=\"step2_batches/step2_batch_5.json\",\n",
    "    titles=[\"Article_220\"],\n",
    "    out_dir=\"step3_batches_debug\",\n",
    "    out_suffix=\"__subset_debug\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "8d13f2b8-486d-43b5-bd9e-b573fb898179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ è¼‰å…¥æ—¢æœ‰ step3_all.jsonï¼Œå·²æœ‰ 584 ç¯‡æ–‡ç« \n",
      "ğŸ” åµæ¸¬åˆ° 11 å€‹ subset æª”æ¡ˆ\n",
      "âœ… åˆä½µ step3_batch_10__subset_debug.jsonï¼Œ+0 ç¯‡\n",
      "âœ… åˆä½µ step3_batch_11__subset_debug.jsonï¼Œ+0 ç¯‡\n",
      "âœ… åˆä½µ step3_batch_1__subset_debug.jsonï¼Œ+0 ç¯‡\n",
      "âœ… åˆä½µ step3_batch_2__subset_debug.jsonï¼Œ+0 ç¯‡\n",
      "âœ… åˆä½µ step3_batch_3__subset_debug.jsonï¼Œ+0 ç¯‡\n",
      "âœ… åˆä½µ step3_batch_4__subset_debug.jsonï¼Œ+0 ç¯‡\n",
      "âœ… åˆä½µ step3_batch_5__subset_debug.jsonï¼Œ+0 ç¯‡\n",
      "âœ… åˆä½µ step3_batch_6__subset_debug.jsonï¼Œ+0 ç¯‡\n",
      "âœ… åˆä½µ step3_batch_7__subset_debug.jsonï¼Œ+0 ç¯‡\n",
      "âœ… åˆä½µ step3_batch_8__subset_debug.jsonï¼Œ+0 ç¯‡\n",
      "âœ… åˆä½µ step3_batch_9__subset_debug.jsonï¼Œ+0 ç¯‡\n",
      "ğŸ’¾ å·²æ›´æ–° step3_all.json å’Œ step3_all.csvï¼Œå…± 6063 ç­† reactions\n"
     ]
    }
   ],
   "source": [
    "# ä¿®å¾©å¾Œçš„çµæœåˆä½µåˆ°step3_all\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def merge_step3_with_existing(all_json=\"step3_all.json\", all_csv=\"step3_all.csv\",\n",
    "                              subset_dir=\"step3_batches_debug\",\n",
    "                              prefix=\"step3_batch_\", suffix=\"__subset_debug.json\"):\n",
    "    # å…ˆè®€èˆŠçš„ all.json\n",
    "    if os.path.exists(all_json):\n",
    "        with open(all_json, \"r\", encoding=\"utf-8\") as f:\n",
    "            merged_result = json.load(f)\n",
    "        print(f\"ğŸ“‚ è¼‰å…¥æ—¢æœ‰ {all_json}ï¼Œå·²æœ‰ {len(merged_result)} ç¯‡æ–‡ç« \")\n",
    "    else:\n",
    "        merged_result = {}\n",
    "        print(f\"âš ï¸ æ‰¾ä¸åˆ° {all_json}ï¼Œå»ºç«‹æ–°æª”æ¡ˆ\")\n",
    "\n",
    "    # æ‰¾ subset æª”æ¡ˆ\n",
    "    subset_files = sorted(glob.glob(os.path.join(subset_dir, f\"{prefix}*{suffix}\")))\n",
    "    print(f\"ğŸ” åµæ¸¬åˆ° {len(subset_files)} å€‹ subset æª”æ¡ˆ\")\n",
    "\n",
    "    # åˆä½µ subset â†’ all\n",
    "    for file in subset_files:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            before = len(merged_result)\n",
    "            merged_result.update(data)  # âš ï¸ è‹¥æœ‰åŒä¸€ç¯‡æ–‡ç« ï¼Œæœƒè¦†è“‹\n",
    "            after = len(merged_result)\n",
    "        print(f\"âœ… åˆä½µ {os.path.basename(file)}ï¼Œ+{after-before} ç¯‡\")\n",
    "\n",
    "    # å­˜å› all.json\n",
    "    with open(all_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(merged_result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # è½‰æˆ CSV\n",
    "    rows = []\n",
    "    idx = 1\n",
    "    for article_id, entities in merged_result.items():\n",
    "        for entity, meta in entities.items():\n",
    "            rows.append({\n",
    "                \"reaction_id\": f\"reaction_{idx}\",\n",
    "                \"article_id\": article_id,\n",
    "                \"entity\": entity,\n",
    "                \"entity_type\": meta.get(\"entity_type\", \"\"),\n",
    "                \"asian_status\": meta.get(\"asian_status\", \"\"),\n",
    "                \"reaction\": meta.get(\"reaction\", \"\"),\n",
    "                \"reaction_reason\": meta.get(\"reaction_reason\", \"\")\n",
    "            })\n",
    "            idx += 1\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(all_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"ğŸ’¾ å·²æ›´æ–° {all_json} å’Œ {all_csv}ï¼Œå…± {len(df)} ç­† reactions\")\n",
    "\n",
    "# ä½¿ç”¨ç¯„ä¾‹\n",
    "if __name__ == \"__main__\":\n",
    "    merge_step3_with_existing(\n",
    "        all_json=\"step3_all.json\",\n",
    "        all_csv=\"step3_all.csv\",\n",
    "        subset_dir=\"step3_batches_debug\",\n",
    "        prefix=\"step3_batch_\",\n",
    "        suffix=\"__subset_debug.json\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b0ecdafc-1264-4916-a3e1-8d703b730f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ è¼‰å…¥æ—¢æœ‰ step4_all.jsonï¼Œå·²æœ‰ 584 ç¯‡æ–‡ç« \n",
      "ğŸ” åµæ¸¬åˆ° 6 å€‹ subset æª”æ¡ˆ\n",
      "âœ… åˆä½µ step4_batch_10__subset_debug.jsonï¼Œ+0 ç¯‡\n",
      "âœ… åˆä½µ step4_batch_11__subset_debug.jsonï¼Œ+0 ç¯‡\n",
      "âœ… åˆä½µ step4_batch_1__subset_debug.jsonï¼Œ+0 ç¯‡\n",
      "âœ… åˆä½µ step4_batch_5__subset_debug.jsonï¼Œ+0 ç¯‡\n",
      "âœ… åˆä½µ step4_batch_7__subset_debug.jsonï¼Œ+0 ç¯‡\n",
      "âœ… åˆä½µ step4_batch_8__subset_debug.jsonï¼Œ+0 ç¯‡\n",
      "ğŸ’¾ å·²æ›´æ–° step4_all.json å’Œ step4_all.csvï¼Œå…± 8127 ç­† emotions\n"
     ]
    }
   ],
   "source": [
    "# ä¿®å¾©å¾Œçš„çµæœåˆä½µåˆ°step4_all\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def merge_step4_with_existing(all_json=\"step4_all.json\", all_csv=\"step4_all.csv\",\n",
    "                              subset_dir=\"step4_batches_debug\",\n",
    "                              prefix=\"step4_batch_\", suffix=\"__subset_debug.json\"):\n",
    "    # å…ˆè®€èˆŠçš„ all.json\n",
    "    if os.path.exists(all_json):\n",
    "        with open(all_json, \"r\", encoding=\"utf-8\") as f:\n",
    "            merged_result = json.load(f)\n",
    "        print(f\"ğŸ“‚ è¼‰å…¥æ—¢æœ‰ {all_json}ï¼Œå·²æœ‰ {len(merged_result)} ç¯‡æ–‡ç« \")\n",
    "    else:\n",
    "        merged_result = {}\n",
    "        print(f\"âš ï¸ æ‰¾ä¸åˆ° {all_json}ï¼Œå»ºç«‹æ–°æª”æ¡ˆ\")\n",
    "\n",
    "    # æ‰¾ subset æª”æ¡ˆ\n",
    "    subset_files = sorted(glob.glob(os.path.join(subset_dir, f\"{prefix}*{suffix}\")))\n",
    "    print(f\"ğŸ” åµæ¸¬åˆ° {len(subset_files)} å€‹ subset æª”æ¡ˆ\")\n",
    "\n",
    "    # åˆä½µ subset â†’ all\n",
    "    for file in subset_files:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            before = len(merged_result)\n",
    "            merged_result.update(data)  # âš ï¸ è‹¥æœ‰åŒä¸€ç¯‡æ–‡ç« ï¼Œæœƒè¦†è“‹\n",
    "            after = len(merged_result)\n",
    "        print(f\"âœ… åˆä½µ {os.path.basename(file)}ï¼Œ+{after-before} ç¯‡\")\n",
    "\n",
    "    # å­˜å› all.json\n",
    "    with open(all_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(merged_result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # è½‰æˆ CSV\n",
    "    rows = []\n",
    "    idx = 1\n",
    "    for article_id, entities in merged_result.items():\n",
    "        for entity, meta in entities.items():\n",
    "            emotions = meta.get(\"emotions\", [])\n",
    "\n",
    "            # emotions å¯èƒ½æ˜¯å­—ä¸²æˆ– list\n",
    "            if isinstance(emotions, str):\n",
    "                rows.append({\n",
    "                    \"emotion_id\": f\"emotion_{idx}\",\n",
    "                    \"article_id\": article_id,\n",
    "                    \"entity\": entity,\n",
    "                    \"entity_type\": meta.get(\"entity_type\", \"\"),\n",
    "                    \"asian_status\": meta.get(\"asian_status\", \"\"),\n",
    "                    \"emotion\": emotions,\n",
    "                    \"emotion_reason\": \"\"\n",
    "                })\n",
    "                idx += 1\n",
    "\n",
    "            elif isinstance(emotions, list):\n",
    "                if not emotions:  # ç©º list\n",
    "                    rows.append({\n",
    "                        \"emotion_id\": f\"emotion_{idx}\",\n",
    "                        \"article_id\": article_id,\n",
    "                        \"entity\": entity,\n",
    "                        \"entity_type\": meta.get(\"entity_type\", \"\"),\n",
    "                        \"asian_status\": meta.get(\"asian_status\", \"\"),\n",
    "                        \"emotion\": \"Cannot be inferred\",\n",
    "                        \"emotion_reason\": \"\"\n",
    "                    })\n",
    "                    idx += 1\n",
    "                else:\n",
    "                    for emo in emotions:\n",
    "                        if isinstance(emo, dict):\n",
    "                            rows.append({\n",
    "                                \"emotion_id\": f\"emotion_{idx}\",\n",
    "                                \"article_id\": article_id,\n",
    "                                \"entity\": entity,\n",
    "                                \"entity_type\": meta.get(\"entity_type\", \"\"),\n",
    "                                \"asian_status\": meta.get(\"asian_status\", \"\"),\n",
    "                                \"emotion\": emo.get(\"emotion\", \"Cannot be inferred\"),\n",
    "                                \"emotion_reason\": emo.get(\"emotion_reason\", \"\")\n",
    "                            })\n",
    "                        else:  # list è£¡é‚„æ˜¯å­—ä¸²\n",
    "                            rows.append({\n",
    "                                \"emotion_id\": f\"emotion_{idx}\",\n",
    "                                \"article_id\": article_id,\n",
    "                                \"entity\": entity,\n",
    "                                \"entity_type\": meta.get(\"entity_type\", \"\"),\n",
    "                                \"asian_status\": meta.get(\"asian_status\", \"\"),\n",
    "                                \"emotion\": str(emo),\n",
    "                                \"emotion_reason\": \"\"\n",
    "                            })\n",
    "                        idx += 1\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(all_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"ğŸ’¾ å·²æ›´æ–° {all_json} å’Œ {all_csv}ï¼Œå…± {len(df)} ç­† emotions\")\n",
    "\n",
    "\n",
    "# ä½¿ç”¨ç¯„ä¾‹\n",
    "if __name__ == \"__main__\":\n",
    "    merge_step4_with_existing(\n",
    "        all_json=\"step4_all.json\",\n",
    "        all_csv=\"step4_all.csv\",\n",
    "        subset_dir=\"step4_batches_debug\",\n",
    "        prefix=\"step4_batch_\",\n",
    "        suffix=\"__subset_debug.json\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "f21615a2-63d8-476f-aa5c-61a2ecfd3787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Article_349 å·²ä¿®å¾©ä¸¦å¯«å› step2_batches/step2_batch_7.json\n"
     ]
    }
   ],
   "source": [
    "# è‡ªå‹•ä¿®å¾©\n",
    "\n",
    "import json, re\n",
    "\n",
    "def fix_relevant_sentences_block(txt: str) -> str:\n",
    "    \"\"\"\n",
    "    æ‰¾å‡º relevant_sentences å€å¡Šï¼ŒæŠŠè£¡é¢çš„å¥å­é€æ¢ä¿®æ­£\n",
    "    \"\"\"\n",
    "    def fix_sentence_block(match):\n",
    "        block = match.group(0)\n",
    "        # æŠ½å–å¥å­å…§å®¹ï¼ˆåˆªæ‰ JSON æ ¼å¼ï¼‰\n",
    "        sentences = re.findall(r'\"(.*?)\"', block, flags=re.S)\n",
    "        fixed = []\n",
    "        for s in sentences:\n",
    "            # åˆä½µå ±å°å¼å¼•è™Ÿ\n",
    "            s = re.sub(r',\"\\s*([A-Z][^\"]+?\\s+said)', r', \\1', s)\n",
    "            s = re.sub(r'\"\\s*([A-Z][^\"]+?\\s+said)', r' \\1', s)\n",
    "            # åˆªæ‰å¥å­ä¸­æ®˜ç•™çš„è£¸å¼•è™Ÿ\n",
    "            s = s.replace('\\\\\"', '\"')  # é¿å…é‡è¤‡ escape\n",
    "            s = re.sub(r'(?<!\\\\)\"', \"'\", s)  # æŠŠå…§éƒ¨è£¸å¼•è™Ÿæ›æˆå–®å¼•è™Ÿ\n",
    "            fixed.append(s.strip())\n",
    "        # é‡å»ºæˆåˆæ³• JSON é™£åˆ—\n",
    "        rebuilt = \"[\\n      \" + \",\\n      \".join(json.dumps(s) for s in fixed) + \"\\n    ]\"\n",
    "        return rebuilt\n",
    "\n",
    "    # æ‰¾ relevant_sentences block\n",
    "    return re.sub(r'\\[\\s*(\".*?\")\\s*\\]', fix_sentence_block, txt, flags=re.S)\n",
    "\n",
    "\n",
    "# def fix_unclosed_blocks(txt: str) -> str:\n",
    "#     \"\"\"\n",
    "#     å˜—è©¦ä¿®å¾©æœªé—œé–‰çš„ relevant_sentences å€å¡Š\n",
    "#     \"\"\"\n",
    "#     # å¦‚æœæœ‰ relevant_sentences: [ ä½†å¾Œé¢æ²’æœ‰ ]\n",
    "#     if '\"relevant_sentences\": [' in txt and not re.search(r'\\]\\s*\\}', txt):\n",
    "#         print(\"ğŸ”§ åµæ¸¬åˆ° relevant_sentences æœªé—œé–‰ï¼Œè£œä¸Š ] }\")\n",
    "#         # è£œä¸Šç¼ºå¤±çš„çµå°¾\n",
    "#         txt = re.sub(r'(\"relevant_sentences\": \\[[^\\]]+)$',\n",
    "#                      r'\\1\\n    ]\\n  }',\n",
    "#                      txt, flags=re.S)\n",
    "#     return txt\n",
    "    \n",
    "def remove_trailing_commas(txt: str) -> str:\n",
    "    \"\"\"\n",
    "    ç§»é™¤ JSON ä¸­ä¸åˆæ³•çš„å°¾é€—è™Ÿ\n",
    "    \"\"\"\n",
    "    # é™£åˆ—æˆ–ç‰©ä»¶çµå°¾å‰çš„é€—è™Ÿ\n",
    "    txt = re.sub(r\",(\\s*[\\]}])\", r\"\\1\", txt)\n",
    "    return txt\n",
    "\n",
    "def fix_outer_braces(txt: str) -> str:\n",
    "    \"\"\"\n",
    "    ç¢ºä¿ JSON ä»¥ { é–‹é ­ï¼Œä»¥ } çµå°¾\n",
    "    \"\"\"\n",
    "    txt = txt.strip()\n",
    "    if not txt.startswith(\"{\"):\n",
    "        txt = \"{\\n\" + txt\n",
    "    if not txt.endswith(\"}\"):\n",
    "        txt = txt + \"\\n}\"\n",
    "    return txt\n",
    "\n",
    "def fix_unclosed_blocks(txt: str) -> str:\n",
    "    if '\"relevant_sentences\": [' in txt and not re.search(r'\\]\\s*[\\},]', txt):\n",
    "        print(\"ğŸ”§ åµæ¸¬åˆ° relevant_sentences æœªé—œé–‰ï¼Œè£œä¸Š ] }\")\n",
    "        txt = re.sub(r'(\"relevant_sentences\": \\[[^\\]]+)$',\n",
    "                     r'\\1\\n    ]\\n  }',\n",
    "                     txt, flags=re.S)\n",
    "    return txt\n",
    "\n",
    "def sanitize_relevant_sentences(txt: str) -> str:\n",
    "    def fix_block(match):\n",
    "        block = match.group(0)\n",
    "        sentences = re.findall(r'\"(.*?)\"', block, flags=re.S)\n",
    "\n",
    "        fixed = []\n",
    "        for s in sentences:\n",
    "            s = s.replace(\"\\n\", \" \").strip()   # æ¸…ç†æ›è¡Œ\n",
    "            fixed.append(s)\n",
    "\n",
    "        return \"[\\n      \" + \",\\n      \".join(json.dumps(s) for s in fixed) + \"\\n    ]\"\n",
    "\n",
    "    return re.sub(r'\\[\\s*(\".*?\")\\s*\\]', fix_block, txt, flags=re.S)\n",
    "\n",
    "\n",
    "\n",
    "def fix_single_article(step2_file, article_id):\n",
    "    with open(step2_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    raw = data[article_id]\n",
    "\n",
    "    if isinstance(raw, str):\n",
    "        txt = raw.strip()\n",
    "        if txt.startswith(\"```\"):\n",
    "            txt = re.sub(r\"^```json\", \"\", txt, flags=re.I).strip()\n",
    "            txt = re.sub(r\"```$\", \"\", txt).strip()\n",
    "\n",
    "        # ä¿® relevant_sentences\n",
    "        txt = fix_relevant_sentences_block(txt)\n",
    "        txt = sanitize_relevant_sentences(txt)\n",
    "        txt = fix_unclosed_blocks(txt)\n",
    "        txt = remove_trailing_commas(txt)\n",
    "        txt = fix_outer_braces(txt)\n",
    "\n",
    "        try:\n",
    "            parsed = json.loads(txt)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"âŒ ä¿®å¾©å¾Œä»ç„¡æ³• parse: {e}\")\n",
    "            print(\"âš ï¸ ä¿®å¾©å¾Œç‰‡æ®µ:\\n\", \"\\n\".join(txt.splitlines()[:20]))\n",
    "            return\n",
    "\n",
    "        # âœ… æ”¾å› Article_x\n",
    "        data[article_id] = parsed\n",
    "\n",
    "    # âœ… å­˜å›å®Œæ•´ batch\n",
    "    with open(step2_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"âœ… {article_id} å·²ä¿®å¾©ä¸¦å¯«å› {step2_file}\")\n",
    "\n",
    "\n",
    "\n",
    "# æ¸¬è©¦ä¿®å¾© batch_2 çš„å¹¾ç¯‡æ–‡ç« \n",
    "# fix_single_article(\"step2_batches/step2_batch_2.json\", \"Article_59\")\n",
    "# fix_single_article(\"step2_batches/step2_batch_4.json\", \"Article_197\")\n",
    "fix_single_article(\"step2_batches/step2_batch_7.json\", \"Article_349\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c598583f-420c-4e92-a27b-a30c5ada2927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Article_220 å·²æ‰‹å‹•ä¿®å¾©\n",
      "ğŸ’¾ å·²å¯«å› step2_batches/step2_batch_5.json\n"
     ]
    }
   ],
   "source": [
    "# æ‰‹å‹•ä¿®å¾©\n",
    "\n",
    "import json\n",
    "\n",
    "def manual_fix_articles(step2_file, fixes: dict):\n",
    "    \"\"\"\n",
    "    fixes: dictï¼Œkey = Article_xï¼Œvalue = Python dictï¼ˆä¿®æ­£ç‰ˆ JSONï¼‰\n",
    "    \"\"\"\n",
    "    with open(step2_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for article_id, fixed_content in fixes.items():\n",
    "        if article_id in data:\n",
    "            data[article_id] = fixed_content\n",
    "            print(f\"âœ… {article_id} å·²æ‰‹å‹•ä¿®å¾©\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ {article_id} ä¸å­˜åœ¨æ–¼ {step2_file}\")\n",
    "\n",
    "    with open(step2_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"ğŸ’¾ å·²å¯«å› {step2_file}\")\n",
    "\n",
    "\n",
    "# =======================\n",
    "# ç›´æ¥æ‰‹å‹•ä¿®å¾©ä¸‰ç¯‡æ–‡ç« \n",
    "# =======================\n",
    "fixes_batch5 = {\n",
    "    \"Article_220\": {\n",
    "        \"Sen. Kevin Thomas (D-Levittown)\": {\n",
    "            \"entity_type\": \"politician\",\n",
    "            \"asian_status\": \"Asian\",\n",
    "            \"relevant_sentences\": [\n",
    "                \"We are not looking for any different way of treating us, said Sen. Kevin Thomas (D-Levittown), who arrived at the United States at the age of 10.\",\n",
    "                \"Just treat us the same as everyone else.\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "manual_fix_articles(\"step2_batches/step2_batch_5.json\", fixes_batch5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5da521f-6d0d-4e01-b97a-b3da2327e4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"step2_batches\\\\step2_batch_10.json\": [\n",
      "    \"Article_492\",\n",
      "    \"Article_493\",\n",
      "    \"Article_524\"\n",
      "  ],\n",
      "  \"step2_batches\\\\step2_batch_11.json\": [\n",
      "    \"Article_553\"\n",
      "  ],\n",
      "  \"step2_batches\\\\step2_batch_5.json\": [\n",
      "    \"Article_218\"\n",
      "  ],\n",
      "  \"step2_batches\\\\step2_batch_7.json\": [\n",
      "    \"Article_350\",\n",
      "    \"Article_379\"\n",
      "  ],\n",
      "  \"step2_batches\\\\step2_batch_8.json\": [\n",
      "    \"Article_429\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import glob, json\n",
    "\n",
    "missing_articles = [\n",
    "    \"Article_218\", \"Article_492\", \"Article_493\", \"Article_350\", \"Article_379\", \"Article_429\", \"Article_524\", \"Article_553\"\n",
    "]\n",
    "\n",
    "step2_files = sorted(glob.glob(\"step2_batches/step2_batch_*.json\"))\n",
    "missing_map = {}\n",
    "\n",
    "for f in step2_files:\n",
    "    with open(f, \"r\", encoding=\"utf-8\") as fh:   # âœ… ç”¨ fh\n",
    "        data = json.load(fh)                    # âœ… json.load(fh) è€Œä¸æ˜¯ f\n",
    "    for art in missing_articles:\n",
    "        if art in data:\n",
    "            missing_map.setdefault(f, []).append(art)\n",
    "\n",
    "print(json.dumps(missing_map, indent=2, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02abf62-6b50-4158-96e5-e868a510cf72",
   "metadata": {},
   "source": [
    "# é‡åˆ† entity_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28dbb82d-0a39-4fe6-a614-2b3379ab7f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… finish\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === 1. è®€å–åŸå§‹è³‡æ–™ ===\n",
    "df = pd.read_csv(\"step3_all_new.csv\")  # æ›æˆä½ çš„æª”æ¡ˆåç¨±\n",
    "\n",
    "# === 2. å®šç¾©æ¨™æº–åŒ–å°æ‡‰è¡¨ ===\n",
    "entity_type_mapping = {\n",
    "    # Individuals\n",
    "    \"victim\": \"victims\",\n",
    "    \"victims\": \"victims\",\n",
    "    \"perpetrator\": \"perpetrators\",\n",
    "    \"perpetrators\": \"perpetrators\",\n",
    "    \"politician\": \"politicians\",\n",
    "    \"politicians\": \"politicians\",\n",
    "    \"professional\": \"professionals\",\n",
    "    \"professionals\": \"professionals\",\n",
    "    \"celebrity\": \"celebrities\",\n",
    "    \"musician\": \"celebrities\",\n",
    "    \"actor\": \"celebrities\",\n",
    "    \"actress\": \"celebrities\",\n",
    "    \"journalist\": \"professionals\",\n",
    "    \"reporter\": \"professionals\",\n",
    "    \"professor\": \"professionals\",\n",
    "    \"student\": \"professionals\",\n",
    "    \"educator\": \"professionals\",\n",
    "    \"scholar\": \"professionals\",\n",
    "    \"adjunct instructor\": \"professionals\",\n",
    "    \"attorney\": \"professionals\",\n",
    "    \"director\": \"professionals\",\n",
    "    \"city_manager\": \"professionals\",\n",
    "    \"sociology professor\": \"professionals\",\n",
    "    \"editor\": \"professionals\",\n",
    "    \"deputy inspector\": \"law_enforcement_agencies\",\n",
    "    \"police_officer\": \"law_enforcement_agencies\",\n",
    "    \"police spokesperson\": \"law_enforcement_agencies\",\n",
    "    \"family_member\": \"other_individuals\",\n",
    "    \"friend\": \"other_individuals\",\n",
    "    \"witness\": \"other_individuals\",\n",
    "    \"individual\": \"other_individuals\",\n",
    "    \"individuals\": \"other_individuals\",\n",
    "    \"general public\": \"other_individuals\",\n",
    "    \"general_public\": \"other_individuals\",\n",
    "    \"social_circle\": \"other_individuals\",\n",
    "    \"community_activist\": \"other_individuals\",\n",
    "    \"community_leader\": \"other_individuals\",\n",
    "    \"organizer\": \"other_individuals\",\n",
    "    \"community organizer\": \"other_individuals\",\n",
    "    \"other individual\": \"other_individuals\",\n",
    "    \"rally organizer\": \"other_individuals\",\n",
    "    \"activist\": \"other_individuals\",\n",
    "    \"supporter\": \"other_individuals\",\n",
    "    \"co-host\": \"celebrities\",\n",
    "    \"artist\": \"celebrities\",\n",
    "    \"former assistant district attorney\": \"professionals\",\n",
    "    \"official\": \"professionals\",\n",
    "    \"non-Asian\": \"other_individuals\",\n",
    "    \"youth coordinator\": \"professionals\",\n",
    "    \"school_board_member\": \"professionals\",\n",
    "    \"Dean\": \"professionals\",\n",
    "    \"community leader\": \"other_individuals\",\n",
    "    \"government body\": \"government_bodies\",\n",
    "\n",
    "    # Organizations\n",
    "    \"law_enforcement_agency\": \"law_enforcement_agencies\",\n",
    "    \"law_enforcement_agencies\": \"law_enforcement_agencies\",\n",
    "    \"government_body\": \"government_bodies\",\n",
    "    \"government_bodies\": \"government_bodies\",\n",
    "    \"ngo_or_advocacy_group\": \"ngo_or_advocacy_groups\",\n",
    "    \"ngo_or_advocacy_groups\": \"ngo_or_advocacy_groups\",\n",
    "    \"business_entity\": \"business_entities\",\n",
    "    \"business_entities\": \"business_entities\",\n",
    "    \"community_group\": \"community_groups\",\n",
    "    \"community_groups\": \"community_groups\",\n",
    "    \"educational_institution\": \"government_bodies\",  # å‡è¨­ç‚ºæ­£å¼æ©Ÿæ§‹\n",
    "\n",
    "    # Fallback\n",
    "    \"other\": \"other_individuals\",\n",
    "    \"other_individual\": \"other_individuals\",\n",
    "    \"other_individuals\": \"other_individuals\",\n",
    "    \"group\": \"unknown\",\n",
    "    # \"Cannot be inferred\": \"unknown\",\n",
    "}\n",
    "\n",
    "# === 3. æ›¿æ› entity_type æ¬„ä½ï¼ˆç›´æ¥è¦†è“‹ï¼‰===\n",
    "df[\"entity_type\"] = df[\"entity_type\"].map(entity_type_mapping).fillna(df[\"entity_type\"])\n",
    "\n",
    "# === 4. è¼¸å‡ºæˆæ–°æª”æ¡ˆ ===\n",
    "df.to_csv(\"step3_all_new.csv\", index=False)\n",
    "print(\"âœ… finish\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db2ccddd-d383-49a3-b74a-6dd57ab3f180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” æ‰¾åˆ° 118 ç­† fostering éœ€è¦é‡åˆ†\n",
      "âœ… å·²å®Œæˆé‡åˆ†ï¼Œè¼¸å‡ºåˆ° step3_all_refined.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# ======================\n",
    "# LLM åˆ†é¡æç¤º\n",
    "# ======================\n",
    "def build_refine_prompt(relevant_sentences: str) -> str:\n",
    "    return f\"\"\"You are a sociology professor re-checking misclassified reactions to anti-Asian hate.\n",
    "\n",
    "Task: Re-classify the reaction based ONLY on the exact `reaction_reason`.\n",
    "\n",
    "Reaction Concept Tree:\n",
    "- Support Asian Americans:People or organizations condemned/do not want Anti-Asian incidents to happen, including all forms of crimes, attacks, violence, assaults, physical, verbal, and online harassment. This kind of support is at a conscientious (cognitive) level, not yet taking concrete actions to stop AAPI hate. \n",
    "  - Attending marches/rallies:People or organizations actively attended marches/rallies for supporting Asian American communities.\n",
    "  - Speaking up on social media:People or organizations spoke up in public, such as via social media, to condemn Anti-Asian hate.\n",
    "  - Calling for being united:Asians/Asian Americans become united to support each other to overcome Anti-Asian hate.\n",
    "  - Fostering conversations about anti-Asian hate:The public fostered conversations regarding the Anti-Asian hate issues. Some organizations/groups (e.g., Asian American communities) also think itâ€™s important to have conversations to address the root cause behind the Anti-Asian hate incidents so this will help us to make change.\n",
    "  - Providing shopkeepers with air horns:Some stores or groups provide shopkeepers with air horns. If any anti-Asian hate crimes happened, they can use the air horns to draw everyoneâ€™s attention\n",
    "- Advocacy/take actions for changes:Individuals, groups, or organizations want or advocate for changing the current situation where Asians/Asian Americans suffered from Anti-Asian hate, such as physical, verbal, and online harassment, attacks, violence, assaults, and hate crimes. They want cultural shift, open dialogue and listening sessions regarding incidents, practical change in racial stereotypes\n",
    "and perceptions, more awareness about Anti-Asian hate, and human rights.\n",
    "- Politicians initiated anti-Asian hate and racism:Trump and some republican politicians made a lot of comments on the COVID-19 pandemic. One kind of comment is that he used/dubbed Asian-related objects to combine with disease (virus/flu/covid) or directly calling coronavirus/covid (e.g., countryâ€™s or regionâ€™s names or Kung flu[modified by Kung Fu], or races) to verbally attack Asians/Asian Americans, such as  â€œChina/Chinese virusâ€ or â€œKung flu.â€ Another type of comment is that he blamed China for causing the pandemic. Such kinds of comments initiated racism, Anti-Asian bigotry/hate, and Anti-Asian hate incidents in the US society. Such comments also led some Americans to blame Asians/Asian Americans for causing the pandemic.\n",
    "- Undermining human rights:People want to dehumanize and to undermine the fundamental rights, dignity and belonging of those they target.\n",
    "- Color blind/minimizing racism:Some Americans do not believe discrimination, racism, or racist bias/bigotry against Asians/Asian Americans exists in the community. Incidents of anti-Asian hate (including physical, verbal, and online harassment, attacks, violence, assaults, and Anti-Asian crimes) were downplayed, ignored, or perceived as not existing by the public, the law enforcement system (e.g., the police), and governors (e.g., Mr. Donald Trump). When Asians/Asian Americans were attacked, Anti-Asian hate or racism was not perceived as the perpetratorsâ€™ motives/motivations by the police or the perpetrators said their motivations were not triggered by Anti-Asian bigotry or racism.\n",
    "- Youth as not an excuse:Robert Aaron Long murdered eight people in the incident of the 2021 Atlanta Spa Shootings. In news reports, he was called â€œthe 21-year-old.â€ Some comments advocated stopping calling him â€œthe 21-year-oldâ€ as if his youth is an excuse to murder others because of their race, ethnicities, and sex.\n",
    "- Videotaping/confronting harasser/attacker:Asians/Asian Americans or bystanders videotaped/recorded the incidents of physical or verbal harassment; Anti-Asian attacks, assaults, or violence; and Anti-Asian crimes. Asians/Asian Americans who suffered physical harassment, attacks, assaults, violence attacked back to the harassers or attackers. Bystandersâ€™ behaviors aim to defense those who were attacked. Cell phones and survelliance system can be used for videotaping or recording. Additionally, those who experienced verbal harassment speak out to the harassers to let them know their thoughts were biased, offensive, and unjust and tell them to stop. \n",
    "- Sex (sexual) addiction:Excessive sexual thoughts, desires, urges or behaviors that canâ€™t be controlled and cause distress and harm to your relationships, finances and other aspects of life. It is also called hypersexuality or compulsive sexual behavior. It is what the Atlanta shooter claimed as a motivation that led to his senseless killings of the victims.\n",
    "- Religion as a reason:In the 2021 Atlanta Spa Shootings, Robert Aaron Long was the killer who murdered eight people. He told the police that his motive was religious guilt about his sexuality. He said he had sexual desire so he wanted to eliminate it. Thatâ€™s why he went to the spa to skill women of Asian descent. Asian advocacy groups mentioned whether the killerâ€™s motive was religious guilt about his sexuality, no one should ignore the broader context of Anti-Asian violence and hate crimes. Asian advocacy groups tend to attribute the killerâ€™s motive stems from racism or xenophobia, misogyny, and gendered racism\n",
    "- Feeling hopeless or support AAPI being not enough:Asians/Asian Americans felt worried, frustrated, anxious, and afraid that they may experience Anti-Asian hate crimes, attacks, assaults, and violence. But they felt that nothing happened to stop them. Support for Asian American communities is not enough.\n",
    "- Not confronting attacker/harasser or not reporting:Asians/Asian Americans did not want to confront attackers/harassers/bullies who physically or verbally harassed or attacked them. They thought it is not worthy of reporting the incidents. They did not want to confront because they were afraid of their safety. They just wanted to leave from the incidents soon.\n",
    "- Useless law enforcement:Police did not take a police report and denied there was an Anti-Asian hate crime for the incidents of physical, verbal, or online harassment, attacks, assaults, violence, and Anti-Asian crimes. Another situation is that police affirmed there was a crime, but the motivation did not come from Anti-Asian hate or bigotry/prejudice or racism. Additionally, Asian Americans thought if police often patrolled the streets, a lot of Anti-Asian hate crimes, attacks, assaults, and violence would not happen. But in reality, policy did not do so. \n",
    "  - Did not take a report on Anti-Asian hate crime:police did not take a report on Anti-Asian hate crime, including physical, verbal, or online harassment, attacks, assaults, and violence.\n",
    "  - Did not often patrol the streets:police affirmed there was a crime, but police did not often patrol the streets so that there were a lot of Anti-Asian hate crimes (e.g., physical, verbal, or online harassment, attacks, assaults, and violence) happened.\n",
    "- Takes actions to stop AAPI hate:After the incidents of Anti-Asian hate crimes, attacks, assaults, and violence, state or city government or individuals take concrete actions that aim to stop AAPI hate.\n",
    "  - Installing hotlines:This is a type of action to stop AAPI hate. Some organizations (e.g., city and state governments) install hotlines for victims or people who witness Anti-Asian incidents to report.\n",
    "  - Launching a hate-crime task force:This is a type of action to stop AAPI hate. Some organizations (e.g., city and state governments) launched an Asian hate crime task force to develop approaches to stopping anti-Asian hate crimes.\n",
    "  - Making an announcement to condemn anti-Asian hate:This is a type of action to stop AAPI hate. Some organizations (e.g., city and state governments)  made an open announcement to condemn anti-Asian hate.\n",
    "  - Increasing patrols:Some organizations (e.g., city and state governments) increased patrolling the streets to ensure the safety of Asian Americans. \n",
    "  - Organizing a town hall:Some organizations (e.g., city and state governments) organized a town hall meeting to discuss how to stop anti-Asian hate racism.\n",
    "  - Hiring security guards:Some stores or groups hired security guards to increase safety for Asian Americans and prevent anti-Asian hate crimes or racism.\n",
    "  - Educating students:Schoolteachers and university faculty took actions to educate students on current social and political issues on Anti-Asian hate. They aim to use education to change the publicâ€™s view about Asian Americans/Asians and increase the awareness of respecting Asian Americans/Asians\n",
    "  - Rewarding the public to report the info about the suspects:Individuals, groups, or organizations provide rewards to the public when they report any information regarding the suspects who may commit anti-Asian hate crimes.\n",
    "\n",
    "\n",
    "Strict Rules:\n",
    "- Use ONLY the given reaction_reason, no outside knowledge.\n",
    "- If no observable reaction, return \"Cannot be inferred\".\n",
    "- Always pick the most specific category.\n",
    "\n",
    "reaction_reason:\n",
    "{relevant_sentences}\n",
    "\n",
    "Output JSON:\n",
    "{{\n",
    "  \"reaction\": \"<one label from the tree>\",\n",
    "  \"reaction_reason\": \"{relevant_sentences}\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "def get_llm_response(prompt: str) -> dict:\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",   # å¯æ›æˆä½ å¸¸ç”¨çš„æ¨¡å‹\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.0\n",
    "    )\n",
    "    text = resp.choices[0].message.content.strip()\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except:\n",
    "        return {\"reaction\": \"Cannot be inferred\", \"reaction_reason\": \"\"}\n",
    "\n",
    "# ======================\n",
    "# ä¸»ç¨‹å¼ï¼šé‡åˆ† Fostering\n",
    "# ======================\n",
    "# def reclassify_fostering(input_csv=\"step3_all_new.csv\", output_csv=\"step3_all_refined.csv\"):\n",
    "#     df = pd.read_csv(input_csv)\n",
    "\n",
    "#     # æ‰¾å‡º fostering çš„è³‡æ–™\n",
    "#     mask = df['reaction'] == \"Fostering conversations about anti-Asian hate\"\n",
    "#     fostering_df = df[mask].copy()\n",
    "\n",
    "#     print(f\"ğŸ” æ‰¾åˆ° {len(fostering_df)} ç­† fostering éœ€è¦é‡åˆ†\")\n",
    "\n",
    "#     new_labels = []\n",
    "#     for _, row in fostering_df.iterrows():\n",
    "#         prompt = build_refine_prompt(str(row['reaction_reason']))\n",
    "#         result = get_llm_response(prompt)\n",
    "#         new_labels.append(result.get(\"reaction\", \"Cannot be inferred\"))\n",
    "\n",
    "#     # æ›´æ–°å›å»\n",
    "#     df.loc[mask, \"reaction\"] = new_labels\n",
    "\n",
    "#     # å­˜æ–°æª”\n",
    "#     df.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
    "#     print(f\"âœ… å·²å®Œæˆé‡åˆ†ï¼Œè¼¸å‡ºåˆ° {output_csv}\")\n",
    "\n",
    "# # ======================\n",
    "# # åŸ·è¡Œ\n",
    "# # ======================\n",
    "# if __name__ == \"__main__\":\n",
    "#     reclassify_fostering()\n",
    "\n",
    "def reclassify_fostering(input_csv=\"step3_all_refined.csv\", output_csv=\"step3_all_new_refined.csv\"):\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # æ‰¾å‡º fostering çš„è³‡æ–™\n",
    "    mask = df['reaction'] == \"Support Asian Americans\"\n",
    "    fostering_df = df[mask].copy()\n",
    "\n",
    "    print(f\"ğŸ” æ‰¾åˆ° {len(fostering_df)} ç­† fostering éœ€è¦é‡åˆ†\")\n",
    "\n",
    "    new_labels = []\n",
    "    for _, row in fostering_df.iterrows():\n",
    "        prompt = build_refine_prompt(str(row['reaction_reason']))\n",
    "        result = get_llm_response(prompt)\n",
    "        new_labels.append(result.get(\"reaction\", \"Cannot be inferred\"))\n",
    "\n",
    "    # æ›´æ–°å›å»\n",
    "    df.loc[mask, \"reaction\"] = new_labels\n",
    "\n",
    "    # å­˜æ–°æª”\n",
    "    df.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… å·²å®Œæˆé‡åˆ†ï¼Œè¼¸å‡ºåˆ° {output_csv}\")\n",
    "\n",
    "# ======================\n",
    "# åŸ·è¡Œ\n",
    "# ======================\n",
    "if __name__ == \"__main__\":\n",
    "    reclassify_fostering()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a569e0-82bd-437f-97cd-a4ea32796158",
   "metadata": {},
   "source": [
    "# é‡åˆ† emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c67d52-5974-4ae1-a4ed-0e8ec62ccaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# è¼‰å…¥ CSV\n",
    "df = pd.read_csv(\"step4_all_with_date.csv\")\n",
    "\n",
    "# å®šç¾© emotion å°ç…§è¡¨ï¼ˆç´°åˆ†é¡ â†’ å…­å¤§æƒ…ç·’ï¼Œå…¨å°å¯«ï¼‰\n",
    "emotion_map = {\n",
    "    # love\n",
    "    \"love\": \"love\",\n",
    "    \"support\": \"love\", \"solidarity\": \"love\",\n",
    "    \"empathy\": \"love\", \"compassion\": \"love\",\n",
    "    \"recognition\": \"love\", \"gratitude\": \"love\",\n",
    "    \"appreciation\": \"love\", \"encouragement\": \"love\",\n",
    "    \"affection\": \"love\", \"lust\": \"love\", \"longing\": \"love\",\n",
    "\n",
    "    # joy\n",
    "    \"joy\": \"joy\",\n",
    "    \"confidence\": \"joy\", \"optimism\": \"joy\", \"empowerment\": \"joy\",\n",
    "    \"cheerfulness\": \"joy\", \"zest\": \"joy\", \"contentment\": \"joy\",\n",
    "    \"pride\": \"joy\", \"relief\": \"joy\",\n",
    "\n",
    "    # anger\n",
    "    \"anger\": \"anger\",\n",
    "    \"outrage\": \"anger\", \"defiance\": \"anger\", \"responsibility\": \"anger\",\n",
    "    \"irritation\": \"anger\", \"exasperation\": \"anger\", \"rage\": \"anger\",\n",
    "    \"disgust\": \"anger\", \"envy\": \"anger\", \"determination\": \"anger\",\n",
    "    \"urgency\": \"anger\", \"frustration\": \"anger\",\n",
    "\n",
    "    # sadness\n",
    "    \"sadness\": \"sadness\",\n",
    "    \"worry\": \"sadness\", \"resignation\": \"sadness\", \"regret\": \"sadness\",\n",
    "    \"mixed emotions\": \"sadness\", \"dismay\": \"sadness\",\n",
    "    \"disquiet\": \"sadness\", \"disturbance\": \"sadness\",\n",
    "    \"guilt\": \"sadness\",\n",
    "    \"suffering\": \"sadness\", \"disappointment\": \"sadness\", \"shame\": \"sadness\",\n",
    "    \"neglect\": \"sadness\", \"sympathy\": \"sadness\", \"heartbreak\": \"sadness\",\n",
    "    \"pain\": \"sadness\", \"grief\": \"sadness\", \"grieving\": \"sadness\",\n",
    "    \"hurt\": \"sadness\", \"loneliness\": \"sadness\", \"despondency\": \"sadness\",\n",
    "    \"helplessness\": \"sadness\", \"exhaustion\": \"sadness\",\n",
    "\n",
    "    # fear\n",
    "    \"fear\": \"fear\",\n",
    "    \"terror\": \"fear\", \"doubt\": \"fear\",\n",
    "    \"alarm\": \"fear\", \"anxiety\": \"fear\", \"insecurity\": \"fear\",\n",
    "    \"panic\": \"fear\", \"dread\": \"fear\", \"overwhelming\": \"fear\",\n",
    "    \"overwhelmed\": \"fear\", \"horror\": \"fear\", \"shock\": \"fear\",\n",
    "\n",
    "    # surprise\n",
    "    \"surprise\": \"surprise\",\n",
    "    \"confusion\": \"surprise\", \"lightbulb moment\": \"surprise\",\n",
    "    \"amazement\": \"surprise\", \"wonder\": \"surprise\"\n",
    "}\n",
    "\n",
    "def map_emotions(emotion_str):\n",
    "    \"\"\"æŠŠæƒ…ç·’æ”¶æ–‚æˆå…­å¤§åŸºæœ¬æƒ…ç·’ï¼Œå…¶ä»–æ­¸ç‚º cannot be inferredï¼Œå…¨å°å¯«\"\"\"\n",
    "    if pd.isna(emotion_str):\n",
    "        return \"cannot be inferred\"\n",
    "    emotions = [e.strip().lower() for e in emotion_str.split(\"|\")]\n",
    "    mapped = [emotion_map.get(e, \"cannot be inferred\") for e in emotions]\n",
    "    mapped = list(dict.fromkeys(mapped))  # å»é‡ä½†ä¿ç•™é †åº\n",
    "    return \" | \".join(mapped)\n",
    "\n",
    "# å»ºç«‹æ–°çš„æ¬„ä½\n",
    "df[\"emotion\"] = df[\"emotion\"].apply(map_emotions)\n",
    "\n",
    "# è¼¸å‡ºçµæœ\n",
    "df.to_csv(\"step4_all_with_date.csv\", index=False)\n",
    "print(\"âœ… å·²å®Œæˆï¼šemotion å…¨éƒ¨è½‰æˆå°å¯« (love, joy, anger, sadness, fear, surprise, cannot be inferred)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "6f46e2f9-a698-4ca8-9b94-36b6c8508a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²è®€å– step2_batch_1.jsonï¼Œç›®å‰ç¸½æ–‡ç« æ•¸ï¼š50\n",
      "âœ… å·²è®€å– step2_batch_10.jsonï¼Œç›®å‰ç¸½æ–‡ç« æ•¸ï¼š100\n",
      "âœ… å·²è®€å– step2_batch_11.jsonï¼Œç›®å‰ç¸½æ–‡ç« æ•¸ï¼š150\n",
      "âœ… å·²è®€å– step2_batch_12.jsonï¼Œç›®å‰ç¸½æ–‡ç« æ•¸ï¼š184\n",
      "âœ… å·²è®€å– step2_batch_2.jsonï¼Œç›®å‰ç¸½æ–‡ç« æ•¸ï¼š234\n",
      "âœ… å·²è®€å– step2_batch_3.jsonï¼Œç›®å‰ç¸½æ–‡ç« æ•¸ï¼š284\n",
      "âœ… å·²è®€å– step2_batch_4.jsonï¼Œç›®å‰ç¸½æ–‡ç« æ•¸ï¼š334\n",
      "âœ… å·²è®€å– step2_batch_5.jsonï¼Œç›®å‰ç¸½æ–‡ç« æ•¸ï¼š384\n",
      "âœ… å·²è®€å– step2_batch_6.jsonï¼Œç›®å‰ç¸½æ–‡ç« æ•¸ï¼š434\n",
      "âœ… å·²è®€å– step2_batch_7.jsonï¼Œç›®å‰ç¸½æ–‡ç« æ•¸ï¼š484\n",
      "âœ… å·²è®€å– step2_batch_8.jsonï¼Œç›®å‰ç¸½æ–‡ç« æ•¸ï¼š534\n",
      "âœ… å·²è®€å– step2_batch_9.jsonï¼Œç›®å‰ç¸½æ–‡ç« æ•¸ï¼š584\n",
      "ğŸ’¾ å·²è¼¸å‡º step2_all.json å’Œ step2_all.csvï¼Œå…± 6068 ç­† entities\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def merge_step2_to_csv(input_dir=\"step2_batches\", prefix=\"step2_batch_\", \n",
    "                       output_json=\"step2_all.json\", output_csv=\"step2_all.csv\"):\n",
    "    batch_files = sorted(glob.glob(os.path.join(input_dir, f\"{prefix}*.json\")))\n",
    "    merged_result = {}\n",
    "\n",
    "    # åˆä½µæ‰€æœ‰ batch JSON\n",
    "    for file in batch_files:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            merged_result.update(data)\n",
    "        print(f\"âœ… å·²è®€å– {os.path.basename(file)}ï¼Œç›®å‰ç¸½æ–‡ç« æ•¸ï¼š{len(merged_result)}\")\n",
    "\n",
    "    # å­˜æˆ step2_all.json\n",
    "    with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(merged_result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # è½‰æˆ CSV\n",
    "    rows = []\n",
    "    idx = 1\n",
    "    for article_id, entities in merged_result.items():\n",
    "        if isinstance(entities, dict):  # æ­£å¸¸ç‹€æ³\n",
    "            for entity, meta in entities.items():\n",
    "                rows.append({\n",
    "                    \"id\": f\"entity_{idx}\",  # â† é€™è£¡æ”¹æˆ id\n",
    "                    \"article_id\": article_id,\n",
    "                    \"entity\": entity,\n",
    "                    \"entity_type\": meta.get(\"entity_type\", \"\"),\n",
    "                    \"asian_status\": meta.get(\"asian_status\", \"\"),\n",
    "                    \"relevant_sentences\": \"\\n\".join(meta.get(\"relevant_sentences\", [])) \n",
    "                                           if isinstance(meta.get(\"relevant_sentences\", []), list)\n",
    "                                           else str(meta.get(\"relevant_sentences\", \"\"))\n",
    "                })\n",
    "                idx += 1\n",
    "        else:\n",
    "            # å¦‚æœ step2 æœ‰å£æ‰çš„ï¼ˆå­˜æˆå­—ä¸²ï¼‰ï¼Œå°±ä¿ç•™åŸå§‹\n",
    "            rows.append({\n",
    "                \"id\": f\"entity_{idx}\",  # â† åŒæ¨£æ”¹æˆ id\n",
    "                \"article_id\": article_id,\n",
    "                \"entity\": \"\",\n",
    "                \"entity_type\": \"\",\n",
    "                \"asian_status\": \"\",\n",
    "                \"relevant_sentences\": str(entities)\n",
    "            })\n",
    "            idx += 1\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"ğŸ’¾ å·²è¼¸å‡º {output_json} å’Œ {output_csv}ï¼Œå…± {len(df)} ç­† entities\")\n",
    "\n",
    "# åŸ·è¡Œ\n",
    "if __name__ == \"__main__\":\n",
    "    merge_step2_to_csv(\n",
    "        input_dir=\"step2_batches\",\n",
    "        prefix=\"step2_batch_\",\n",
    "        output_json=\"step2_all.json\",\n",
    "        output_csv=\"step2_all.csv\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
