{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "925aefd2-80c7-4d84-9617-3b2f1f26cc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ CSV ä¸­çš„æ–‡ç« ç·¨è™Ÿï¼š\n",
      "['Article_1', 'Article_2', 'Article_3', 'Article_4', 'Article_5', 'Article_6', 'Article_8', 'Article_9', 'Article_10', 'Article_11', 'Article_12', 'Article_13', 'Article_14', 'Article_15', 'Article_16', 'Article_17', 'Article_18', 'Article_19', 'Article_20', 'Article_21', 'Article_22', 'Article_23', 'Article_24', 'Article_25', 'Article_26', 'Article_27', 'Article_28', 'Article_30', 'Article_31', 'Article_32', 'Article_33', 'Article_34', 'Article_35', 'Article_36', 'Article_37', 'Article_38', 'Article_39', 'Article_40', 'Article_42', 'Article_43', 'Article_44', 'Article_45', 'Article_46', 'Article_47', 'Article_48', 'Article_49', 'Article_50', 'Article_52', 'Article_53', 'Article_56', 'Article_57', 'Article_58', 'Article_59', 'Article_60', 'Article_61', 'Article_62', 'Article_65', 'Article_66', 'Article_67', 'Article_68', 'Article_69', 'Article_70', 'Article_71', 'Article_72', 'Article_73', 'Article_74', 'Article_75', 'Article_76', 'Article_79', 'Article_80', 'Article_81', 'Article_82', 'Article_83', 'Article_84', 'Article_85', 'Article_86', 'Article_87', 'Article_88', 'Article_89', 'Article_90', 'Article_91', 'Article_92', 'Article_93', 'Article_94', 'Article_96', 'Article_97', 'Article_98', 'Article_99', 'Article_100', 'Article_101', 'Article_102', 'Article_103', 'Article_104', 'Article_105', 'Article_107', 'Article_108', 'Article_109', 'Article_110', 'Article_112', 'Article_113', 'Article_114', 'Article_115', 'Article_116', 'Article_117', 'Article_118', 'Article_119', 'Article_120', 'Article_121', 'Article_122', 'Article_123', 'Article_124', 'Article_125', 'Article_126', 'Article_127', 'Article_128', 'Article_129', 'Article_130', 'Article_131', 'Article_132', 'Article_133', 'Article_134', 'Article_136', 'Article_137', 'Article_139', 'Article_140', 'Article_141', 'Article_142', 'Article_143', 'Article_144', 'Article_145', 'Article_147', 'Article_148', 'Article_149', 'Article_150', 'Article_151', 'Article_152', 'Article_153', 'Article_154', 'Article_155', 'Article_156', 'Article_157', 'Article_158', 'Article_159', 'Article_160', 'Article_161', 'Article_162', 'Article_163', 'Article_164', 'Article_165', 'Article_166', 'Article_167', 'Article_168', 'Article_169', 'Article_170', 'Article_171', 'Article_172', 'Article_173', 'Article_174', 'Article_175', 'Article_176', 'Article_177', 'Article_178', 'Article_179', 'Article_180', 'Article_181', 'Article_182', 'Article_183', 'Article_184', 'Article_185', 'Article_186', 'Article_187', 'Article_188', 'Article_189', 'Article_190', 'Article_191', 'Article_192', 'Article_193', 'Article_194', 'Article_195', 'Article_196', 'Article_197', 'Article_198', 'Article_199', 'Article_200', 'Article_201', 'Article_202', 'Article_203', 'Article_204', 'Article_205', 'Article_206', 'Article_207', 'Article_208', 'Article_209', 'Article_210', 'Article_211', 'Article_212', 'Article_213', 'Article_214', 'Article_216', 'Article_217', 'Article_218', 'Article_219', 'Article_220', 'Article_221', 'Article_222', 'Article_223', 'Article_224', 'Article_225', 'Article_226', 'Article_227', 'Article_228', 'Article_230', 'Article_231', 'Article_232', 'Article_233', 'Article_235', 'Article_236', 'Article_237', 'Article_238', 'Article_239', 'Article_240', 'Article_241', 'Article_242', 'Article_243', 'Article_244', 'Article_245', 'Article_246', 'Article_247', 'Article_248', 'Article_249', 'Article_250', 'Article_251', 'Article_252', 'Article_253', 'Article_255', 'Article_256', 'Article_257', 'Article_258', 'Article_259', 'Article_260', 'Article_262', 'Article_263', 'Article_264', 'Article_265', 'Article_266', 'Article_267', 'Article_268', 'Article_270', 'Article_272', 'Article_273', 'Article_274', 'Article_275', 'Article_276', 'Article_277', 'Article_278', 'Article_279', 'Article_280', 'Article_281', 'Article_282', 'Article_283', 'Article_284', 'Article_285', 'Article_286', 'Article_288', 'Article_290', 'Article_291', 'Article_292', 'Article_293', 'Article_294', 'Article_296', 'Article_297', 'Article_298', 'Article_299', 'Article_300', 'Article_301', 'Article_302', 'Article_303', 'Article_304', 'Article_305', 'Article_306', 'Article_307', 'Article_308', 'Article_309', 'Article_310', 'Article_311', 'Article_312', 'Article_313', 'Article_314', 'Article_315', 'Article_316', 'Article_317', 'Article_318', 'Article_319', 'Article_320', 'Article_321', 'Article_322', 'Article_323', 'Article_324', 'Article_325', 'Article_326', 'Article_327', 'Article_328', 'Article_329', 'Article_330', 'Article_331', 'Article_332', 'Article_333', 'Article_334', 'Article_335', 'Article_336', 'Article_337', 'Article_338', 'Article_339', 'Article_340', 'Article_341', 'Article_342', 'Article_343', 'Article_344', 'Article_345', 'Article_346', 'Article_347', 'Article_348', 'Article_349', 'Article_350', 'Article_351', 'Article_352', 'Article_355', 'Article_356', 'Article_357', 'Article_358', 'Article_359', 'Article_360', 'Article_361', 'Article_363', 'Article_364', 'Article_365', 'Article_366', 'Article_367', 'Article_368', 'Article_369', 'Article_370', 'Article_371', 'Article_373', 'Article_374', 'Article_375', 'Article_376', 'Article_377', 'Article_379', 'Article_380', 'Article_381', 'Article_382', 'Article_383', 'Article_384', 'Article_385', 'Article_386', 'Article_387', 'Article_388', 'Article_389', 'Article_390', 'Article_391', 'Article_392', 'Article_393', 'Article_394', 'Article_395', 'Article_396', 'Article_397', 'Article_398', 'Article_399', 'Article_400', 'Article_401', 'Article_402', 'Article_403', 'Article_404', 'Article_405', 'Article_406', 'Article_409', 'Article_410', 'Article_411', 'Article_412', 'Article_413', 'Article_414', 'Article_415', 'Article_416', 'Article_417', 'Article_418', 'Article_419', 'Article_420', 'Article_421', 'Article_422', 'Article_423', 'Article_424', 'Article_425', 'Article_426', 'Article_427', 'Article_428', 'Article_429', 'Article_430', 'Article_431', 'Article_433', 'Article_434', 'Article_435', 'Article_436', 'Article_437', 'Article_438', 'Article_439', 'Article_440', 'Article_441', 'Article_442', 'Article_443', 'Article_445', 'Article_446', 'Article_448', 'Article_449', 'Article_450', 'Article_451', 'Article_452', 'Article_453', 'Article_454', 'Article_455', 'Article_456', 'Article_457', 'Article_458', 'Article_459', 'Article_460', 'Article_461', 'Article_463', 'Article_465', 'Article_466', 'Article_467', 'Article_468', 'Article_469', 'Article_470', 'Article_471', 'Article_473', 'Article_474', 'Article_475', 'Article_476', 'Article_477', 'Article_478', 'Article_479', 'Article_480', 'Article_481', 'Article_483', 'Article_484', 'Article_485', 'Article_486', 'Article_487', 'Article_488', 'Article_489', 'Article_490', 'Article_492', 'Article_493', 'Article_494', 'Article_495', 'Article_496', 'Article_499', 'Article_500', 'Article_501', 'Article_503', 'Article_504', 'Article_505', 'Article_506', 'Article_507', 'Article_508', 'Article_510', 'Article_511', 'Article_512', 'Article_513', 'Article_514', 'Article_515', 'Article_516', 'Article_517', 'Article_518', 'Article_519', 'Article_520', 'Article_521', 'Article_522', 'Article_523', 'Article_524', 'Article_525', 'Article_526', 'Article_527', 'Article_528', 'Article_529', 'Article_530', 'Article_531', 'Article_532', 'Article_533', 'Article_534', 'Article_535', 'Article_536', 'Article_537', 'Article_538', 'Article_539', 'Article_540', 'Article_541', 'Article_542', 'Article_543', 'Article_544', 'Article_545', 'Article_546', 'Article_547', 'Article_548', 'Article_549', 'Article_550', 'Article_551', 'Article_552', 'Article_553', 'Article_554', 'Article_555', 'Article_556', 'Article_557', 'Article_558', 'Article_559', 'Article_560', 'Article_561', 'Article_562', 'Article_563', 'Article_564', 'Article_565', 'Article_566', 'Article_567', 'Article_568', 'Article_569', 'Article_570', 'Article_571', 'Article_572', 'Article_573', 'Article_574', 'Article_575', 'Article_576', 'Article_577', 'Article_578', 'Article_579', 'Article_580', 'Article_581', 'Article_582', 'Article_583', 'Article_584', 'Article_585', 'Article_586', 'Article_587', 'Article_588', 'Article_589', 'Article_591', 'Article_592', 'Article_593', 'Article_594', 'Article_596', 'Article_597', 'Article_598', 'Article_599', 'Article_600', 'Article_601', 'Article_602', 'Article_603', 'Article_604', 'Article_605', 'Article_606', 'Article_607', 'Article_609', 'Article_610', 'Article_611', 'Article_613', 'Article_614', 'Article_616', 'Article_617', 'Article_618', 'Article_619', 'Article_620', 'Article_621', 'Article_622', 'Article_623', 'Article_624', 'Article_625', 'Article_626', 'Article_627', 'Article_628', 'Article_629', 'Article_630', 'Article_631', 'Article_632', 'Article_633', 'Article_634']\n",
      "âœ… ç¸½å…±æœ‰ 584 ç¯‡æ–‡ç« \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from typing import Dict, Any\n",
    "from openai import OpenAI\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "# ==============================\n",
    "# Gemini è¨­å®šèˆ‡å‘¼å«\n",
    "# ==============================\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "# é¸ä¸€å€‹æ¨¡å‹ï¼šå¸¸ç”¨ \"gemini-1.5-flash\"ï¼ˆå¿«ï¼‰æˆ– \"gemini-1.5-pro\"ï¼ˆæº–ï¼‰\n",
    "_model_name = \"gemini-1.5-flash\"\n",
    "\n",
    "def get_response(prompt: str, temperature: float = 0.0) -> str:\n",
    "    \"\"\"å‘¼å« Geminiï¼Œå›å‚³æ¨¡å‹è¼¸å‡ºï¼ˆç´”æ–‡å­—ï¼‰\"\"\"\n",
    "    try:\n",
    "        model = genai.GenerativeModel(_model_name)\n",
    "        resp = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config=genai.types.GenerationConfig(\n",
    "                temperature=temperature,\n",
    "            )\n",
    "        )\n",
    "        # å–ç¬¬ä¸€æ®µæ–‡å­—ï¼›è‹¥ä½ æœŸæœ› JSONï¼Œå¯åœ¨ prompt è£¡è¦æ±‚åš´æ ¼ JSON æ ¼å¼\n",
    "        return resp.text or \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"API èª¿ç”¨éŒ¯èª¤: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# ==============================\n",
    "# è®€å–æ–‡ç« ï¼ˆåªç‚ºäº†ä¿ç•™é †åºï¼Œä¸ä¸Ÿé€²æ¨¡å‹ï¼‰\n",
    "# ==============================\n",
    "import pandas as pd\n",
    "\n",
    "# è®€å– CSV\n",
    "df = pd.read_csv(\"./articles_584.csv\", encoding=\"ISO-8859-1\")\n",
    "\n",
    "# åªä¿ç•™æœ‰æ–‡ç« çš„è³‡æ–™\n",
    "df = df.dropna(subset=[\"ARTICLE_TEXT\"])\n",
    "\n",
    "# ç¢ºä¿ id æ¬„ä½æ˜¯æ•´æ•¸å‹æ…‹ï¼ˆé¿å…å¾Œé¢æ‹¼æ¥å‡ºå•é¡Œï¼‰\n",
    "df[\"id\"] = df[\"id\"].astype(int)\n",
    "\n",
    "# å»ºç«‹ dictï¼šç”¨ CSV è£¡çš„ id ç•¶ç·¨è™Ÿ\n",
    "articles = {f\"Article_{row['id']}\": row[\"ARTICLE_TEXT\"] for _, row in df.iterrows()}\n",
    "\n",
    "# åˆ—å‡ºå…¨éƒ¨æ–‡ç« ç·¨è™Ÿ\n",
    "all_articles = list(articles.keys())\n",
    "print(\"ğŸ“„ CSV ä¸­çš„æ–‡ç« ç·¨è™Ÿï¼š\")\n",
    "print(all_articles)  # å…ˆåªå°å‰ 20 ç­†\n",
    "print(f\"âœ… ç¸½å…±æœ‰ {len(all_articles)} ç¯‡æ–‡ç« \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c453f1f-c7bb-4338-8446-6a9e374f52db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å¯¦éš›æ–‡ç« æ•¸é‡: 584 / 584\n",
      "âŒ ç¼ºå°‘çš„ Article ç·¨è™Ÿï¼ˆå…± 45 ç¯‡ï¼‰:\n",
      "Article_7, Article_29, Article_41, Article_51, Article_54, Article_55, Article_63, Article_64, Article_77, Article_78, Article_95, Article_106, Article_111, Article_135, Article_138, Article_146, Article_215, Article_229, Article_234, Article_254, Article_261, Article_269, Article_271, Article_287, Article_289, Article_295, Article_353, Article_354, Article_362, Article_372, Article_378, Article_407, Article_408, Article_432, Article_444, Article_447, Article_462, Article_464, Article_472, Article_482, Article_491, Article_497, Article_498, Article_502, Article_509\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# è¼¸å…¥ä½ çš„ CSV æª”æ¡ˆåç¨±\n",
    "csv_file = \"articles_584.csv\"\n",
    "\n",
    "# å‡è¨­ä½ çŸ¥é“ç¸½å…±æ‡‰è©²æœ‰å¤šå°‘ç¯‡ï¼ˆä¾‹ï¼š584ï¼‰\n",
    "expected_total = 584\n",
    "\n",
    "# è®€å– CSV\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# å–å¾—å¯¦éš›å‡ºç¾éçš„ idï¼ˆè½‰æˆ intï¼‰\n",
    "existing_numbers = set(df[\"id\"].dropna().astype(int))\n",
    "\n",
    "# å»ºç«‹å®Œæ•´æ‡‰æœ‰çš„ ID é›†åˆ\n",
    "# âš ï¸ å‡è¨­ id å¾ 1 é–‹å§‹ç·¨è™Ÿ\n",
    "expected_numbers = set(range(1, expected_total + 1))\n",
    "\n",
    "# æ‰¾å‡ºç¼ºå°‘çš„ç·¨è™Ÿ\n",
    "missing = sorted(expected_numbers - existing_numbers)\n",
    "\n",
    "print(f\"âœ… å¯¦éš›æ–‡ç« æ•¸é‡: {len(existing_numbers)} / {expected_total}\")\n",
    "print(f\"âŒ ç¼ºå°‘çš„ Article ç·¨è™Ÿï¼ˆå…± {len(missing)} ç¯‡ï¼‰:\")\n",
    "print(\", \".join(f\"Article_{i}\" for i in missing))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4936bcaa-d529-457a-92ad-55356de3f848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 1/12...\n",
      "âœ… æ‰¹æ¬¡ 1/12 å·²å®Œæˆï¼Œå„²å­˜è‡³ step1_batches/step1_batch_1.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 2/12...\n",
      "âœ… æ‰¹æ¬¡ 2/12 å·²å®Œæˆï¼Œå„²å­˜è‡³ step1_batches/step1_batch_2.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 3/12...\n",
      "âœ… æ‰¹æ¬¡ 3/12 å·²å®Œæˆï¼Œå„²å­˜è‡³ step1_batches/step1_batch_3.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 4/12...\n",
      "âœ… æ‰¹æ¬¡ 4/12 å·²å®Œæˆï¼Œå„²å­˜è‡³ step1_batches/step1_batch_4.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 5/12...\n",
      "âœ… æ‰¹æ¬¡ 5/12 å·²å®Œæˆï¼Œå„²å­˜è‡³ step1_batches/step1_batch_5.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 6/12...\n",
      "âœ… æ‰¹æ¬¡ 6/12 å·²å®Œæˆï¼Œå„²å­˜è‡³ step1_batches/step1_batch_6.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 7/12...\n",
      "âœ… æ‰¹æ¬¡ 7/12 å·²å®Œæˆï¼Œå„²å­˜è‡³ step1_batches/step1_batch_7.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 8/12...\n",
      "âœ… æ‰¹æ¬¡ 8/12 å·²å®Œæˆï¼Œå„²å­˜è‡³ step1_batches/step1_batch_8.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 9/12...\n",
      "âœ… æ‰¹æ¬¡ 9/12 å·²å®Œæˆï¼Œå„²å­˜è‡³ step1_batches/step1_batch_9.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 10/12...\n",
      "âœ… æ‰¹æ¬¡ 10/12 å·²å®Œæˆï¼Œå„²å­˜è‡³ step1_batches/step1_batch_10.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 11/12...\n",
      "âœ… æ‰¹æ¬¡ 11/12 å·²å®Œæˆï¼Œå„²å­˜è‡³ step1_batches/step1_batch_11.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 12/12...\n",
      "âœ… æ‰¹æ¬¡ 12/12 å·²å®Œæˆï¼Œå„²å­˜è‡³ step1_batches/step1_batch_12.json\n",
      "\n",
      "ğŸ‰ æ‰€æœ‰æ‰¹æ¬¡è™•ç†å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Colab ç¨‹å¼ç¢¼å€å¡Š 1: åˆå§‹åŒ–å’Œæ­¥é©Ÿ1\n",
    "# ==========================================\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Step 1 çš„æç¤ºè©\n",
    "step1_prompt = \"\"\"You are a sociology professor with 30 years of experience analyzing Asian racism in the United States.\n",
    "Your task is to analyze the following article by identifying named entities and classifying them into the appropriate social roles and institutional categories. Focus especially on people or groups reacting to or affected by anti-Asian incidents.\n",
    "\n",
    "Step 1: Named Entity Recognition and Categorization\n",
    "\n",
    "1. Identify named entities.\n",
    "2. Classify them into appropriate social roles or institutional categories.\n",
    "3. Determine whether each **individual** is **Asian**, **Non-Asian**, or **Cannot be inferred** based on the text.\n",
    "4. For each entity, include a `\"reference\"` field that reflects **the exact phrase or wording** used in the article to refer to them.\n",
    "\n",
    "Please extract all named entities from the text and categorize them according to the following schema:\n",
    "\n",
    "---\n",
    "\n",
    "**INDIVIDUALS** (Specific persons or actors representing individual agency)\n",
    "\n",
    "1. politicians\n",
    "   - Elected officials acting in an individual capacity.\n",
    "   - Examples: senators, representatives, mayors, governors\n",
    "\n",
    "2. professionals\n",
    "   - Individuals recognized by their expertise or institutional role.\n",
    "   - Examples: professors, doctors, lawyers, foundation presidents\n",
    "\n",
    "3. celebrities\n",
    "   - Public figures in entertainment or sports (e.g., actors, athletes) unless clearly acting in a professional or political role.\n",
    "   - If overlapping with another role, assign to the more institutionally grounded category.\n",
    "\n",
    "4. perpetrators\n",
    "   - Individuals directly identified as committing or responsible for anti-Asian actions.\n",
    "   - Do not include vague or generalized public unless clearly specified.\n",
    "\n",
    "5. victims\n",
    "   - Individuals or racial/ethnic groups explicitly targeted by anti-Asian acts.\n",
    "   - Examples: â€œa woman attacked on the subway,â€ â€œJapanese Americans during WWIIâ€\n",
    "\n",
    "6. other_individuals\n",
    "   - All other named or unnamed individuals who do not fall into the above categories.\n",
    "   - Includes the general public, community members, business owners, or relatives (e.g., â€œmy mom,â€ â€œa neighborâ€).\n",
    "\n",
    "---\n",
    "\n",
    "**ORGANIZATIONS** (Named institutions or collectives)\n",
    "\n",
    "1. law_enforcement_agencies\n",
    "   - Official police or investigative institutions.\n",
    "   - Examples: Chicago Police Department, FBI, local sheriffâ€™s office\n",
    "\n",
    "2. government_bodies\n",
    "   - Government agencies, departments, or offices at any level (local/state/federal).\n",
    "   - Examples: CDC, Department of Justice, City Council\n",
    "\n",
    "3. ngo_or_advocacy_groups\n",
    "   - Civil rights organizations, foundations, or advocacy nonprofits.\n",
    "   - Examples: Stop AAPI Hate, Robert Wood Johnson Foundation\n",
    "\n",
    "4. business_entities\n",
    "   - Named companies, hotels, restaurants, or stores.\n",
    "   - Examples: Wrap-on Tools, Edgewater Beach Hotel\n",
    "\n",
    "5. community_groups\n",
    "   - Named cultural, ethnic, or neighborhood associations.\n",
    "   - Examples: Chinatown Association, Asian-American Coalition\n",
    "\n",
    "---\n",
    "\n",
    "**ETHNICITY INFERENCE RULES:**\n",
    "\n",
    "- For each **individual**, determine whether they are **Asian**, **Non-Asian**, or **Cannot be inferred**.\n",
    "- Use contextual clues such as ethnicity indicators, names, or explicit mentions.\n",
    "- If ethnicity is ambiguous or not stated, return `\"Cannot be inferred\"`.\n",
    "\n",
    "---\n",
    "\n",
    "**ADDITIONAL INSTRUCTIONS:**\n",
    "\n",
    "- Use `\"reference\"` to capture how the person/group was referred to in the original article (e.g., `\"an 80-year-old woman\"`, `\"Lee\"`, `\"the attacker\"`).\n",
    "- Normalize all name variants to a canonical form (e.g., â€œDr. Church,â€ â€œJ. Church,â€ and â€œChurchâ€ â†’ â€œJacqueline Churchâ€).\n",
    "- If an individual belongs to multiple roles, assign them to the most institutionally specific one (e.g., categorize a lawyer-celebrity as a professional).\n",
    "- Include only individuals explicitly involved in specific incidents under â€œvictimsâ€ and â€œperpetrators.â€\n",
    "- Do not classify individual police officers or sheriffs as individualsâ€”assign them under law_enforcement_agencies.\n",
    "- Classify individual owners under â€œbusiness_actorsâ€ and company names under â€œbusiness_entities.â€\n",
    "\n",
    "---\n",
    "\n",
    "**Output format (in JSON):**\n",
    "\n",
    "For all individuals, return an object with \"name\" and \"asian_status\" fields.\n",
    "For all organizations, return an object with \"name\" and \"asian_status\": \"Not applicable\".\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"individuals\": {\n",
    "    \"politicians\": [\n",
    "      {\n",
    "        \"name\": \"Tammy Duckworth\",\n",
    "        \"reference\": \"Senator Tammy Duckworth\",\n",
    "        \"asian_status\": \"Asian\"\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"Joe Biden\",\n",
    "        \"reference\": \"President Joe Biden\",\n",
    "        \"asian_status\": \"Non-Asian\"\n",
    "      }\n",
    "    ],\n",
    "    \"professionals\": [\n",
    "      {\n",
    "        \"name\": \"Julie Morita\",\n",
    "        \"reference\": \"Julie Morita\",\n",
    "        \"asian_status\": \"Asian\"\n",
    "      }\n",
    "    ],\n",
    "    \"celebrities\": [\n",
    "      {\n",
    "        \"name\": \"Awkwafina\",\n",
    "        \"reference\": \"Awkwafina\",\n",
    "        \"asian_status\": \"Asian\"\n",
    "      }\n",
    "    ],\n",
    "    \"perpetrators\": [\n",
    "      {\n",
    "        \"name\": \"Unknown Attacker\",\n",
    "        \"reference\": \"the attacker\",\n",
    "        \"asian_status\": \"Non-Asian\"\n",
    "      }\n",
    "    ],\n",
    "    \"victims\": [\n",
    "      {\n",
    "        \"name\": \"Asian Elderly Woman\",\n",
    "        \"reference\": \"an 80-year-old woman\",\n",
    "        \"asian_status\": \"Asian\"\n",
    "      }\n",
    "    ],\n",
    "    \"other_individuals\": [\n",
    "      {\n",
    "        \"name\": \"my mom\",\n",
    "        \"reference\": \"my mom\",\n",
    "        \"asian_status\": \"Asian\"\n",
    "      }\n",
    "    ]\n",
    "    \n",
    "  },\n",
    "  \"organizations\": {\n",
    "    \"law_enforcement_agencies\": [\n",
    "      {\n",
    "        \"name\": \"Chicago Police Department\",\n",
    "        \"reference\": \"Chicago Police Department\",\n",
    "        \"asian_status\": \"Not applicable\"\n",
    "      }\n",
    "    ],\n",
    "    \"government_bodies\": [\n",
    "      {\n",
    "        \"name\": \"City Council\",\n",
    "        \"reference\": \"City Council\",\n",
    "        \"asian_status\": \"Not applicable\"\n",
    "      }\n",
    "    ],\n",
    "    \"ngo_or_advocacy_groups\": [\n",
    "      {\n",
    "        \"name\": \"Stop AAPI Hate\",\n",
    "        \"reference\": \"Stop AAPI Hate\",\n",
    "        \"asian_status\": \"Not applicable\"\n",
    "      }\n",
    "    ],\n",
    "    \"business_entities\": [\n",
    "      {\n",
    "        \"name\": \"Edgewater Beach Hotel\",\n",
    "        \"reference\": \"Edgewater Beach Hotel\",\n",
    "        \"asian_status\": \"Not applicable\"\n",
    "      }\n",
    "    ],\n",
    "    \"community_groups\": [\n",
    "      {\n",
    "        \"name\": \"Chinatown Association\",\n",
    "        \"reference\": \"Chinatown Association\",\n",
    "        \"asian_status\": \"Not applicable\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# å‘¼å« Step 1\n",
    "\n",
    "# å»ºç«‹è¼¸å‡ºè³‡æ–™å¤¾\n",
    "output_dir = \"step1_batches\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# æ¯æ‰¹æ–‡ç« æ•¸é‡\n",
    "batch_size = 50\n",
    "article_items = list(articles.items())\n",
    "total_articles = len(article_items)\n",
    "total_batches = (total_articles + batch_size - 1) // batch_size  # ç„¡æ¢ä»¶é€²ä½\n",
    "\n",
    "# é€æ‰¹è™•ç†\n",
    "for batch_idx in range(0, total_articles, batch_size):\n",
    "    batch_number = batch_idx // batch_size + 1\n",
    "    filename = os.path.join(output_dir, f\"step1_batch_{batch_number}.json\")\n",
    "\n",
    "    # å¦‚æœæª”æ¡ˆå·²ç¶“å­˜åœ¨ï¼Œå°±è·³éé€™æ‰¹\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"â­ï¸ æ‰¹æ¬¡ {batch_number}/{total_batches} å·²å­˜åœ¨ï¼Œè·³é\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ {batch_number}/{total_batches}...\")\n",
    "\n",
    "    step1_result = {}\n",
    "    batch = article_items[batch_idx: batch_idx + batch_size]\n",
    "\n",
    "    for title, content in batch:\n",
    "        full_prompt = step1_prompt + \"\\n\\nArticle Text:\\n\" + content\n",
    "        response = get_response(full_prompt)\n",
    "        step1_result[title] = response\n",
    "\n",
    "    # å„²å­˜é€™ä¸€æ‰¹çµæœ\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(step1_result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"âœ… æ‰¹æ¬¡ {batch_number}/{total_batches} å·²å®Œæˆï¼Œå„²å­˜è‡³ {filename}\")\n",
    "\n",
    "print(\"\\nğŸ‰ æ‰€æœ‰æ‰¹æ¬¡è™•ç†å®Œæˆï¼\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97f46f96-40d0-4d23-a71d-fdf5e7931277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ é–‹å§‹è™•ç† step1_batches/step1_batch_1.json â†’ step2_batches/step2_batch_1.json\n",
      "âœ”ï¸ å·²è™•ç† Article_1\n",
      "âœ”ï¸ å·²è™•ç† Article_2\n",
      "âœ”ï¸ å·²è™•ç† Article_3\n",
      "âœ”ï¸ å·²è™•ç† Article_4\n",
      "âœ”ï¸ å·²è™•ç† Article_5\n",
      "âœ”ï¸ å·²è™•ç† Article_6\n",
      "âœ”ï¸ å·²è™•ç† Article_8\n",
      "âœ”ï¸ å·²è™•ç† Article_9\n",
      "âœ”ï¸ å·²è™•ç† Article_10\n",
      "âœ”ï¸ å·²è™•ç† Article_11\n",
      "âœ”ï¸ å·²è™•ç† Article_12\n",
      "âœ”ï¸ å·²è™•ç† Article_13\n",
      "âœ”ï¸ å·²è™•ç† Article_14\n",
      "âœ”ï¸ å·²è™•ç† Article_15\n",
      "âœ”ï¸ å·²è™•ç† Article_16\n",
      "âœ”ï¸ å·²è™•ç† Article_17\n",
      "âœ”ï¸ å·²è™•ç† Article_18\n",
      "âœ”ï¸ å·²è™•ç† Article_19\n",
      "âœ”ï¸ å·²è™•ç† Article_20\n",
      "âœ”ï¸ å·²è™•ç† Article_21\n",
      "âœ”ï¸ å·²è™•ç† Article_22\n",
      "âœ”ï¸ å·²è™•ç† Article_23\n",
      "âœ”ï¸ å·²è™•ç† Article_24\n",
      "âœ”ï¸ å·²è™•ç† Article_25\n",
      "âœ”ï¸ å·²è™•ç† Article_26\n",
      "âœ”ï¸ å·²è™•ç† Article_27\n",
      "âœ”ï¸ å·²è™•ç† Article_28\n",
      "âœ”ï¸ å·²è™•ç† Article_30\n",
      "âœ”ï¸ å·²è™•ç† Article_31\n",
      "âœ”ï¸ å·²è™•ç† Article_32\n",
      "âœ”ï¸ å·²è™•ç† Article_33\n",
      "âœ”ï¸ å·²è™•ç† Article_34\n",
      "âœ”ï¸ å·²è™•ç† Article_35\n",
      "âœ”ï¸ å·²è™•ç† Article_36\n",
      "âœ”ï¸ å·²è™•ç† Article_37\n",
      "âœ”ï¸ å·²è™•ç† Article_38\n",
      "âœ”ï¸ å·²è™•ç† Article_39\n",
      "âœ”ï¸ å·²è™•ç† Article_40\n",
      "âœ”ï¸ å·²è™•ç† Article_42\n",
      "âœ”ï¸ å·²è™•ç† Article_43\n",
      "âœ”ï¸ å·²è™•ç† Article_44\n",
      "âœ”ï¸ å·²è™•ç† Article_45\n",
      "âœ”ï¸ å·²è™•ç† Article_46\n",
      "âœ”ï¸ å·²è™•ç† Article_47\n",
      "âœ”ï¸ å·²è™•ç† Article_48\n",
      "âœ”ï¸ å·²è™•ç† Article_49\n",
      "âœ”ï¸ å·²è™•ç† Article_50\n",
      "âœ”ï¸ å·²è™•ç† Article_52\n",
      "âœ”ï¸ å·²è™•ç† Article_53\n",
      "âœ”ï¸ å·²è™•ç† Article_56\n",
      "âœ… step2_batch_1.json å·²å®Œæˆä¸¦å„²å­˜è‡³ step2_batches/step2_batch_1.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç† step1_batches/step1_batch_10.json â†’ step2_batches/step2_batch_10.json\n",
      "âœ”ï¸ å·²è™•ç† Article_492\n",
      "âœ”ï¸ å·²è™•ç† Article_493\n",
      "âœ”ï¸ å·²è™•ç† Article_494\n",
      "âœ”ï¸ å·²è™•ç† Article_495\n",
      "âœ”ï¸ å·²è™•ç† Article_496\n",
      "âœ”ï¸ å·²è™•ç† Article_499\n",
      "âœ”ï¸ å·²è™•ç† Article_500\n",
      "âœ”ï¸ å·²è™•ç† Article_501\n",
      "âœ”ï¸ å·²è™•ç† Article_503\n",
      "âœ”ï¸ å·²è™•ç† Article_504\n",
      "âœ”ï¸ å·²è™•ç† Article_505\n",
      "âœ”ï¸ å·²è™•ç† Article_506\n",
      "âœ”ï¸ å·²è™•ç† Article_507\n",
      "âœ”ï¸ å·²è™•ç† Article_508\n",
      "âœ”ï¸ å·²è™•ç† Article_510\n",
      "âœ”ï¸ å·²è™•ç† Article_511\n",
      "âœ”ï¸ å·²è™•ç† Article_512\n",
      "âœ”ï¸ å·²è™•ç† Article_513\n",
      "âœ”ï¸ å·²è™•ç† Article_514\n",
      "âœ”ï¸ å·²è™•ç† Article_515\n",
      "âœ”ï¸ å·²è™•ç† Article_516\n",
      "âœ”ï¸ å·²è™•ç† Article_517\n",
      "âœ”ï¸ å·²è™•ç† Article_518\n",
      "âœ”ï¸ å·²è™•ç† Article_519\n",
      "âœ”ï¸ å·²è™•ç† Article_520\n",
      "âœ”ï¸ å·²è™•ç† Article_521\n",
      "âœ”ï¸ å·²è™•ç† Article_522\n",
      "âœ”ï¸ å·²è™•ç† Article_523\n",
      "âœ”ï¸ å·²è™•ç† Article_524\n",
      "âœ”ï¸ å·²è™•ç† Article_525\n",
      "âœ”ï¸ å·²è™•ç† Article_526\n",
      "âœ”ï¸ å·²è™•ç† Article_527\n",
      "âœ”ï¸ å·²è™•ç† Article_528\n",
      "âœ”ï¸ å·²è™•ç† Article_529\n",
      "âœ”ï¸ å·²è™•ç† Article_530\n",
      "âœ”ï¸ å·²è™•ç† Article_531\n",
      "âœ”ï¸ å·²è™•ç† Article_532\n",
      "âœ”ï¸ å·²è™•ç† Article_533\n",
      "âœ”ï¸ å·²è™•ç† Article_534\n",
      "âœ”ï¸ å·²è™•ç† Article_535\n",
      "âœ”ï¸ å·²è™•ç† Article_536\n",
      "âœ”ï¸ å·²è™•ç† Article_537\n",
      "âœ”ï¸ å·²è™•ç† Article_538\n",
      "âœ”ï¸ å·²è™•ç† Article_539\n",
      "âœ”ï¸ å·²è™•ç† Article_540\n",
      "âœ”ï¸ å·²è™•ç† Article_541\n",
      "âœ”ï¸ å·²è™•ç† Article_542\n",
      "âœ”ï¸ å·²è™•ç† Article_543\n",
      "âœ”ï¸ å·²è™•ç† Article_544\n",
      "âœ”ï¸ å·²è™•ç† Article_545\n",
      "âœ… step2_batch_10.json å·²å®Œæˆä¸¦å„²å­˜è‡³ step2_batches/step2_batch_10.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç† step1_batches/step1_batch_11.json â†’ step2_batches/step2_batch_11.json\n",
      "âœ”ï¸ å·²è™•ç† Article_546\n",
      "âœ”ï¸ å·²è™•ç† Article_547\n",
      "âœ”ï¸ å·²è™•ç† Article_548\n",
      "âœ”ï¸ å·²è™•ç† Article_549\n",
      "âœ”ï¸ å·²è™•ç† Article_550\n",
      "âœ”ï¸ å·²è™•ç† Article_551\n",
      "âœ”ï¸ å·²è™•ç† Article_552\n",
      "âœ”ï¸ å·²è™•ç† Article_553\n",
      "âœ”ï¸ å·²è™•ç† Article_554\n",
      "âœ”ï¸ å·²è™•ç† Article_555\n",
      "âœ”ï¸ å·²è™•ç† Article_556\n",
      "âœ”ï¸ å·²è™•ç† Article_557\n",
      "âœ”ï¸ å·²è™•ç† Article_558\n",
      "âœ”ï¸ å·²è™•ç† Article_559\n",
      "âœ”ï¸ å·²è™•ç† Article_560\n",
      "âœ”ï¸ å·²è™•ç† Article_561\n",
      "âœ”ï¸ å·²è™•ç† Article_562\n",
      "âœ”ï¸ å·²è™•ç† Article_563\n",
      "âœ”ï¸ å·²è™•ç† Article_564\n",
      "âœ”ï¸ å·²è™•ç† Article_565\n",
      "âœ”ï¸ å·²è™•ç† Article_566\n",
      "âœ”ï¸ å·²è™•ç† Article_567\n",
      "âœ”ï¸ å·²è™•ç† Article_568\n",
      "âœ”ï¸ å·²è™•ç† Article_569\n",
      "âœ”ï¸ å·²è™•ç† Article_570\n",
      "âœ”ï¸ å·²è™•ç† Article_571\n",
      "âœ”ï¸ å·²è™•ç† Article_572\n",
      "âœ”ï¸ å·²è™•ç† Article_573\n",
      "âœ”ï¸ å·²è™•ç† Article_574\n",
      "âœ”ï¸ å·²è™•ç† Article_575\n",
      "âœ”ï¸ å·²è™•ç† Article_576\n",
      "âœ”ï¸ å·²è™•ç† Article_577\n",
      "âœ”ï¸ å·²è™•ç† Article_578\n",
      "âœ”ï¸ å·²è™•ç† Article_579\n",
      "âœ”ï¸ å·²è™•ç† Article_580\n",
      "âœ”ï¸ å·²è™•ç† Article_581\n",
      "âœ”ï¸ å·²è™•ç† Article_582\n",
      "âœ”ï¸ å·²è™•ç† Article_583\n",
      "âœ”ï¸ å·²è™•ç† Article_584\n",
      "âœ”ï¸ å·²è™•ç† Article_585\n",
      "âœ”ï¸ å·²è™•ç† Article_586\n",
      "âœ”ï¸ å·²è™•ç† Article_587\n",
      "âœ”ï¸ å·²è™•ç† Article_588\n",
      "âœ”ï¸ å·²è™•ç† Article_589\n",
      "âœ”ï¸ å·²è™•ç† Article_591\n",
      "âœ”ï¸ å·²è™•ç† Article_592\n",
      "âœ”ï¸ å·²è™•ç† Article_593\n",
      "âœ”ï¸ å·²è™•ç† Article_594\n",
      "âœ”ï¸ å·²è™•ç† Article_596\n",
      "âœ”ï¸ å·²è™•ç† Article_597\n",
      "âœ… step2_batch_11.json å·²å®Œæˆä¸¦å„²å­˜è‡³ step2_batches/step2_batch_11.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç† step1_batches/step1_batch_12.json â†’ step2_batches/step2_batch_12.json\n",
      "âœ”ï¸ å·²è™•ç† Article_598\n",
      "âœ”ï¸ å·²è™•ç† Article_599\n",
      "âœ”ï¸ å·²è™•ç† Article_600\n",
      "âœ”ï¸ å·²è™•ç† Article_601\n",
      "âœ”ï¸ å·²è™•ç† Article_602\n",
      "âœ”ï¸ å·²è™•ç† Article_603\n",
      "âœ”ï¸ å·²è™•ç† Article_604\n",
      "âœ”ï¸ å·²è™•ç† Article_605\n",
      "âœ”ï¸ å·²è™•ç† Article_606\n",
      "âœ”ï¸ å·²è™•ç† Article_607\n",
      "âœ”ï¸ å·²è™•ç† Article_609\n",
      "âœ”ï¸ å·²è™•ç† Article_610\n",
      "âœ”ï¸ å·²è™•ç† Article_611\n",
      "âœ”ï¸ å·²è™•ç† Article_613\n",
      "âœ”ï¸ å·²è™•ç† Article_614\n",
      "âœ”ï¸ å·²è™•ç† Article_616\n",
      "âœ”ï¸ å·²è™•ç† Article_617\n",
      "âœ”ï¸ å·²è™•ç† Article_618\n",
      "âœ”ï¸ å·²è™•ç† Article_619\n",
      "âœ”ï¸ å·²è™•ç† Article_620\n",
      "âœ”ï¸ å·²è™•ç† Article_621\n",
      "âœ”ï¸ å·²è™•ç† Article_622\n",
      "âœ”ï¸ å·²è™•ç† Article_623\n",
      "âœ”ï¸ å·²è™•ç† Article_624\n",
      "âœ”ï¸ å·²è™•ç† Article_625\n",
      "âœ”ï¸ å·²è™•ç† Article_626\n",
      "âœ”ï¸ å·²è™•ç† Article_627\n",
      "âœ”ï¸ å·²è™•ç† Article_628\n",
      "âœ”ï¸ å·²è™•ç† Article_629\n",
      "âœ”ï¸ å·²è™•ç† Article_630\n",
      "âœ”ï¸ å·²è™•ç† Article_631\n",
      "âœ”ï¸ å·²è™•ç† Article_632\n",
      "âœ”ï¸ å·²è™•ç† Article_633\n",
      "âœ”ï¸ å·²è™•ç† Article_634\n",
      "âœ… step2_batch_12.json å·²å®Œæˆä¸¦å„²å­˜è‡³ step2_batches/step2_batch_12.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç† step1_batches/step1_batch_2.json â†’ step2_batches/step2_batch_2.json\n",
      "âœ”ï¸ å·²è™•ç† Article_57\n",
      "âœ”ï¸ å·²è™•ç† Article_58\n",
      "âœ”ï¸ å·²è™•ç† Article_59\n",
      "âœ”ï¸ å·²è™•ç† Article_60\n",
      "âœ”ï¸ å·²è™•ç† Article_61\n",
      "âœ”ï¸ å·²è™•ç† Article_62\n",
      "âœ”ï¸ å·²è™•ç† Article_65\n",
      "âœ”ï¸ å·²è™•ç† Article_66\n",
      "âœ”ï¸ å·²è™•ç† Article_67\n",
      "âœ”ï¸ å·²è™•ç† Article_68\n",
      "âœ”ï¸ å·²è™•ç† Article_69\n",
      "âœ”ï¸ å·²è™•ç† Article_70\n",
      "âœ”ï¸ å·²è™•ç† Article_71\n",
      "âœ”ï¸ å·²è™•ç† Article_72\n",
      "âœ”ï¸ å·²è™•ç† Article_73\n",
      "âœ”ï¸ å·²è™•ç† Article_74\n",
      "âœ”ï¸ å·²è™•ç† Article_75\n",
      "âœ”ï¸ å·²è™•ç† Article_76\n",
      "âœ”ï¸ å·²è™•ç† Article_79\n",
      "âœ”ï¸ å·²è™•ç† Article_80\n",
      "âœ”ï¸ å·²è™•ç† Article_81\n",
      "âœ”ï¸ å·²è™•ç† Article_82\n",
      "âœ”ï¸ å·²è™•ç† Article_83\n",
      "âœ”ï¸ å·²è™•ç† Article_84\n",
      "âœ”ï¸ å·²è™•ç† Article_85\n",
      "âœ”ï¸ å·²è™•ç† Article_86\n",
      "âœ”ï¸ å·²è™•ç† Article_87\n",
      "âœ”ï¸ å·²è™•ç† Article_88\n",
      "âœ”ï¸ å·²è™•ç† Article_89\n",
      "âœ”ï¸ å·²è™•ç† Article_90\n",
      "âœ”ï¸ å·²è™•ç† Article_91\n",
      "âœ”ï¸ å·²è™•ç† Article_92\n",
      "âœ”ï¸ å·²è™•ç† Article_93\n",
      "âœ”ï¸ å·²è™•ç† Article_94\n",
      "âœ”ï¸ å·²è™•ç† Article_96\n",
      "âœ”ï¸ å·²è™•ç† Article_97\n",
      "âœ”ï¸ å·²è™•ç† Article_98\n",
      "âœ”ï¸ å·²è™•ç† Article_99\n",
      "âœ”ï¸ å·²è™•ç† Article_100\n",
      "âœ”ï¸ å·²è™•ç† Article_101\n",
      "âœ”ï¸ å·²è™•ç† Article_102\n",
      "âœ”ï¸ å·²è™•ç† Article_103\n",
      "âœ”ï¸ å·²è™•ç† Article_104\n",
      "âœ”ï¸ å·²è™•ç† Article_105\n",
      "âœ”ï¸ å·²è™•ç† Article_107\n",
      "âœ”ï¸ å·²è™•ç† Article_108\n",
      "âœ”ï¸ å·²è™•ç† Article_109\n",
      "âœ”ï¸ å·²è™•ç† Article_110\n",
      "âœ”ï¸ å·²è™•ç† Article_112\n",
      "âœ”ï¸ å·²è™•ç† Article_113\n",
      "âœ… step2_batch_2.json å·²å®Œæˆä¸¦å„²å­˜è‡³ step2_batches/step2_batch_2.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç† step1_batches/step1_batch_3.json â†’ step2_batches/step2_batch_3.json\n",
      "âœ”ï¸ å·²è™•ç† Article_114\n",
      "âœ”ï¸ å·²è™•ç† Article_115\n",
      "âœ”ï¸ å·²è™•ç† Article_116\n",
      "âœ”ï¸ å·²è™•ç† Article_117\n",
      "âœ”ï¸ å·²è™•ç† Article_118\n",
      "âœ”ï¸ å·²è™•ç† Article_119\n",
      "âœ”ï¸ å·²è™•ç† Article_120\n",
      "âœ”ï¸ å·²è™•ç† Article_121\n",
      "âœ”ï¸ å·²è™•ç† Article_122\n",
      "âœ”ï¸ å·²è™•ç† Article_123\n",
      "âœ”ï¸ å·²è™•ç† Article_124\n",
      "âœ”ï¸ å·²è™•ç† Article_125\n",
      "âœ”ï¸ å·²è™•ç† Article_126\n",
      "âœ”ï¸ å·²è™•ç† Article_127\n",
      "âœ”ï¸ å·²è™•ç† Article_128\n",
      "âœ”ï¸ å·²è™•ç† Article_129\n",
      "âœ”ï¸ å·²è™•ç† Article_130\n",
      "âœ”ï¸ å·²è™•ç† Article_131\n",
      "âœ”ï¸ å·²è™•ç† Article_132\n",
      "âœ”ï¸ å·²è™•ç† Article_133\n",
      "âœ”ï¸ å·²è™•ç† Article_134\n",
      "âœ”ï¸ å·²è™•ç† Article_136\n",
      "âœ”ï¸ å·²è™•ç† Article_137\n",
      "âœ”ï¸ å·²è™•ç† Article_139\n",
      "âœ”ï¸ å·²è™•ç† Article_140\n",
      "âœ”ï¸ å·²è™•ç† Article_141\n",
      "âœ”ï¸ å·²è™•ç† Article_142\n",
      "âœ”ï¸ å·²è™•ç† Article_143\n",
      "âœ”ï¸ å·²è™•ç† Article_144\n",
      "âœ”ï¸ å·²è™•ç† Article_145\n",
      "âœ”ï¸ å·²è™•ç† Article_147\n",
      "âœ”ï¸ å·²è™•ç† Article_148\n",
      "âœ”ï¸ å·²è™•ç† Article_149\n",
      "âœ”ï¸ å·²è™•ç† Article_150\n",
      "âœ”ï¸ å·²è™•ç† Article_151\n",
      "âœ”ï¸ å·²è™•ç† Article_152\n",
      "âœ”ï¸ å·²è™•ç† Article_153\n",
      "âœ”ï¸ å·²è™•ç† Article_154\n",
      "âœ”ï¸ å·²è™•ç† Article_155\n",
      "âœ”ï¸ å·²è™•ç† Article_156\n",
      "âœ”ï¸ å·²è™•ç† Article_157\n",
      "âœ”ï¸ å·²è™•ç† Article_158\n",
      "âœ”ï¸ å·²è™•ç† Article_159\n",
      "âœ”ï¸ å·²è™•ç† Article_160\n",
      "âœ”ï¸ å·²è™•ç† Article_161\n",
      "âœ”ï¸ å·²è™•ç† Article_162\n",
      "âœ”ï¸ å·²è™•ç† Article_163\n",
      "âœ”ï¸ å·²è™•ç† Article_164\n",
      "âœ”ï¸ å·²è™•ç† Article_165\n",
      "âœ”ï¸ å·²è™•ç† Article_166\n",
      "âœ… step2_batch_3.json å·²å®Œæˆä¸¦å„²å­˜è‡³ step2_batches/step2_batch_3.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç† step1_batches/step1_batch_4.json â†’ step2_batches/step2_batch_4.json\n",
      "âœ”ï¸ å·²è™•ç† Article_167\n",
      "âœ”ï¸ å·²è™•ç† Article_168\n",
      "âœ”ï¸ å·²è™•ç† Article_169\n",
      "âœ”ï¸ å·²è™•ç† Article_170\n",
      "âœ”ï¸ å·²è™•ç† Article_171\n",
      "âœ”ï¸ å·²è™•ç† Article_172\n",
      "âœ”ï¸ å·²è™•ç† Article_173\n",
      "âœ”ï¸ å·²è™•ç† Article_174\n",
      "âœ”ï¸ å·²è™•ç† Article_175\n",
      "âœ”ï¸ å·²è™•ç† Article_176\n",
      "âœ”ï¸ å·²è™•ç† Article_177\n",
      "âœ”ï¸ å·²è™•ç† Article_178\n",
      "âœ”ï¸ å·²è™•ç† Article_179\n",
      "âœ”ï¸ å·²è™•ç† Article_180\n",
      "âœ”ï¸ å·²è™•ç† Article_181\n",
      "âœ”ï¸ å·²è™•ç† Article_182\n",
      "âœ”ï¸ å·²è™•ç† Article_183\n",
      "âœ”ï¸ å·²è™•ç† Article_184\n",
      "âœ”ï¸ å·²è™•ç† Article_185\n",
      "âœ”ï¸ å·²è™•ç† Article_186\n",
      "âœ”ï¸ å·²è™•ç† Article_187\n",
      "âœ”ï¸ å·²è™•ç† Article_188\n",
      "âœ”ï¸ å·²è™•ç† Article_189\n",
      "âœ”ï¸ å·²è™•ç† Article_190\n",
      "âœ”ï¸ å·²è™•ç† Article_191\n",
      "âœ”ï¸ å·²è™•ç† Article_192\n",
      "âœ”ï¸ å·²è™•ç† Article_193\n",
      "âœ”ï¸ å·²è™•ç† Article_194\n",
      "âœ”ï¸ å·²è™•ç† Article_195\n",
      "âœ”ï¸ å·²è™•ç† Article_196\n",
      "âœ”ï¸ å·²è™•ç† Article_197\n",
      "âœ”ï¸ å·²è™•ç† Article_198\n",
      "âœ”ï¸ å·²è™•ç† Article_199\n",
      "âœ”ï¸ å·²è™•ç† Article_200\n",
      "âœ”ï¸ å·²è™•ç† Article_201\n",
      "âœ”ï¸ å·²è™•ç† Article_202\n",
      "âœ”ï¸ å·²è™•ç† Article_203\n",
      "âœ”ï¸ å·²è™•ç† Article_204\n",
      "âœ”ï¸ å·²è™•ç† Article_205\n",
      "âœ”ï¸ å·²è™•ç† Article_206\n",
      "âœ”ï¸ å·²è™•ç† Article_207\n",
      "âœ”ï¸ å·²è™•ç† Article_208\n",
      "âœ”ï¸ å·²è™•ç† Article_209\n",
      "âœ”ï¸ å·²è™•ç† Article_210\n",
      "âœ”ï¸ å·²è™•ç† Article_211\n",
      "âœ”ï¸ å·²è™•ç† Article_212\n",
      "âœ”ï¸ å·²è™•ç† Article_213\n",
      "âœ”ï¸ å·²è™•ç† Article_214\n",
      "âœ”ï¸ å·²è™•ç† Article_216\n",
      "âœ”ï¸ å·²è™•ç† Article_217\n",
      "âœ… step2_batch_4.json å·²å®Œæˆä¸¦å„²å­˜è‡³ step2_batches/step2_batch_4.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç† step1_batches/step1_batch_5.json â†’ step2_batches/step2_batch_5.json\n",
      "âœ”ï¸ å·²è™•ç† Article_218\n",
      "âœ”ï¸ å·²è™•ç† Article_219\n",
      "âœ”ï¸ å·²è™•ç† Article_220\n",
      "âœ”ï¸ å·²è™•ç† Article_221\n",
      "âœ”ï¸ å·²è™•ç† Article_222\n",
      "âœ”ï¸ å·²è™•ç† Article_223\n",
      "âœ”ï¸ å·²è™•ç† Article_224\n",
      "âœ”ï¸ å·²è™•ç† Article_225\n",
      "âœ”ï¸ å·²è™•ç† Article_226\n",
      "âœ”ï¸ å·²è™•ç† Article_227\n",
      "âœ”ï¸ å·²è™•ç† Article_228\n",
      "âœ”ï¸ å·²è™•ç† Article_230\n",
      "âœ”ï¸ å·²è™•ç† Article_231\n",
      "âœ”ï¸ å·²è™•ç† Article_232\n",
      "âœ”ï¸ å·²è™•ç† Article_233\n",
      "âœ”ï¸ å·²è™•ç† Article_235\n",
      "âœ”ï¸ å·²è™•ç† Article_236\n",
      "âœ”ï¸ å·²è™•ç† Article_237\n",
      "âœ”ï¸ å·²è™•ç† Article_238\n",
      "âœ”ï¸ å·²è™•ç† Article_239\n",
      "âœ”ï¸ å·²è™•ç† Article_240\n",
      "âœ”ï¸ å·²è™•ç† Article_241\n",
      "âœ”ï¸ å·²è™•ç† Article_242\n",
      "âœ”ï¸ å·²è™•ç† Article_243\n",
      "âœ”ï¸ å·²è™•ç† Article_244\n",
      "âœ”ï¸ å·²è™•ç† Article_245\n",
      "âœ”ï¸ å·²è™•ç† Article_246\n",
      "âœ”ï¸ å·²è™•ç† Article_247\n",
      "âœ”ï¸ å·²è™•ç† Article_248\n",
      "âœ”ï¸ å·²è™•ç† Article_249\n",
      "âœ”ï¸ å·²è™•ç† Article_250\n",
      "âœ”ï¸ å·²è™•ç† Article_251\n",
      "âœ”ï¸ å·²è™•ç† Article_252\n",
      "âœ”ï¸ å·²è™•ç† Article_253\n",
      "âœ”ï¸ å·²è™•ç† Article_255\n",
      "âœ”ï¸ å·²è™•ç† Article_256\n",
      "âœ”ï¸ å·²è™•ç† Article_257\n",
      "âœ”ï¸ å·²è™•ç† Article_258\n",
      "âœ”ï¸ å·²è™•ç† Article_259\n",
      "âœ”ï¸ å·²è™•ç† Article_260\n",
      "âœ”ï¸ å·²è™•ç† Article_262\n",
      "âœ”ï¸ å·²è™•ç† Article_263\n",
      "âœ”ï¸ å·²è™•ç† Article_264\n",
      "âœ”ï¸ å·²è™•ç† Article_265\n",
      "âœ”ï¸ å·²è™•ç† Article_266\n",
      "âœ”ï¸ å·²è™•ç† Article_267\n",
      "âœ”ï¸ å·²è™•ç† Article_268\n",
      "âœ”ï¸ å·²è™•ç† Article_270\n",
      "âœ”ï¸ å·²è™•ç† Article_272\n",
      "âœ”ï¸ å·²è™•ç† Article_273\n",
      "âœ… step2_batch_5.json å·²å®Œæˆä¸¦å„²å­˜è‡³ step2_batches/step2_batch_5.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç† step1_batches/step1_batch_6.json â†’ step2_batches/step2_batch_6.json\n",
      "âœ”ï¸ å·²è™•ç† Article_274\n",
      "âœ”ï¸ å·²è™•ç† Article_275\n",
      "âœ”ï¸ å·²è™•ç† Article_276\n",
      "âœ”ï¸ å·²è™•ç† Article_277\n",
      "âœ”ï¸ å·²è™•ç† Article_278\n",
      "âœ”ï¸ å·²è™•ç† Article_279\n",
      "âœ”ï¸ å·²è™•ç† Article_280\n",
      "âœ”ï¸ å·²è™•ç† Article_281\n",
      "âœ”ï¸ å·²è™•ç† Article_282\n",
      "âœ”ï¸ å·²è™•ç† Article_283\n",
      "âœ”ï¸ å·²è™•ç† Article_284\n",
      "âœ”ï¸ å·²è™•ç† Article_285\n",
      "âœ”ï¸ å·²è™•ç† Article_286\n",
      "âœ”ï¸ å·²è™•ç† Article_288\n",
      "âœ”ï¸ å·²è™•ç† Article_290\n",
      "âœ”ï¸ å·²è™•ç† Article_291\n",
      "âœ”ï¸ å·²è™•ç† Article_292\n",
      "âœ”ï¸ å·²è™•ç† Article_293\n",
      "âœ”ï¸ å·²è™•ç† Article_294\n",
      "âœ”ï¸ å·²è™•ç† Article_296\n",
      "âœ”ï¸ å·²è™•ç† Article_297\n",
      "âœ”ï¸ å·²è™•ç† Article_298\n",
      "âœ”ï¸ å·²è™•ç† Article_299\n",
      "âœ”ï¸ å·²è™•ç† Article_300\n",
      "âœ”ï¸ å·²è™•ç† Article_301\n",
      "âœ”ï¸ å·²è™•ç† Article_302\n",
      "âœ”ï¸ å·²è™•ç† Article_303\n",
      "âœ”ï¸ å·²è™•ç† Article_304\n",
      "âœ”ï¸ å·²è™•ç† Article_305\n",
      "âœ”ï¸ å·²è™•ç† Article_306\n",
      "âœ”ï¸ å·²è™•ç† Article_307\n",
      "âœ”ï¸ å·²è™•ç† Article_308\n",
      "âœ”ï¸ å·²è™•ç† Article_309\n",
      "âœ”ï¸ å·²è™•ç† Article_310\n",
      "âœ”ï¸ å·²è™•ç† Article_311\n",
      "âœ”ï¸ å·²è™•ç† Article_312\n",
      "âœ”ï¸ å·²è™•ç† Article_313\n",
      "âœ”ï¸ å·²è™•ç† Article_314\n",
      "âœ”ï¸ å·²è™•ç† Article_315\n",
      "âœ”ï¸ å·²è™•ç† Article_316\n",
      "âœ”ï¸ å·²è™•ç† Article_317\n",
      "âœ”ï¸ å·²è™•ç† Article_318\n",
      "âœ”ï¸ å·²è™•ç† Article_319\n",
      "âœ”ï¸ å·²è™•ç† Article_320\n",
      "âœ”ï¸ å·²è™•ç† Article_321\n",
      "âœ”ï¸ å·²è™•ç† Article_322\n",
      "âœ”ï¸ å·²è™•ç† Article_323\n",
      "âœ”ï¸ å·²è™•ç† Article_324\n",
      "âœ”ï¸ å·²è™•ç† Article_325\n",
      "âœ”ï¸ å·²è™•ç† Article_326\n",
      "âœ… step2_batch_6.json å·²å®Œæˆä¸¦å„²å­˜è‡³ step2_batches/step2_batch_6.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç† step1_batches/step1_batch_7.json â†’ step2_batches/step2_batch_7.json\n",
      "âœ”ï¸ å·²è™•ç† Article_327\n",
      "âœ”ï¸ å·²è™•ç† Article_328\n",
      "âœ”ï¸ å·²è™•ç† Article_329\n",
      "âœ”ï¸ å·²è™•ç† Article_330\n",
      "âœ”ï¸ å·²è™•ç† Article_331\n",
      "âœ”ï¸ å·²è™•ç† Article_332\n",
      "âœ”ï¸ å·²è™•ç† Article_333\n",
      "âœ”ï¸ å·²è™•ç† Article_334\n",
      "âœ”ï¸ å·²è™•ç† Article_335\n",
      "âœ”ï¸ å·²è™•ç† Article_336\n",
      "âœ”ï¸ å·²è™•ç† Article_337\n",
      "âœ”ï¸ å·²è™•ç† Article_338\n",
      "âœ”ï¸ å·²è™•ç† Article_339\n",
      "âœ”ï¸ å·²è™•ç† Article_340\n",
      "âœ”ï¸ å·²è™•ç† Article_341\n",
      "âœ”ï¸ å·²è™•ç† Article_342\n",
      "âœ”ï¸ å·²è™•ç† Article_343\n",
      "âœ”ï¸ å·²è™•ç† Article_344\n",
      "âœ”ï¸ å·²è™•ç† Article_345\n",
      "âœ”ï¸ å·²è™•ç† Article_346\n",
      "âœ”ï¸ å·²è™•ç† Article_347\n",
      "âœ”ï¸ å·²è™•ç† Article_348\n",
      "âœ”ï¸ å·²è™•ç† Article_349\n",
      "âœ”ï¸ å·²è™•ç† Article_350\n",
      "âœ”ï¸ å·²è™•ç† Article_351\n",
      "âœ”ï¸ å·²è™•ç† Article_352\n",
      "âœ”ï¸ å·²è™•ç† Article_355\n",
      "âœ”ï¸ å·²è™•ç† Article_356\n",
      "âœ”ï¸ å·²è™•ç† Article_357\n",
      "âœ”ï¸ å·²è™•ç† Article_358\n",
      "âœ”ï¸ å·²è™•ç† Article_359\n",
      "âœ”ï¸ å·²è™•ç† Article_360\n",
      "âœ”ï¸ å·²è™•ç† Article_361\n",
      "âœ”ï¸ å·²è™•ç† Article_363\n",
      "âœ”ï¸ å·²è™•ç† Article_364\n",
      "âœ”ï¸ å·²è™•ç† Article_365\n",
      "âœ”ï¸ å·²è™•ç† Article_366\n",
      "âœ”ï¸ å·²è™•ç† Article_367\n",
      "âœ”ï¸ å·²è™•ç† Article_368\n",
      "âœ”ï¸ å·²è™•ç† Article_369\n",
      "âœ”ï¸ å·²è™•ç† Article_370\n",
      "âœ”ï¸ å·²è™•ç† Article_371\n",
      "âœ”ï¸ å·²è™•ç† Article_373\n",
      "âœ”ï¸ å·²è™•ç† Article_374\n",
      "âœ”ï¸ å·²è™•ç† Article_375\n",
      "âœ”ï¸ å·²è™•ç† Article_376\n",
      "âœ”ï¸ å·²è™•ç† Article_377\n",
      "âœ”ï¸ å·²è™•ç† Article_379\n",
      "âœ”ï¸ å·²è™•ç† Article_380\n",
      "âœ”ï¸ å·²è™•ç† Article_381\n",
      "âœ… step2_batch_7.json å·²å®Œæˆä¸¦å„²å­˜è‡³ step2_batches/step2_batch_7.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç† step1_batches/step1_batch_8.json â†’ step2_batches/step2_batch_8.json\n",
      "âœ”ï¸ å·²è™•ç† Article_382\n",
      "âœ”ï¸ å·²è™•ç† Article_383\n",
      "âœ”ï¸ å·²è™•ç† Article_384\n",
      "âœ”ï¸ å·²è™•ç† Article_385\n",
      "âœ”ï¸ å·²è™•ç† Article_386\n",
      "âœ”ï¸ å·²è™•ç† Article_387\n",
      "âœ”ï¸ å·²è™•ç† Article_388\n",
      "âœ”ï¸ å·²è™•ç† Article_389\n",
      "âœ”ï¸ å·²è™•ç† Article_390\n",
      "âœ”ï¸ å·²è™•ç† Article_391\n",
      "âœ”ï¸ å·²è™•ç† Article_392\n",
      "âœ”ï¸ å·²è™•ç† Article_393\n",
      "âœ”ï¸ å·²è™•ç† Article_394\n",
      "âœ”ï¸ å·²è™•ç† Article_395\n",
      "âœ”ï¸ å·²è™•ç† Article_396\n",
      "âœ”ï¸ å·²è™•ç† Article_397\n",
      "âœ”ï¸ å·²è™•ç† Article_398\n",
      "âœ”ï¸ å·²è™•ç† Article_399\n",
      "âœ”ï¸ å·²è™•ç† Article_400\n",
      "âœ”ï¸ å·²è™•ç† Article_401\n",
      "âœ”ï¸ å·²è™•ç† Article_402\n",
      "âœ”ï¸ å·²è™•ç† Article_403\n",
      "âœ”ï¸ å·²è™•ç† Article_404\n",
      "âœ”ï¸ å·²è™•ç† Article_405\n",
      "âœ”ï¸ å·²è™•ç† Article_406\n",
      "âœ”ï¸ å·²è™•ç† Article_409\n",
      "âœ”ï¸ å·²è™•ç† Article_410\n",
      "âœ”ï¸ å·²è™•ç† Article_411\n",
      "âœ”ï¸ å·²è™•ç† Article_412\n",
      "âœ”ï¸ å·²è™•ç† Article_413\n",
      "âœ”ï¸ å·²è™•ç† Article_414\n",
      "âœ”ï¸ å·²è™•ç† Article_415\n",
      "âœ”ï¸ å·²è™•ç† Article_416\n",
      "âœ”ï¸ å·²è™•ç† Article_417\n",
      "âœ”ï¸ å·²è™•ç† Article_418\n",
      "âœ”ï¸ å·²è™•ç† Article_419\n",
      "âœ”ï¸ å·²è™•ç† Article_420\n",
      "âœ”ï¸ å·²è™•ç† Article_421\n",
      "âœ”ï¸ å·²è™•ç† Article_422\n",
      "âœ”ï¸ å·²è™•ç† Article_423\n",
      "âœ”ï¸ å·²è™•ç† Article_424\n",
      "âœ”ï¸ å·²è™•ç† Article_425\n",
      "âœ”ï¸ å·²è™•ç† Article_426\n",
      "âœ”ï¸ å·²è™•ç† Article_427\n",
      "âœ”ï¸ å·²è™•ç† Article_428\n",
      "âœ”ï¸ å·²è™•ç† Article_429\n",
      "âœ”ï¸ å·²è™•ç† Article_430\n",
      "âœ”ï¸ å·²è™•ç† Article_431\n",
      "âœ”ï¸ å·²è™•ç† Article_433\n",
      "âœ”ï¸ å·²è™•ç† Article_434\n",
      "âœ… step2_batch_8.json å·²å®Œæˆä¸¦å„²å­˜è‡³ step2_batches/step2_batch_8.json\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç† step1_batches/step1_batch_9.json â†’ step2_batches/step2_batch_9.json\n",
      "âœ”ï¸ å·²è™•ç† Article_435\n",
      "âœ”ï¸ å·²è™•ç† Article_436\n",
      "âœ”ï¸ å·²è™•ç† Article_437\n",
      "âœ”ï¸ å·²è™•ç† Article_438\n",
      "âœ”ï¸ å·²è™•ç† Article_439\n",
      "âœ”ï¸ å·²è™•ç† Article_440\n",
      "âœ”ï¸ å·²è™•ç† Article_441\n",
      "âœ”ï¸ å·²è™•ç† Article_442\n",
      "âœ”ï¸ å·²è™•ç† Article_443\n",
      "âœ”ï¸ å·²è™•ç† Article_445\n",
      "âœ”ï¸ å·²è™•ç† Article_446\n",
      "âœ”ï¸ å·²è™•ç† Article_448\n",
      "âœ”ï¸ å·²è™•ç† Article_449\n",
      "âœ”ï¸ å·²è™•ç† Article_450\n",
      "âœ”ï¸ å·²è™•ç† Article_451\n",
      "âœ”ï¸ å·²è™•ç† Article_452\n",
      "âœ”ï¸ å·²è™•ç† Article_453\n",
      "âœ”ï¸ å·²è™•ç† Article_454\n",
      "âœ”ï¸ å·²è™•ç† Article_455\n",
      "âœ”ï¸ å·²è™•ç† Article_456\n",
      "âœ”ï¸ å·²è™•ç† Article_457\n",
      "âœ”ï¸ å·²è™•ç† Article_458\n",
      "âœ”ï¸ å·²è™•ç† Article_459\n",
      "âœ”ï¸ å·²è™•ç† Article_460\n",
      "âœ”ï¸ å·²è™•ç† Article_461\n",
      "âœ”ï¸ å·²è™•ç† Article_463\n",
      "âœ”ï¸ å·²è™•ç† Article_465\n",
      "âœ”ï¸ å·²è™•ç† Article_466\n",
      "âœ”ï¸ å·²è™•ç† Article_467\n",
      "âœ”ï¸ å·²è™•ç† Article_468\n",
      "âœ”ï¸ å·²è™•ç† Article_469\n",
      "âœ”ï¸ å·²è™•ç† Article_470\n",
      "âœ”ï¸ å·²è™•ç† Article_471\n",
      "âœ”ï¸ å·²è™•ç† Article_473\n",
      "âœ”ï¸ å·²è™•ç† Article_474\n",
      "âœ”ï¸ å·²è™•ç† Article_475\n",
      "âœ”ï¸ å·²è™•ç† Article_476\n",
      "âœ”ï¸ å·²è™•ç† Article_477\n",
      "âœ”ï¸ å·²è™•ç† Article_478\n",
      "âœ”ï¸ å·²è™•ç† Article_479\n",
      "âœ”ï¸ å·²è™•ç† Article_480\n",
      "âœ”ï¸ å·²è™•ç† Article_481\n",
      "âœ”ï¸ å·²è™•ç† Article_483\n",
      "âœ”ï¸ å·²è™•ç† Article_484\n",
      "âœ”ï¸ å·²è™•ç† Article_485\n",
      "âœ”ï¸ å·²è™•ç† Article_486\n",
      "âœ”ï¸ å·²è™•ç† Article_487\n",
      "âœ”ï¸ å·²è™•ç† Article_488\n",
      "âœ”ï¸ å·²è™•ç† Article_489\n",
      "âœ”ï¸ å·²è™•ç† Article_490\n",
      "âœ… step2_batch_9.json å·²å®Œæˆä¸¦å„²å­˜è‡³ step2_batches/step2_batch_9.json\n",
      "\n",
      "ğŸ‰ Step 2 å…¨éƒ¨æ‰¹æ¬¡è™•ç†å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "step2_prompt = \"\"\"You are a sociology professor with 30 years of experience analyzing Asian racism in the United States.\n",
    "\n",
    "**Step 2: Extract Related Sentences**\n",
    "\n",
    "Based on the named entities identified in **Step 1**, extract **all relevant complete sentences** from the text for each entity.\n",
    "\n",
    "**For each named entity:**\n",
    "1. Use the `\"reference\"` field (the phrase as it appears in the article) to identify relevant sentences.\n",
    "2. Identify **all complete, verbatim sentences** in the text that mention or describe the entityâ€™s **involvement, action, reaction, statement, or experience** related to anti-Asian hate (directly or indirectly).\n",
    "3. Do **not paraphrase** or summarize. Use the **exact wording** from the text.\n",
    "4. If no relevant sentence is found, set `\"relevant_sentences\": []`.\n",
    "5. Return results **grouped by entity name**, exactly matching the names used in Step 1.\n",
    "6. Include the following structured metadata for each entity:\n",
    "   - `\"entity_type\"`:  \n",
    "     - For individuals: the **social role** (e.g., \"politician\", \"celebrity\", \"victim\")  \n",
    "     - For organizations: the **institutional category** (e.g., \"law_enforcement_agency\", \"ngo_or_advocacy_group\")\n",
    "   - `\"asian_status\"`:  \n",
    "     - For individuals: **\"Asian\"**, **\"Non-Asian\"**, or **\"Cannot be inferred\"**  \n",
    "     - For organizations: Always **\"Not applicable\"**\n",
    "\n",
    "**Note:** These sentences will later be used to infer **behavioral reactions** and **emotional responses**, so include **any sentence** that provides context about what the entity did, said, or experienced.\n",
    "\n",
    "### Output format (JSON):\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"Entity Name\": {\n",
    "    \"entity_type\": \"e.g., politician, law_enforcement_agency\",\n",
    "    \"asian_status\": \"Asian / Non-Asian / Cannot be inferred / Not applicable\",\n",
    "    \"relevant_sentences\": [\n",
    "      \"Sentence 1 from the article.\",\n",
    "      \"Sentence 2 from the article.\"\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# å»ºç«‹è¼¸å‡ºè³‡æ–™å¤¾\n",
    "output_dir = \"step2_batches\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# æ‰¾å‡º Step 1 çš„æ‰€æœ‰æ‰¹æ¬¡æª”æ¡ˆ\n",
    "step1_files = sorted(glob.glob(\"step1_batches/step1_batch_*.json\"))\n",
    "\n",
    "for step1_file in step1_files:\n",
    "    batch_name = os.path.basename(step1_file).replace(\"step1_\", \"step2_\")\n",
    "    output_file = os.path.join(output_dir, batch_name)\n",
    "\n",
    "    # å¦‚æœå·²ç¶“æœ‰ Step 2 çš„çµæœï¼Œå°±è·³é\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"â­ï¸ {batch_name} å·²å­˜åœ¨ï¼Œè·³é\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nğŸš€ é–‹å§‹è™•ç† {step1_file} â†’ {output_file}\")\n",
    "\n",
    "    # è®€å– Step 1 çš„çµæœ\n",
    "    with open(step1_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        step1_result = json.load(f)\n",
    "\n",
    "    step2_result = {}\n",
    "\n",
    "    # é€ç¯‡æ–‡ç« è™•ç†\n",
    "    for title, step1_text in step1_result.items():\n",
    "        content = articles.get(title, \"\")  # å¾åŸå§‹æ–‡ç«  dict æ‹¿å…§å®¹\n",
    "\n",
    "        full_prompt = (\n",
    "            step2_prompt +\n",
    "            f\"\\n\\nStep 1 Results:\\n{step1_text}\" +\n",
    "            f\"\\n\\nOriginal Article Text:\\n{content}\"\n",
    "        )\n",
    "\n",
    "        response = get_response(full_prompt)\n",
    "        step2_result[title] = response\n",
    "        print(f\"âœ”ï¸ å·²è™•ç† {title}\")\n",
    "\n",
    "    # å„²å­˜é€™ä¸€æ‰¹çš„ Step 2 çµæœ\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(step2_result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"âœ… {batch_name} å·²å®Œæˆä¸¦å„²å­˜è‡³ {output_file}\")\n",
    "\n",
    "print(\"\\nğŸ‰ Step 2 å…¨éƒ¨æ‰¹æ¬¡è™•ç†å®Œæˆï¼\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc2b10c2-8265-45e8-8c31-e98d7f31ab67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‘Œ æ²’æœ‰éœ€è¦è£œè·‘çš„æ–‡ç« ï¼Œå…¨æ•¸å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def rerun_incomplete_step2(step1_file, step2_file):\n",
    "    # è®€å– Step1 çš„çµæœ\n",
    "    with open(step1_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        step1_result = json.load(f)\n",
    "\n",
    "    # å¦‚æœ step2_file å·²å­˜åœ¨ï¼Œå…ˆè®€å–ï¼›å¦å‰‡æ–°å»º\n",
    "    if os.path.exists(step2_file):\n",
    "        with open(step2_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            step2_result = json.load(f)\n",
    "    else:\n",
    "        step2_result = {}\n",
    "\n",
    "    updated = False\n",
    "\n",
    "    for title, step1_text in step1_result.items():\n",
    "        # åªè™•ç†ç©ºçš„ or ç¼ºå¤±çš„\n",
    "        if not step2_result.get(title):\n",
    "            content = articles.get(title, \"\")  # åŸå§‹æ–‡ç« \n",
    "            full_prompt = (\n",
    "                step2_prompt +\n",
    "                f\"\\n\\nStep 1 Results:\\n{step1_text}\" +\n",
    "                f\"\\n\\nOriginal Article Text:\\n{content}\"\n",
    "            )\n",
    "            response = get_response(full_prompt)\n",
    "            step2_result[title] = response\n",
    "            print(f\"ğŸ”„ é‡æ–°è™•ç† {title}\")\n",
    "            updated = True\n",
    "\n",
    "    if updated:\n",
    "        with open(step2_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(step2_result, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"âœ… å·²æ›´æ–° {step2_file}\")\n",
    "    else:\n",
    "        print(\"ğŸ‘Œ æ²’æœ‰éœ€è¦è£œè·‘çš„æ–‡ç« ï¼Œå…¨æ•¸å®Œæˆ\")\n",
    "\n",
    "# ä½¿ç”¨ç¯„ä¾‹\n",
    "rerun_incomplete_step2(\n",
    "    step1_file=\"step1_batches/step1_batch_9.json\",\n",
    "    step2_file=\"step2_batches/step2_batch_9.json\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1c2ee9-87b3-42bb-8bef-d271575ff76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "from collections import Counter\n",
    "from typing import Dict, Any\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# å·¥å…·ï¼šJSON å¯¬é¬†è§£æ + å¥å­æ­£è¦åŒ–\n",
    "# ==============================\n",
    "def strip_code_fences(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return s\n",
    "    s = s.strip()\n",
    "    m = re.search(r\"```(?:json)?\\s*(.*?)```\", s, flags=re.S | re.I)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "    return s\n",
    "\n",
    "def parse_json_loose(s: str):\n",
    "    if not isinstance(s, str):\n",
    "        return None\n",
    "    s = strip_code_fences(s).strip()\n",
    "    if not s:\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        pass\n",
    "    m = re.search(r\"\\{.*\\}\", s, flags=re.S)\n",
    "    if m:\n",
    "        try:\n",
    "            return json.loads(m.group(0))\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def to_text(x: Any) -> str:\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    if isinstance(x, str):\n",
    "        return x\n",
    "    if isinstance(x, list):\n",
    "        return \"\\n\".join(str(item) for item in x)\n",
    "    if isinstance(x, dict):\n",
    "        if \"relevant_sentences\" in x:\n",
    "            return to_text(x[\"relevant_sentences\"])\n",
    "        return \"\\n\".join(to_text(v) for v in x.values())\n",
    "    return str(x)\n",
    "\n",
    "def parse_model_json(s: str, default: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return default\n",
    "    s2 = strip_code_fences(s)\n",
    "    try:\n",
    "        return json.loads(s2)\n",
    "    except Exception:\n",
    "        m = re.search(r\"\\{.*\\}\", s2, flags=re.S)\n",
    "        if m:\n",
    "            try:\n",
    "                return json.loads(m.group(0))\n",
    "            except Exception:\n",
    "                return default\n",
    "        return default\n",
    "\n",
    "# ==============================\n",
    "# Step2 çµæœæ­£è¦åŒ–\n",
    "# ==============================\n",
    "def normalize_step2_result(title: str, raw_obj) -> Dict[str, Dict[str, str]]:\n",
    "    if isinstance(raw_obj, dict):\n",
    "        if all(isinstance(v, dict) for v in raw_obj.values()):\n",
    "            out = {}\n",
    "            for k, v in raw_obj.items():\n",
    "                out[k] = {\n",
    "                    \"entity_type\": v.get(\"entity_type\", \"\"),\n",
    "                    \"asian_status\": v.get(\"asian_status\", \"\"),\n",
    "                    \"relevant_sentences\": to_text(v.get(\"relevant_sentences\"))\n",
    "                }\n",
    "            return out\n",
    "    if isinstance(raw_obj, str):\n",
    "        parsed = parse_json_loose(raw_obj)\n",
    "        if parsed:\n",
    "            return normalize_step2_result(title, parsed)\n",
    "    return {}\n",
    "\n",
    "# ==============================\n",
    "# Prompts\n",
    "# ==============================\n",
    "def build_gate_prompt(step2_sentences: str) -> str:\n",
    "    return f\"\"\"You are a sociology professor analyzing reactions to anti-Asian hate.\n",
    "\n",
    "Task: Decide if the extracted sentences show any OBSERVABLE reaction (action or inaction) to anti-Asian hate by the entity.\n",
    "\n",
    "Rules:\n",
    "- Observable = concrete action/inaction or explicit public stance (e.g., speaking up, condemning, organizing, reporting, policy ask, government action, refusing to act).\n",
    "- Pure emotions/concerns are NOT reactions.\n",
    "- Pure incident descriptions are NOT reactions.\n",
    "- Use ONLY the exact `relevant_sentences`.\n",
    "\n",
    "Output JSON:\n",
    "{{\n",
    "  \"has_reaction\": \"yes\" | \"no\",\n",
    "  \"evidence\": \"Exact sentence(s) from relevant_sentences (or empty if no).\"\n",
    "}}\n",
    "\n",
    "relevant_sentences:\n",
    "{step2_sentences}\n",
    "\"\"\"\n",
    "\n",
    "def build_classifier_prompt(entity_type: str, asian_status: str, step2_sentences: str) -> str:\n",
    "    return f\"\"\"You are a sociology professor with 30 years of experience analyzing anti-Asian racism.\n",
    "\n",
    "Task: Classify the entityâ€™s REACTION strictly using the Reaction Concept Tree. Use ONLY `relevant_sentences` as evidence. Do NOT infer emotions. Do NOT paraphrase.\n",
    "\n",
    "Reaction Concept Tree:\n",
    "- Support Asian Americans\n",
    "  - Attending marches/rallies\n",
    "  - Speaking up on social media\n",
    "  - Calling for being united\n",
    "  - Educating students\n",
    "  - Fostering conversations about anti-Asian hate\n",
    "  - Hiring security guards\n",
    "  - Providing shopkeepers with air horns\n",
    "  - Rewarding the public to report the info about the suspects\n",
    "- Advocacy/take actions for changes\n",
    "- Politicians initiated anti-Asian hate and racism\n",
    "- Undermining human rights\n",
    "- Color blind/minimizing racism\n",
    "- Youth as not an excuse\n",
    "- Videotaping/confronting harasser/attacker\n",
    "- Sex (sexual) addiction\n",
    "- Religion as a reason\n",
    "- Feeling hopeless or support AAPI being not enough\n",
    "- Not confronting attacker/harasser or not reporting\n",
    "- Useless law enforcement\n",
    "  - Did not take a report on Anti-Asian hate crime\n",
    "  - Did not often patrol the streets\n",
    "- Government takes actions to stop AAPI hate\n",
    "  - Installing hotlines\n",
    "  - Launching a hate-crime task force\n",
    "  - Increasing patrols\n",
    "  - Organizing a town hall\n",
    "\n",
    "Strict Rules:\n",
    "1) Pure concerns/worries â‰  reaction; return \"Cannot be inferred\".\n",
    "2) Arrests/charges/prosecutions â‡’ â€œGovernment takes actionsâ€¦â€.\n",
    "3) Explicit condemnation â‡’ â€œSupport Asian Americansâ€.\n",
    "4) If no clear reaction, return \"Cannot be inferred\".\n",
    "5) Do NOT invent labels.\n",
    "6) Always choose the most specific subcategory.\n",
    "\n",
    "entity_type: {entity_type}\n",
    "asian_status: {asian_status}\n",
    "\n",
    "relevant_sentences:\n",
    "{step2_sentences}\n",
    "\n",
    "Output JSON:\n",
    "{{\n",
    "  \"reaction\": \"<one label from the tree or 'Cannot be inferred'>\",\n",
    "  \"reaction_reason\": \"Exact sentence(s) from relevant_sentences\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "# ==============================\n",
    "# ä¸»æµç¨‹ï¼šå¾ Step2 â†’ Step3\n",
    "# ==============================\n",
    "def run_step3_from_step2(step2_dir=\"step2_batches\", step3_dir=\"step3_batches_new\"):\n",
    "    os.makedirs(step3_dir, exist_ok=True)\n",
    "    step2_files = sorted(glob.glob(os.path.join(step2_dir, \"step2_batch_*.json\")))\n",
    "    total_batches = len(step2_files)\n",
    "\n",
    "    for idx, step2_file in enumerate(step2_files, start=1):\n",
    "        batch_name = os.path.basename(step2_file).replace(\"step2_\", \"step3_\")\n",
    "        out_path = os.path.join(step3_dir, batch_name)\n",
    "\n",
    "        # æ–·é»çºŒè·‘\n",
    "        if os.path.exists(out_path):\n",
    "            print(f\"â­ï¸ æ‰¹æ¬¡ {idx}/{total_batches} {batch_name} å·²å­˜åœ¨ï¼Œè·³é\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ {idx}/{total_batches}: {step2_file}\")\n",
    "\n",
    "        with open(step2_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            step2_batch_result = json.load(f)\n",
    "\n",
    "        step3_batch_result = {}\n",
    "        gate_stats = Counter()\n",
    "        label_stats = Counter()\n",
    "\n",
    "        for title, raw in step2_batch_result.items():\n",
    "            entities = normalize_step2_result(title, raw)\n",
    "            entity_outputs = {}\n",
    "\n",
    "            if not entities:\n",
    "                step3_batch_result[title] = entity_outputs\n",
    "                continue\n",
    "\n",
    "            for entity, meta in entities.items():\n",
    "                entity_type = meta.get(\"entity_type\", \"\")\n",
    "                asian_status = meta.get(\"asian_status\", \"\")\n",
    "                relevant_sentences = to_text(meta.get(\"relevant_sentences\", \"\")).strip()\n",
    "\n",
    "                # --- 3A: Gate ---\n",
    "                gate_prompt = build_gate_prompt(relevant_sentences)\n",
    "                gate_resp = get_response(gate_prompt, temperature=0.0)\n",
    "                gate_json = parse_model_json(gate_resp, default={\"has_reaction\": \"no\", \"evidence\": \"\"})\n",
    "\n",
    "                has_reaction = str(gate_json.get(\"has_reaction\", \"no\")).lower()\n",
    "                gate_stats[has_reaction] += 1\n",
    "\n",
    "                if has_reaction != \"yes\":\n",
    "                    out = {\n",
    "                        \"entity_type\": entity_type,\n",
    "                        \"asian_status\": asian_status,\n",
    "                        \"reaction\": \"Cannot be inferred\",\n",
    "                        \"reaction_reason\": \"\"\n",
    "                    }\n",
    "                    entity_outputs[entity] = out\n",
    "                    label_stats[\"Cannot be inferred\"] += 1\n",
    "                    continue\n",
    "\n",
    "                # --- 3B: Classifier ---\n",
    "                cls_prompt = build_classifier_prompt(entity_type, asian_status, relevant_sentences)\n",
    "                cls_resp = get_response(cls_prompt, temperature=0.0)\n",
    "                cls_json = parse_model_json(cls_resp, default={\"reaction\": \"Cannot be inferred\", \"reaction_reason\": \"\"})\n",
    "\n",
    "                label = cls_json.get(\"reaction\", \"Cannot be inferred\")\n",
    "                reason = cls_json.get(\"reaction_reason\", \"\")\n",
    "\n",
    "                out = {\n",
    "                    \"entity_type\": entity_type,\n",
    "                    \"asian_status\": asian_status,\n",
    "                    \"reaction\": label,\n",
    "                    \"reaction_reason\": reason\n",
    "                }\n",
    "                entity_outputs[entity] = out\n",
    "                label_stats[label] += 1\n",
    "\n",
    "            step3_batch_result[title] = entity_outputs\n",
    "\n",
    "        # å„²å­˜\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(step3_batch_result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"âœ… æ‰¹æ¬¡ {idx}/{total_batches} å·²å®Œæˆä¸¦å„²å­˜è‡³ {out_path}\")\n",
    "        print(\"   Gate stats:\", dict(gate_stats))\n",
    "        print(\"   Label stats:\", dict(label_stats))\n",
    "\n",
    "# ==============================\n",
    "# åŸ·è¡Œ\n",
    "# ==============================\n",
    "if __name__ == \"__main__\":\n",
    "    run_step3_from_step2(\"step2_batches\", \"step3_batches_new\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "235f9159-8986-4aaa-8c91-8533ad327d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 1/12: step2_batches/step2_batch_1.json\n",
      "âœ… æ‰¹æ¬¡ 1/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step4_batches/step4_batch_1.json\n",
      "   Stats: {'done': 50}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 2/12: step2_batches/step2_batch_10.json\n",
      "âœ… æ‰¹æ¬¡ 2/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step4_batches/step4_batch_10.json\n",
      "   Stats: {'done': 50}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 3/12: step2_batches/step2_batch_11.json\n",
      "âœ… æ‰¹æ¬¡ 3/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step4_batches/step4_batch_11.json\n",
      "   Stats: {'done': 50}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 4/12: step2_batches/step2_batch_12.json\n",
      "âœ… æ‰¹æ¬¡ 4/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step4_batches/step4_batch_12.json\n",
      "   Stats: {'done': 34}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 5/12: step2_batches/step2_batch_2.json\n",
      "âœ… æ‰¹æ¬¡ 5/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step4_batches/step4_batch_2.json\n",
      "   Stats: {'done': 50}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 6/12: step2_batches/step2_batch_3.json\n",
      "âœ… æ‰¹æ¬¡ 6/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step4_batches/step4_batch_3.json\n",
      "   Stats: {'done': 50}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 7/12: step2_batches/step2_batch_4.json\n",
      "âœ… æ‰¹æ¬¡ 7/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step4_batches/step4_batch_4.json\n",
      "   Stats: {'done': 50}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 8/12: step2_batches/step2_batch_5.json\n",
      "âœ… æ‰¹æ¬¡ 8/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step4_batches/step4_batch_5.json\n",
      "   Stats: {'done': 50}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 9/12: step2_batches/step2_batch_6.json\n",
      "âœ… æ‰¹æ¬¡ 9/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step4_batches/step4_batch_6.json\n",
      "   Stats: {'done': 50}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 10/12: step2_batches/step2_batch_7.json\n",
      "âœ… æ‰¹æ¬¡ 10/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step4_batches/step4_batch_7.json\n",
      "   Stats: {'done': 50}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 11/12: step2_batches/step2_batch_8.json\n",
      "âœ… æ‰¹æ¬¡ 11/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step4_batches/step4_batch_8.json\n",
      "   Stats: {'done': 50}\n",
      "\n",
      "ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ 12/12: step2_batches/step2_batch_9.json\n",
      "âœ… æ‰¹æ¬¡ 12/12 å·²å®Œæˆä¸¦å„²å­˜è‡³ step4_batches/step4_batch_9.json\n",
      "   Stats: {'done': 50}\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Step 4 Prompt\n",
    "# ==============================\n",
    "step4_prompt = \"\"\"You are a sociology professor with 30 years of experience analyzing racial dynamics and anti-Asian racism in the United States.\n",
    "\n",
    "### Step 4: Infer **Emotions and Their Intensity**\n",
    "\n",
    "Your task is to analyze the **extracted sentences** from **Step 2** and infer each entity's **emotional stance** toward anti-Asian hate.\n",
    "\n",
    "You will also receive metadata from Step 2, including:\n",
    "- `entity_type`: for individuals use their social role; for organizations use institutional category.\n",
    "- `asian_status`: \"Asian\", \"Non-Asian\", \"Cannot be inferred\", or \"Not applicable\"\n",
    "\n",
    "Use only the exact `relevant_sentences` from Step 2 as your source â€” do NOT paraphrase or add your own wording.\n",
    "\n",
    "---\n",
    "\n",
    "## Emotion Concept Tree\n",
    "- Love \n",
    "- Joy \n",
    "- Anger \n",
    "- Sadness \n",
    "- Fear \n",
    "- Surprise \n",
    "\n",
    "---\n",
    "\n",
    "## Instructions\n",
    "1. Focus only on emotions â€” do NOT infer actions.\n",
    "2. If no emotion is expressed, output `\"emotion\": \"Cannot be inferred\".\n",
    "3. If multiple emotions appear, list multiple objects.\n",
    "4. Use the exact sentence(s) as `\"emotion_reason\"`.\n",
    "\n",
    "---\n",
    "\n",
    "## Output format\n",
    "{\n",
    "  \"Entity Name\": {\n",
    "    \"entity_type\": \"...\",\n",
    "    \"asian_status\": \"...\",\n",
    "    \"emotions\": [\n",
    "      {\n",
    "        \"emotion\": \"deepest matched term or Cannot be inferred\",\n",
    "        \"emotion_reason\": \"Exact sentence(s)\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# ==============================\n",
    "# ä¸»æµç¨‹ Step2 â†’ Step4\n",
    "# ==============================\n",
    "def run_step4_from_step2(step2_dir=\"step2_batches\", step4_dir=\"step4_batches\"):\n",
    "    os.makedirs(step4_dir, exist_ok=True)\n",
    "    step2_files = sorted(glob.glob(os.path.join(step2_dir, \"step2_batch_*.json\")))\n",
    "    total_batches = len(step2_files)\n",
    "\n",
    "    for idx, step2_file in enumerate(step2_files, start=1):\n",
    "        batch_name = os.path.basename(step2_file).replace(\"step2_\", \"step4_\")\n",
    "        out_path = os.path.join(step4_dir, batch_name)\n",
    "\n",
    "        # æ–·é»çºŒè·‘\n",
    "        if os.path.exists(out_path):\n",
    "            print(f\"â­ï¸ æ‰¹æ¬¡ {idx}/{total_batches} {batch_name} å·²å­˜åœ¨ï¼Œè·³é\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ {idx}/{total_batches}: {step2_file}\")\n",
    "\n",
    "        with open(step2_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            step2_batch_result = json.load(f)\n",
    "\n",
    "        step4_batch_result = {}\n",
    "        emo_stats = Counter()\n",
    "\n",
    "        for title, step2_text in step2_batch_result.items():\n",
    "            full_prompt = (\n",
    "                step4_prompt +\n",
    "                f\"\\n\\nStep 2 Results (Extracted Sentences):\\n{step2_text}\"\n",
    "            )\n",
    "\n",
    "            response = get_response(full_prompt)\n",
    "            step4_batch_result[title] = response\n",
    "            emo_stats[\"done\"] += 1\n",
    "\n",
    "        # å„²å­˜\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(step4_batch_result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"âœ… æ‰¹æ¬¡ {idx}/{total_batches} å·²å®Œæˆä¸¦å„²å­˜è‡³ {out_path}\")\n",
    "        print(\"   Stats:\", dict(emo_stats))\n",
    "\n",
    "# ==============================\n",
    "# åŸ·è¡Œ\n",
    "# ==============================\n",
    "if __name__ == \"__main__\":\n",
    "    run_step4_from_step2(\"step2_batches\", \"step4_batches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8509778e-2d88-4fd6-9130-661ad83aaa9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Step 3 å·²åˆä½µ 12 å€‹æ‰¹æ¬¡æª” â†’ step3_all.csvï¼Œå…± 8973 ç­†\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def merge_step3_with_id(input_dir=\"step3_batches\", prefix=\"step3_batch_\", \n",
    "                        output_json=\"step3_all.json\", output_csv=\"step3_all.csv\"):\n",
    "    batch_files = sorted(glob.glob(os.path.join(input_dir, f\"{prefix}*.json\")))\n",
    "\n",
    "    merged_result = {}\n",
    "    for file in batch_files:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            merged_result.update(data)\n",
    "\n",
    "    with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(merged_result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    rows = []\n",
    "    idx = 1\n",
    "    for article_id, entities in merged_result.items():\n",
    "        for entity, meta in entities.items():\n",
    "            rows.append({\n",
    "                \"reaction_id\": f\"reaction_{idx}\",\n",
    "                \"article_id\": article_id,\n",
    "                \"entity\": entity,\n",
    "                \"entity_type\": meta.get(\"entity_type\", \"\"),\n",
    "                \"asian_status\": meta.get(\"asian_status\", \"\"),\n",
    "                \"reaction\": meta.get(\"reaction\", \"\"),\n",
    "                \"reaction_reason\": meta.get(\"reaction_reason\", \"\")\n",
    "            })\n",
    "            idx += 1\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"âœ… Step 3 å·²åˆä½µ {len(batch_files)} å€‹æ‰¹æ¬¡æª” â†’ {output_csv}ï¼Œå…± {len(df)} ç­†\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    merge_step3_with_id(\n",
    "        input_dir=\"step3_batches\",\n",
    "        prefix=\"step3_batch_\",\n",
    "        output_json=\"step3_all.json\",\n",
    "        output_csv=\"step3_all.csv\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "094b81c0-66dd-4e5b-8e48-f663b5550e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Step 4 å·²åˆä½µ 12 å€‹æ‰¹æ¬¡æª” â†’ step4_all.csvï¼Œå…± 14762 ç­†\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def parse_json_loose(s: str):\n",
    "    \"\"\"å˜—è©¦å¾å­—ä¸²è£¡è§£æ JSON\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return None\n",
    "    s = s.strip()\n",
    "    if not s:\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        # å˜—è©¦æŠ“ç¬¬ä¸€å€‹ {...}\n",
    "        m = re.search(r\"\\{.*\\}\", s, flags=re.S)\n",
    "        if m:\n",
    "            try:\n",
    "                return json.loads(m.group(0))\n",
    "            except Exception:\n",
    "                return None\n",
    "    return None\n",
    "\n",
    "def merge_step4_with_id(input_dir=\"step4_batches\", prefix=\"step4_batch_\", \n",
    "                        output_json=\"step4_all.json\", output_csv=\"step4_all.csv\"):\n",
    "    batch_files = sorted(glob.glob(os.path.join(input_dir, f\"{prefix}*.json\")))\n",
    "    all_articles = {}\n",
    "\n",
    "    for file in batch_files:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # âš ï¸ ä¸ç”¨ updateï¼Œé€ç¯‡å±•é–‹\n",
    "        for article_id, raw in data.items():\n",
    "            if isinstance(raw, dict):\n",
    "                all_articles[article_id] = raw\n",
    "            elif isinstance(raw, str):\n",
    "                parsed = parse_json_loose(raw)\n",
    "                if isinstance(parsed, dict):\n",
    "                    all_articles[article_id] = parsed\n",
    "\n",
    "    # å­˜ JSON\n",
    "    with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_articles, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # è½‰æˆ CSV\n",
    "    rows = []\n",
    "    idx = 1\n",
    "    for article_id, entities in all_articles.items():\n",
    "        for entity, meta in entities.items():\n",
    "            emotions = meta.get(\"emotions\", [])\n",
    "            \n",
    "            # çµ±ä¸€è™•ç†æ ¼å¼\n",
    "            if isinstance(emotions, str):\n",
    "                emotions = [{\n",
    "                    \"emotion\": emotions,\n",
    "                    \"emotion_path\": None,\n",
    "                    \"emotion_reason\": \"\"\n",
    "                }]\n",
    "            elif not isinstance(emotions, list):\n",
    "                emotions = []\n",
    "    \n",
    "            for emo in emotions:\n",
    "                if not isinstance(emo, dict):  # å†ä¿éšªä¸€æ¬¡\n",
    "                    emo = {\"emotion\": str(emo), \"emotion_path\": None, \"emotion_reason\": \"\"}\n",
    "                rows.append({\n",
    "                    \"emotion_id\": f\"emotion_{idx}\",\n",
    "                    \"article_id\": article_id,\n",
    "                    \"entity\": entity,\n",
    "                    \"entity_type\": meta.get(\"entity_type\", \"\"),\n",
    "                    \"asian_status\": meta.get(\"asian_status\", \"\"),\n",
    "                    \"emotion\": emo.get(\"emotion\", \"\"),\n",
    "                    \"emotion_path\": emo.get(\"emotion_path\", \"\"),\n",
    "                    \"emotion_reason\": emo.get(\"emotion_reason\", \"\")\n",
    "                })\n",
    "                idx += 1\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"âœ… Step 4 å·²åˆä½µ {len(batch_files)} å€‹æ‰¹æ¬¡æª” â†’ {output_csv}ï¼Œå…± {len(df)} ç­†\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    merge_step4_with_id(\n",
    "        input_dir=\"step4_batches\",\n",
    "        prefix=\"step4_batch_\",\n",
    "        output_json=\"step4_all.json\",\n",
    "        output_csv=\"step4_all.csv\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fd4dab",
   "metadata": {},
   "source": [
    "# é‡å°ä¹‹å‰ç„¡æ³•æ­£ç¢ºè§£ææª”æ¡ˆæŠ“ bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b536604c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re\n",
    "from typing import Any, Dict\n",
    "\n",
    "# å…ˆçµ±ä¸€æ¸…ç†ï¼šå»åœæ¬„ã€å» BOM/æ§åˆ¶å­—å…ƒã€ä¿®å¸¸è¦‹ mojibakeã€æŠ“æœ€å¤–å±¤ {...}/[...]ã€ç§»é™¤å°¾é€—è™Ÿ\n",
    "def strip_code_fences(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return s\n",
    "    s = s.strip()\n",
    "    m = re.search(r\"```(?:json)?\\s*([\\s\\S]*?)\\s*```\", s, flags=re.I)\n",
    "    return m.group(1).strip() if m else s\n",
    "\n",
    "def _sanitize_json_like(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = strip_code_fences(s).lstrip(\"\\ufeff\")\n",
    "    # ç§»é™¤ä¸å¯è¦‹æ§åˆ¶å­—å…ƒï¼ˆä¿ç•™æ›è¡Œ/ç¸®æ’ï¼‰\n",
    "    s = \"\".join(ch for ch in s if ch.isprintable() or ch in \"\\n\\r\\t\")\n",
    "    # å˜—è©¦ä¿®æ­£å¸¸è¦‹ UTF-8â†â†’latin1 äº‚ç¢¼\n",
    "    if \"Ã¢Â€\" in s or \"Ãƒ\" in s:\n",
    "        try:\n",
    "            s = s.encode(\"latin1\").decode(\"utf-8\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    # å–æœ€å¤–å±¤ {...} æˆ– [...]\n",
    "    m = re.search(r\"(\\{[\\s\\S]*\\}|\\[[\\s\\S]*\\])\", s)\n",
    "    if m:\n",
    "        s = m.group(1)\n",
    "    # ç§»é™¤å°¾é€—è™Ÿï¼ˆ,]} æˆ– ,}ï¼‰\n",
    "    s = re.sub(r\",(\\s*[}\\]])\", r\"\\1\", s)\n",
    "    return s.strip()\n",
    "\n",
    "# å¯¬é¬†è§£æï¼šå…ˆæ¸…ç†ï¼Œå†å˜—è©¦å¤šç¨®å€™é¸ï¼ˆå«å–®å¼•è™Ÿ pseudo-JSON åƒ…åœ¨å¿…è¦æ™‚ï¼‰\n",
    "def parse_json_loose(s: str):\n",
    "    if not isinstance(s, str):\n",
    "        return None\n",
    "    base = _sanitize_json_like(s)\n",
    "    if not base:\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(base)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # å–®å¼•è™Ÿ pseudo-JSONï¼ˆé¿å…èª¤å‚·ï¼Œåƒ…åœ¨å­—ä¸²å…§ç„¡é›™å¼•è™Ÿä¸”æœ‰å–®å¼•è™Ÿæ™‚å˜—è©¦ï¼‰\n",
    "    if base.startswith(\"{\") and base.endswith(\"}\") and '\"' not in base and \"'\" in base:\n",
    "        try:\n",
    "            return json.loads(re.sub(r\"'\", '\"', base))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "# è£œä¸Šç©º dict ä¸èƒ½è¢«ç•¶æˆ False çš„å•é¡Œ\n",
    "def parse_model_json(s: str, default: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return default\n",
    "    cleaned = _sanitize_json_like(s)\n",
    "    try:\n",
    "        return json.loads(cleaned)\n",
    "    except Exception:\n",
    "        m = re.search(r\"\\{[\\s\\S]*\\}\", cleaned)\n",
    "        if m:\n",
    "            try:\n",
    "                return json.loads(m.group(0))\n",
    "            except Exception:\n",
    "                return default\n",
    "        return default\n",
    "\n",
    "# è·ŸåŸæœ¬ä¸€æ¨£\n",
    "def to_text(x: Any) -> str:\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    if isinstance(x, str):\n",
    "        return x\n",
    "    if isinstance(x, list):\n",
    "        return \"\\n\".join(str(item) for item in x)\n",
    "    if isinstance(x, dict):\n",
    "        if \"relevant_sentences\" in x:\n",
    "            return to_text(x[\"relevant_sentences\"])\n",
    "        return \"\\n\".join(to_text(v) for v in x.values())\n",
    "    return str(x)\n",
    "\n",
    "def normalize_step2_result(title: str, raw_obj) -> Dict[str, Dict[str, str]]:\n",
    "    if isinstance(raw_obj, dict) and all(isinstance(v, dict) for v in raw_obj.values()):\n",
    "        out = {}\n",
    "        for k, v in raw_obj.items():\n",
    "            out[k] = {\n",
    "                \"entity_type\": v.get(\"entity_type\", \"\"),\n",
    "                \"asian_status\": v.get(\"asian_status\", \"\"),\n",
    "                \"relevant_sentences\": to_text(v.get(\"relevant_sentences\"))\n",
    "            }\n",
    "        return out\n",
    "    if isinstance(raw_obj, str):\n",
    "        parsed = parse_json_loose(raw_obj)\n",
    "        if parsed is not None: \n",
    "            return normalize_step2_result(title, parsed)\n",
    "    return {}\n",
    "\n",
    "# é™¤éŒ¯è¼”åŠ©ï¼šå°å‡º json å¤±æ•—ä½ç½®èˆ‡å‘¨é‚Š 2 è¡Œ\n",
    "def debug_json_failure(s: str, context=2):\n",
    "    cleaned = _sanitize_json_like(s)\n",
    "    try:\n",
    "        json.loads(cleaned)\n",
    "        print(\"OK\")\n",
    "        return\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON error @ line {e.lineno}, col {e.colno}: {e.msg}\")\n",
    "        lines = cleaned.splitlines()\n",
    "        i = e.lineno - 1\n",
    "        lo, hi = max(0, i-context), min(len(lines), i+context+1)\n",
    "        for idx in range(lo, hi):\n",
    "            mark = \">>\" if idx == i else \"  \"\n",
    "            print(f\"{mark} {idx+1:4d}: {lines[idx]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80ebac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "{'Commissioner Marvin Rodriguez': {'entity_type': 'politician', 'asian_status': 'Non-Asian', 'relevant_sentences': \"TOPEKA Fanny Fang's immediate reaction to a Riley County commissioner's racist remarks involved a profane two-word outburst and reflection on whether she had made a mistake by returning to Kansas a couple of years ago.\\nAfter Commissioner Marvin Rodriguez suggested the threat of COVID-19 was low in the community because there were no Chinese people Ã¢ he later offered a tepid apology while noting his affinity for Chinese food Ã¢ Fang refused to perpetuate the Ã¢model minority myth,Ã¢ as she called it, Ã¢where we keep to ourselves and don't say anything.\\nThat anxiety was heightened by Rodriguez's remarks during a commission meeting last month, as reported by the Manhattan Mercury.\\nÃ¢I know that other people are having a great problem,Ã¢ he said.\\nÃ¢And someone reminded me that in Italy, they have a lot of garment-people there, fashionists, and they have a tremendous amount of Chinese there, and that's where a lot of it started.\\nSo we don't necessarily have any (Chinese people), but I think the board would like to make sure that we're on top of it.Ã¢\\nWhen Fang confronted Rodriguez at a subsequent meeting, he Ã¢apologized for the remarks and how they may have hurt people.Ã¢\\nÃ¢That's not how I think,Ã¢ he said.\\nÃ¢I have everybody on my mind ... and I've got a lot of Chinese food here, and I've got some Chinese friends and stuff.Ã¢\\nRodriguez didnÃ¢t return a phone call seeking comment for this story.\\nAs a county commissioner, Rodriguez also serves on the county's board of health.\\nShana Bender, an organizer for the Manhattan Alliance for Peace and Justice, questioned the effect his Ã¢plainly racistÃ¢ remarks would have on the rest of the department.\\nFor her, that isn't an option.\\nShe plans to continue to push for Rodriguez to resign and, in the alternate, vote him out of office later this year.\"}, 'Gov. Laura Kelly': {'entity_type': 'politician', 'asian_status': 'Cannot be inferred', 'relevant_sentences': \"Gov. Laura Kelly has issued a series of high-profile executive orders, including a statewide mandate to stay at home except for essential needs, as part of her administration's response to the pandemic.\\nÃ¢I abhor any acts of aggression, particularly when they are motivated by race, gender, sexual orientation, whatever, so I would ask Kansans to please not do that,Ã¢ Kelly said.\\nÃ¢This is no one person's fault, no one ethnic group's fault.\\nThis is a pandemic that we all have responsibility to make sure it goes away, and we need to do our part.Ã¢\"}, 'Rep. Rui Xu': {'entity_type': 'politician', 'asian_status': 'Asian', 'relevant_sentences': \"Kansas Rep. Rui Xu, a Democrat from Westwood, said words have an effect on Asian Americans living in the United States.\\nÃ¢I keep making the point that nobody's confused about where this virus is from,Ã¢ Xu said.\\nÃ¢Nobody's confused that China probably hid this for at least a couple of weeks, or at least hid the numbers.\\nNobody is on the other side of that issue.\\nBut for Asian Americans, this is actually dangerous for us here.\\nI know what you're trying to do.\\nI get it.\\nBut it's actually harming people here, so you've got to stop.Ã¢\"}, 'Fanny Fang': {'entity_type': 'victim', 'asian_status': 'Asian', 'relevant_sentences': \"TOPEKA Ã¢ Fanny Fang's immediate reaction to a Riley County commissioner's racist remarks involved a profane two-word outburst and reflection on whether she had made a mistake by returning to Kansas a couple of years ago.\\nThen the Manhattan woman, whose family owns and operates the Asian Market grocery store, realized how much she appreciates the community's support.\\nAfter Commissioner Marvin Rodriguez suggested the threat of COVID-19 was low in the community because there were no Chinese people Ã¢ he later offered a tepid apology while noting his affinity for Chinese food Ã¢ Fang refused to perpetuate the Ã¢model minority myth,Ã¢ as she called it, Ã¢where we keep to ourselves and don't say anything.\\nInstead, Fang has been outspoken about the harassment of Asian Americans.\\nShe and others have called for Rodriguez to resign.\\nFang returned to Manhattan a couple of years ago after studying at New York University.\\nThis is where her parents chose to make a home in 1997 when they immigrated from a rural area of China.\\nBefore opening the grocery store, the family owned a restaurant in town.\\nÃ¢There was one day I picked up the phone and someone asked me if my family cooked cats and dogs,Ã¢ Fang said.\\nÃ¢This was when I was 10.\\nAnd now at the age of 24, I've got a local politician saying these things about Chinese people and COVID-19.\\nSo even 14 years later, it's still happening in this community.\\nAnd it always will.\\nÃ¢The difference now is I'm not 10 Ã¢ I'm 24, and I now have the ability to say something.\\nI'm not scared anymore.\\nI have a very strong support system.\\nI'm in this.\\nI know it's going to be a fight, but if I'm going to go into a fight with anybody, I'm so glad to do it with Manhattan, Kansas.Ã¢\\nFang said her anxiety skyrocketed when she heard COVID-19 broke out in China, fearing racist reaction in the United States.\\nShe has been asked in her store, sometimes by children, if she has the virus.\\nAnd she watched the terrifying video of an attack on an Asian American woman in New York City, who suffered chemical burns when a man poured liquid on her as she took out the trash.\\nThat anxiety was heightened by Rodriguez's remarks during a commission meeting last month, as reported by the Manhattan Mercury.\\nFang's immediate response: Ã¢Excuse my language for this, but it felt like a big f*** you.Ã¢\\nÃ¢I just can't say it any other way,Ã¢ she said.\\nÃ¢It made me question why I even came back to this community.Ã¢\\nWhen Fang confronted Rodriguez at a subsequent meeting, he Ã¢apologized for the remarks and how they may have hurt people.Ã¢\\nSometimes, Fang wonders if it would be safer to close the market and recede into the shadows.\\nFor her, that isn't an option.\\nShe plans to continue to push for Rodriguez to resign and, in the alternate, vote him out of office later this year.\\nÃ¢My hope is that the 13-, 14-, 15-year-old Asian American at Manhattan High sees that someone in this community who looks like them is speaking up,Ã¢ Fang said, Ã¢and I hope that if they stay here or if they go somewhere else, they will remember this and say, 'I can do this.' That's what I hope.Ã¢\"}, 'a man': {'entity_type': 'perpetrator', 'asian_status': 'Non-Asian', 'relevant_sentences': 'And she watched the terrifying video of an attack on an Asian American woman in New York City, who suffered chemical burns when a man poured liquid on her as she took out the trash.'}, 'an Asian American woman': {'entity_type': 'victim', 'asian_status': 'Asian', 'relevant_sentences': 'And she watched the terrifying video of an attack on an Asian American woman in New York City, who suffered chemical burns when a man poured liquid on her as she took out the trash.'}, 'Asian Americans': {'entity_type': 'victim', 'asian_status': 'Asian', 'relevant_sentences': \"Instead, Fang has been outspoken about the harassment of Asian Americans.\\nViolence, threats of violence, bias and hate crimes directed toward Asian Americans is as unacceptable as it is against any persons who are targeted because of what they look like or where they come from,Ã¢ Butler said.\\nAny bias-based crimes reported to the Riley County Police Department will be vigorously investigated.\\nKansas Rep. Rui Xu, a Democrat from Westwood, said words have an effect on Asian Americans living in the United States.\\nÃ¢I keep making the point that nobody's confused about where this virus is from, Xu said.\\nÃ¢Nobody's confused that China probably hid this for at least a couple of weeks, or at least hid the numbers.\\nNobody is on the other side of that issue.\\nBut for Asian Americans, this is actually dangerous for us here.\\nI know what you're trying to do.\\nI get it.\\nBut it's actually harming people here, so you've got to stop.\\nÃ¢My hope is that the 13-, 14-, 15-year-old Asian American at Manhattan High sees that someone in this community who looks like them is speaking up, Fang said, Ã¢and I hope that if they stay here or if they go somewhere else, they will remember this and say, 'I can do this.' That's what I hope.Ã¢\"}, 'Shana Bender': {'entity_type': 'other_individuals', 'asian_status': 'Cannot be inferred', 'relevant_sentences': 'Shana Bender, an organizer for the Manhattan Alliance for Peace and Justice, questioned the effect his Ã¢plainly racistÃ¢ remarks would have on the rest of the department.\\nÃ¢How are they going to respond in a global crisis?Ã¢ Bender said.\\nÃ¢What does that mean for our minorities who come for emergency services?\\nWill there be a bias in place?\\nWill this put people at further harm because of their internal bias?'}, 'Dennis Butler': {'entity_type': 'other_individuals', 'asian_status': 'Cannot be inferred', 'relevant_sentences': 'Dennis Butler, director of the Riley County Police Department, welcomed feedback from Asian Americans who feel unsafe or wish to report bias or a hate crime.\\nViolence, threats of violence, bias and hate crimes directed toward Asian Americans is as unacceptable as it is against any persons who are targeted because of what they look like or where they come from,Ã¢ Butler said.\\nÃ¢Any bias-based crimes reported to the Riley County Police Department will be vigorously investigated.'}, 'Riley County Police Department': {'entity_type': 'law_enforcement_agency', 'asian_status': 'Not applicable', 'relevant_sentences': 'Dennis Butler, director of the Riley County Police Department, welcomed feedback from Asian Americans who feel unsafe or wish to report bias or a hate crime.\\nÃ¢Violence, threats of violence, bias and hate crimes directed toward Asian Americans is as unacceptable as it is against any persons who are targeted because of what they look like or where they come from,Ã¢ Butler said.\\nAny bias-based crimes reported to the Riley County Police Department will be vigorously investigated.'}, \"county's board of health\": {'entity_type': 'government_bodies', 'asian_status': 'Not applicable', 'relevant_sentences': \"As a county commissioner, Rodriguez also serves on the county's board of health.\"}, 'Manhattan Alliance for Peace and Justice': {'entity_type': 'ngo_or_advocacy_group', 'asian_status': 'Not applicable', 'relevant_sentences': 'Shana Bender, an organizer for the Manhattan Alliance for Peace and Justice, questioned the effect his Ã¢plainly racistÃ¢ remarks would have on the rest of the department.'}, 'Asian Market grocery store': {'entity_type': 'business_entities', 'asian_status': 'Not applicable', 'relevant_sentences': \"Then the Manhattan woman, whose family owns and operates the Asian Market grocery store, realized how much she appreciates the community's support.\\nShe has been asked in her store, sometimes by children, if she has the virus.\"}}\n",
      "âœ… å·²å®Œæˆ ['Article_155']ï¼Œå„²å­˜è‡³ step3_batches_new/step3_batch_3__subset_2.json\n",
      "   Gate stats: {'no': 5, 'yes': 8}\n",
      "   Label stats: {'Cannot be inferred': 5, 'Support Asian Americans': 7, 'Advocacy/take actions for changes': 1}\n"
     ]
    }
   ],
   "source": [
    "import os, json, glob\n",
    "from collections import Counter\n",
    "\n",
    "def run_step3_for_titles(step2_file=\"step2_batches/step2_batch_8.json\",\n",
    "                         titles=None,  # ä¾‹å¦‚ [\"Some Article Title\"]ï¼›è‹¥ç‚º None å‰‡æŒ‘ç¬¬ä¸€ç¯‡\n",
    "                         out_dir=\"step3_batches_new\",\n",
    "                         out_suffix=\"__subset_2\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    with open(step2_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        step2_batch_result = json.load(f)\n",
    "\n",
    "    # æ±ºå®šè¦è·‘å“ªäº› title\n",
    "    if titles is None:\n",
    "        # æ²’çµ¦å°±å–ç¬¬ä¸€ç¯‡\n",
    "        first_title = next(iter(step2_batch_result.keys()), None)\n",
    "        if first_title is None:\n",
    "            print(\"æª”æ¡ˆè£¡æ²’æœ‰è³‡æ–™\")\n",
    "            return\n",
    "        titles = [first_title]\n",
    "\n",
    "    # åªä¿ç•™éœ€è¦çš„é …ç›®\n",
    "    filtered = {t: step2_batch_result[t] for t in titles if t in step2_batch_result}\n",
    "    if not filtered:\n",
    "        print(\"æ‰¾ä¸åˆ°æŒ‡å®šçš„ title\")\n",
    "        return\n",
    "\n",
    "    gate_stats = Counter()\n",
    "    label_stats = Counter()\n",
    "    step3_batch_result = {}\n",
    "\n",
    "    for title, raw in filtered.items():\n",
    "        debug_json_failure(raw)                   # çœ‹çœ‹å ±éŒ¯è¡Œ\n",
    "        entities = normalize_step2_result(title, raw)\n",
    "        entity_outputs = {}\n",
    "        print(entities)\n",
    "\n",
    "        if not entities:\n",
    "            step3_batch_result[title] = entity_outputs\n",
    "            continue\n",
    "\n",
    "        for entity, meta in entities.items():\n",
    "            entity_type = meta.get(\"entity_type\", \"\")\n",
    "            asian_status = meta.get(\"asian_status\", \"\")\n",
    "            relevant_sentences = to_text(meta.get(\"relevant_sentences\", \"\")).strip()\n",
    "\n",
    "            # --- 3A: Gate ---\n",
    "            gate_prompt = build_gate_prompt(relevant_sentences)\n",
    "            gate_resp = get_response(gate_prompt, temperature=0.0)\n",
    "            gate_json = parse_model_json(gate_resp, default={\"has_reaction\": \"no\", \"evidence\": \"\"})\n",
    "\n",
    "            has_reaction = str(gate_json.get(\"has_reaction\", \"no\")).lower()\n",
    "            gate_stats[has_reaction] += 1\n",
    "\n",
    "            if has_reaction != \"yes\":\n",
    "                out = {\n",
    "                    \"entity_type\": entity_type,\n",
    "                    \"asian_status\": asian_status,\n",
    "                    \"reaction\": \"Cannot be inferred\",\n",
    "                    \"reaction_reason\": \"\"\n",
    "                }\n",
    "                entity_outputs[entity] = out\n",
    "                label_stats[\"Cannot be inferred\"] += 1\n",
    "                continue\n",
    "\n",
    "            # --- 3B: Classifier ---\n",
    "            cls_prompt = build_classifier_prompt(entity_type, asian_status, relevant_sentences)\n",
    "            cls_resp = get_response(cls_prompt, temperature=0.0)\n",
    "            cls_json = parse_model_json(cls_resp, default={\"reaction\": \"Cannot be inferred\", \"reaction_reason\": \"\"})\n",
    "\n",
    "            label = cls_json.get(\"reaction\", \"Cannot be inferred\")\n",
    "            reason = cls_json.get(\"reaction_reason\", \"\")\n",
    "\n",
    "            out = {\n",
    "                \"entity_type\": entity_type,\n",
    "                \"asian_status\": asian_status,\n",
    "                \"reaction\": label,\n",
    "                \"reaction_reason\": reason\n",
    "            }\n",
    "            entity_outputs[entity] = out\n",
    "            label_stats[label] += 1\n",
    "\n",
    "        step3_batch_result[title] = entity_outputs\n",
    "\n",
    "    # è¼¸å‡ºæª”åï¼šåŸ batch åå†åŠ  subset\n",
    "    base = os.path.basename(step2_file).replace(\"step2_\", \"step3_\").replace(\".json\", f\"{out_suffix}.json\")\n",
    "    out_path = os.path.join(out_dir, base)\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(step3_batch_result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"âœ… å·²å®Œæˆ {list(step3_batch_result.keys())}ï¼Œå„²å­˜è‡³ {out_path}\")\n",
    "    print(\"   Gate stats:\", dict(gate_stats))\n",
    "    print(\"   Label stats:\", dict(label_stats))\n",
    "\n",
    "# ==============================\n",
    "# æ˜ç¢ºæŒ‡å®šæŸç¯‡ titleï¼ˆå¤šç¯‡ä¹Ÿå¯ï¼‰\n",
    "# ==============================\n",
    "run_step3_for_titles(\"step2_batches/step2_batch_3.json\", titles=[\"Article_155\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02abf62-6b50-4158-96e5-e868a510cf72",
   "metadata": {},
   "source": [
    "# é‡åˆ† entity_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "28dbb82d-0a39-4fe6-a614-2b3379ab7f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… finish\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === 1. è®€å–åŸå§‹è³‡æ–™ ===\n",
    "df = pd.read_csv(\"step3_all.csv\")  # æ›æˆä½ çš„æª”æ¡ˆåç¨±\n",
    "\n",
    "# === 2. å®šç¾©æ¨™æº–åŒ–å°æ‡‰è¡¨ ===\n",
    "entity_type_mapping = {\n",
    "    # Individuals\n",
    "    \"victim\": \"victims\",\n",
    "    \"victims\": \"victims\",\n",
    "    \"perpetrator\": \"perpetrators\",\n",
    "    \"perpetrators\": \"perpetrators\",\n",
    "    \"politician\": \"politicians\",\n",
    "    \"politicians\": \"politicians\",\n",
    "    \"professional\": \"professionals\",\n",
    "    \"professionals\": \"professionals\",\n",
    "    \"celebrity\": \"celebrities\",\n",
    "    \"musician\": \"celebrities\",\n",
    "    \"actor\": \"celebrities\",\n",
    "    \"actress\": \"celebrities\",\n",
    "    \"journalist\": \"professionals\",\n",
    "    \"reporter\": \"professionals\",\n",
    "    \"professor\": \"professionals\",\n",
    "    \"student\": \"professionals\",\n",
    "    \"educator\": \"professionals\",\n",
    "    \"scholar\": \"professionals\",\n",
    "    \"adjunct instructor\": \"professionals\",\n",
    "    \"attorney\": \"professionals\",\n",
    "    \"director\": \"professionals\",\n",
    "    \"city_manager\": \"professionals\",\n",
    "    \"sociology professor\": \"professionals\",\n",
    "    \"editor\": \"professionals\",\n",
    "    \"deputy inspector\": \"law_enforcement_agencies\",\n",
    "    \"police_officer\": \"law_enforcement_agencies\",\n",
    "    \"police spokesperson\": \"law_enforcement_agencies\",\n",
    "    \"family_member\": \"other_individuals\",\n",
    "    \"friend\": \"other_individuals\",\n",
    "    \"witness\": \"other_individuals\",\n",
    "    \"individual\": \"other_individuals\",\n",
    "    \"individuals\": \"other_individuals\",\n",
    "    \"general public\": \"other_individuals\",\n",
    "    \"general_public\": \"other_individuals\",\n",
    "    \"social_circle\": \"other_individuals\",\n",
    "    \"community_activist\": \"other_individuals\",\n",
    "    \"community_leader\": \"other_individuals\",\n",
    "    \"organizer\": \"other_individuals\",\n",
    "    \"community organizer\": \"other_individuals\",\n",
    "    \"other individual\": \"other_individuals\",\n",
    "    \"rally organizer\": \"other_individuals\",\n",
    "    \"activist\": \"other_individuals\",\n",
    "    \"supporter\": \"other_individuals\",\n",
    "    \"co-host\": \"celebrities\",\n",
    "    \"artist\": \"celebrities\",\n",
    "    \"former assistant district attorney\": \"professionals\",\n",
    "    \"official\": \"professionals\",\n",
    "    \"non-Asian\": \"other_individuals\",\n",
    "    \"youth coordinator\": \"professionals\",\n",
    "    \"school_board_member\": \"professionals\",\n",
    "    \"Dean\": \"professionals\",\n",
    "    \"community leader\": \"other_individuals\",\n",
    "    \"government body\": \"government_bodies\",\n",
    "\n",
    "    # Organizations\n",
    "    \"law_enforcement_agency\": \"law_enforcement_agencies\",\n",
    "    \"law_enforcement_agencies\": \"law_enforcement_agencies\",\n",
    "    \"government_body\": \"government_bodies\",\n",
    "    \"government_bodies\": \"government_bodies\",\n",
    "    \"ngo_or_advocacy_group\": \"ngo_or_advocacy_groups\",\n",
    "    \"ngo_or_advocacy_groups\": \"ngo_or_advocacy_groups\",\n",
    "    \"business_entity\": \"business_entities\",\n",
    "    \"business_entities\": \"business_entities\",\n",
    "    \"community_group\": \"community_groups\",\n",
    "    \"community_groups\": \"community_groups\",\n",
    "    \"educational_institution\": \"government_bodies\",  # å‡è¨­ç‚ºæ­£å¼æ©Ÿæ§‹\n",
    "\n",
    "    # Fallback\n",
    "    \"other\": \"other_individuals\",\n",
    "    \"other_individual\": \"other_individuals\",\n",
    "    \"other_individuals\": \"other_individuals\",\n",
    "    \"group\": \"unknown\",\n",
    "    # \"Cannot be inferred\": \"unknown\",\n",
    "}\n",
    "\n",
    "# === 3. æ›¿æ› entity_type æ¬„ä½ï¼ˆç›´æ¥è¦†è“‹ï¼‰===\n",
    "df[\"entity_type\"] = df[\"entity_type\"].map(entity_type_mapping).fillna(df[\"entity_type\"])\n",
    "\n",
    "# === 4. è¼¸å‡ºæˆæ–°æª”æ¡ˆ ===\n",
    "df.to_csv(\"step3_all_gemini.csv\", index=False)\n",
    "print(\"âœ… finish\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a569e0-82bd-437f-97cd-a4ea32796158",
   "metadata": {},
   "source": [
    "# é‡åˆ† emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28c67d52-5974-4ae1-a4ed-0e8ec62ccaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²å®Œæˆï¼šemotion å…¨éƒ¨è½‰æˆå°å¯« (love, joy, anger, sadness, fear, surprise, cannot be inferred)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# è¼‰å…¥ CSV\n",
    "df = pd.read_csv(\"step4_all_with_date.csv\")\n",
    "\n",
    "# å®šç¾© emotion å°ç…§è¡¨ï¼ˆç´°åˆ†é¡ â†’ å…­å¤§æƒ…ç·’ï¼Œå…¨å°å¯«ï¼‰\n",
    "emotion_map = {\n",
    "    # love\n",
    "    \"love\": \"love\",\n",
    "    \"support\": \"love\", \"solidarity\": \"love\",\n",
    "    \"empathy\": \"love\", \"compassion\": \"love\",\n",
    "    \"recognition\": \"love\", \"gratitude\": \"love\",\n",
    "    \"appreciation\": \"love\", \"encouragement\": \"love\",\n",
    "    \"affection\": \"love\", \"lust\": \"love\", \"longing\": \"love\",\n",
    "\n",
    "    # joy\n",
    "    \"joy\": \"joy\",\n",
    "    \"confidence\": \"joy\", \"optimism\": \"joy\", \"empowerment\": \"joy\",\n",
    "    \"cheerfulness\": \"joy\", \"zest\": \"joy\", \"contentment\": \"joy\",\n",
    "    \"pride\": \"joy\", \"relief\": \"joy\",\n",
    "\n",
    "    # anger\n",
    "    \"anger\": \"anger\",\n",
    "    \"outrage\": \"anger\", \"defiance\": \"anger\", \"responsibility\": \"anger\",\n",
    "    \"irritation\": \"anger\", \"exasperation\": \"anger\", \"rage\": \"anger\",\n",
    "    \"disgust\": \"anger\", \"envy\": \"anger\", \"determination\": \"anger\",\n",
    "    \"urgency\": \"anger\", \"frustration\": \"anger\",\n",
    "\n",
    "    # sadness\n",
    "    \"sadness\": \"sadness\",\n",
    "    \"worry\": \"sadness\", \"resignation\": \"sadness\", \"regret\": \"sadness\",\n",
    "    \"mixed emotions\": \"sadness\", \"dismay\": \"sadness\",\n",
    "    \"disquiet\": \"sadness\", \"disturbance\": \"sadness\",\n",
    "    \"guilt\": \"sadness\",\n",
    "    \"suffering\": \"sadness\", \"disappointment\": \"sadness\", \"shame\": \"sadness\",\n",
    "    \"neglect\": \"sadness\", \"sympathy\": \"sadness\", \"heartbreak\": \"sadness\",\n",
    "    \"pain\": \"sadness\", \"grief\": \"sadness\", \"grieving\": \"sadness\",\n",
    "    \"hurt\": \"sadness\", \"loneliness\": \"sadness\", \"despondency\": \"sadness\",\n",
    "    \"helplessness\": \"sadness\", \"exhaustion\": \"sadness\",\n",
    "\n",
    "    # fear\n",
    "    \"fear\": \"fear\",\n",
    "    \"terror\": \"fear\", \"doubt\": \"fear\",\n",
    "    \"alarm\": \"fear\", \"anxiety\": \"fear\", \"insecurity\": \"fear\",\n",
    "    \"panic\": \"fear\", \"dread\": \"fear\", \"overwhelming\": \"fear\",\n",
    "    \"overwhelmed\": \"fear\", \"horror\": \"fear\", \"shock\": \"fear\",\n",
    "\n",
    "    # surprise\n",
    "    \"surprise\": \"surprise\",\n",
    "    \"confusion\": \"surprise\", \"lightbulb moment\": \"surprise\",\n",
    "    \"amazement\": \"surprise\", \"wonder\": \"surprise\"\n",
    "}\n",
    "\n",
    "def map_emotions(emotion_str):\n",
    "    \"\"\"æŠŠæƒ…ç·’æ”¶æ–‚æˆå…­å¤§åŸºæœ¬æƒ…ç·’ï¼Œå…¶ä»–æ­¸ç‚º cannot be inferredï¼Œå…¨å°å¯«\"\"\"\n",
    "    if pd.isna(emotion_str):\n",
    "        return \"cannot be inferred\"\n",
    "    emotions = [e.strip().lower() for e in emotion_str.split(\"|\")]\n",
    "    mapped = [emotion_map.get(e, \"cannot be inferred\") for e in emotions]\n",
    "    mapped = list(dict.fromkeys(mapped))  # å»é‡ä½†ä¿ç•™é †åº\n",
    "    return \" | \".join(mapped)\n",
    "\n",
    "# å»ºç«‹æ–°çš„æ¬„ä½\n",
    "df[\"emotion\"] = df[\"emotion\"].apply(map_emotions)\n",
    "\n",
    "# è¼¸å‡ºçµæœ\n",
    "df.to_csv(\"step4_all_with_date_gemini.csv\", index=False)\n",
    "print(\"âœ… å·²å®Œæˆï¼šemotion å…¨éƒ¨è½‰æˆå°å¯« (love, joy, anger, sadness, fear, surprise, cannot be inferred)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
