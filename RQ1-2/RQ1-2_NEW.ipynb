{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73242df9-24a6-408b-ac20-ca77f832645e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "925aefd2-80c7-4d84-9617-3b2f1f26cc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 CSV 中的文章編號：\n",
      "['Article_1', 'Article_2', 'Article_3', 'Article_4', 'Article_5', 'Article_6', 'Article_8', 'Article_9', 'Article_10', 'Article_11', 'Article_12', 'Article_13', 'Article_14', 'Article_15', 'Article_16', 'Article_17', 'Article_18', 'Article_19', 'Article_20', 'Article_21', 'Article_22', 'Article_23', 'Article_24', 'Article_25', 'Article_26', 'Article_27', 'Article_28', 'Article_30', 'Article_31', 'Article_32', 'Article_33', 'Article_34', 'Article_35', 'Article_36', 'Article_37', 'Article_38', 'Article_39', 'Article_40', 'Article_42', 'Article_43', 'Article_44', 'Article_45', 'Article_46', 'Article_47', 'Article_48', 'Article_49', 'Article_50', 'Article_52', 'Article_53', 'Article_56', 'Article_57', 'Article_58', 'Article_59', 'Article_60', 'Article_61', 'Article_62', 'Article_65', 'Article_66', 'Article_67', 'Article_68', 'Article_69', 'Article_70', 'Article_71', 'Article_72', 'Article_73', 'Article_74', 'Article_75', 'Article_76', 'Article_79', 'Article_80', 'Article_81', 'Article_82', 'Article_83', 'Article_84', 'Article_85', 'Article_86', 'Article_87', 'Article_88', 'Article_89', 'Article_90', 'Article_91', 'Article_92', 'Article_93', 'Article_94', 'Article_96', 'Article_97', 'Article_98', 'Article_99', 'Article_100', 'Article_101', 'Article_102', 'Article_103', 'Article_104', 'Article_105', 'Article_107', 'Article_108', 'Article_109', 'Article_110', 'Article_112', 'Article_113', 'Article_114', 'Article_115', 'Article_116', 'Article_117', 'Article_118', 'Article_119', 'Article_120', 'Article_121', 'Article_122', 'Article_123', 'Article_124', 'Article_125', 'Article_126', 'Article_127', 'Article_128', 'Article_129', 'Article_130', 'Article_131', 'Article_132', 'Article_133', 'Article_134', 'Article_136', 'Article_137', 'Article_139', 'Article_140', 'Article_141', 'Article_142', 'Article_143', 'Article_144', 'Article_145', 'Article_147', 'Article_148', 'Article_149', 'Article_150', 'Article_151', 'Article_152', 'Article_153', 'Article_154', 'Article_155', 'Article_156', 'Article_157', 'Article_158', 'Article_159', 'Article_160', 'Article_161', 'Article_162', 'Article_163', 'Article_164', 'Article_165', 'Article_166', 'Article_167', 'Article_168', 'Article_169', 'Article_170', 'Article_171', 'Article_172', 'Article_173', 'Article_174', 'Article_175', 'Article_176', 'Article_177', 'Article_178', 'Article_179', 'Article_180', 'Article_181', 'Article_182', 'Article_183', 'Article_184', 'Article_185', 'Article_186', 'Article_187', 'Article_188', 'Article_189', 'Article_190', 'Article_191', 'Article_192', 'Article_193', 'Article_194', 'Article_195', 'Article_196', 'Article_197', 'Article_198', 'Article_199', 'Article_200', 'Article_201', 'Article_202', 'Article_203', 'Article_204', 'Article_205', 'Article_206', 'Article_207', 'Article_208', 'Article_209', 'Article_210', 'Article_211', 'Article_212', 'Article_213', 'Article_214', 'Article_216', 'Article_217', 'Article_218', 'Article_219', 'Article_220', 'Article_221', 'Article_222', 'Article_223', 'Article_224', 'Article_225', 'Article_226', 'Article_227', 'Article_228', 'Article_230', 'Article_231', 'Article_232', 'Article_233', 'Article_235', 'Article_236', 'Article_237', 'Article_238', 'Article_239', 'Article_240', 'Article_241', 'Article_242', 'Article_243', 'Article_244', 'Article_245', 'Article_246', 'Article_247', 'Article_248', 'Article_249', 'Article_250', 'Article_251', 'Article_252', 'Article_253', 'Article_255', 'Article_256', 'Article_257', 'Article_258', 'Article_259', 'Article_260', 'Article_262', 'Article_263', 'Article_264', 'Article_265', 'Article_266', 'Article_267', 'Article_268', 'Article_270', 'Article_272', 'Article_273', 'Article_274', 'Article_275', 'Article_276', 'Article_277', 'Article_278', 'Article_279', 'Article_280', 'Article_281', 'Article_282', 'Article_283', 'Article_284', 'Article_285', 'Article_286', 'Article_288', 'Article_290', 'Article_291', 'Article_292', 'Article_293', 'Article_294', 'Article_296', 'Article_297', 'Article_298', 'Article_299', 'Article_300', 'Article_301', 'Article_302', 'Article_303', 'Article_304', 'Article_305', 'Article_306', 'Article_307', 'Article_308', 'Article_309', 'Article_310', 'Article_311', 'Article_312', 'Article_313', 'Article_314', 'Article_315', 'Article_316', 'Article_317', 'Article_318', 'Article_319', 'Article_320', 'Article_321', 'Article_322', 'Article_323', 'Article_324', 'Article_325', 'Article_326', 'Article_327', 'Article_328', 'Article_329', 'Article_330', 'Article_331', 'Article_332', 'Article_333', 'Article_334', 'Article_335', 'Article_336', 'Article_337', 'Article_338', 'Article_339', 'Article_340', 'Article_341', 'Article_342', 'Article_343', 'Article_344', 'Article_345', 'Article_346', 'Article_347', 'Article_348', 'Article_349', 'Article_350', 'Article_351', 'Article_352', 'Article_355', 'Article_356', 'Article_357', 'Article_358', 'Article_359', 'Article_360', 'Article_361', 'Article_363', 'Article_364', 'Article_365', 'Article_366', 'Article_367', 'Article_368', 'Article_369', 'Article_370', 'Article_371', 'Article_373', 'Article_374', 'Article_375', 'Article_376', 'Article_377', 'Article_379', 'Article_380', 'Article_381', 'Article_382', 'Article_383', 'Article_384', 'Article_385', 'Article_386', 'Article_387', 'Article_388', 'Article_389', 'Article_390', 'Article_391', 'Article_392', 'Article_393', 'Article_394', 'Article_395', 'Article_396', 'Article_397', 'Article_398', 'Article_399', 'Article_400', 'Article_401', 'Article_402', 'Article_403', 'Article_404', 'Article_405', 'Article_406', 'Article_409', 'Article_410', 'Article_411', 'Article_412', 'Article_413', 'Article_414', 'Article_415', 'Article_416', 'Article_417', 'Article_418', 'Article_419', 'Article_420', 'Article_421', 'Article_422', 'Article_423', 'Article_424', 'Article_425', 'Article_426', 'Article_427', 'Article_428', 'Article_429', 'Article_430', 'Article_431', 'Article_433', 'Article_434', 'Article_435', 'Article_436', 'Article_437', 'Article_438', 'Article_439', 'Article_440', 'Article_441', 'Article_442', 'Article_443', 'Article_445', 'Article_446', 'Article_448', 'Article_449', 'Article_450', 'Article_451', 'Article_452', 'Article_453', 'Article_454', 'Article_455', 'Article_456', 'Article_457', 'Article_458', 'Article_459', 'Article_460', 'Article_461', 'Article_463', 'Article_465', 'Article_466', 'Article_467', 'Article_468', 'Article_469', 'Article_470', 'Article_471', 'Article_473', 'Article_474', 'Article_475', 'Article_476', 'Article_477', 'Article_478', 'Article_479', 'Article_480', 'Article_481', 'Article_483', 'Article_484', 'Article_485', 'Article_486', 'Article_487', 'Article_488', 'Article_489', 'Article_490', 'Article_492', 'Article_493', 'Article_494', 'Article_495', 'Article_496', 'Article_499', 'Article_500', 'Article_501', 'Article_503', 'Article_504', 'Article_505', 'Article_506', 'Article_507', 'Article_508', 'Article_510', 'Article_511', 'Article_512', 'Article_513', 'Article_514', 'Article_515', 'Article_516', 'Article_517', 'Article_518', 'Article_519', 'Article_520', 'Article_521', 'Article_522', 'Article_523', 'Article_524', 'Article_525', 'Article_526', 'Article_527', 'Article_528', 'Article_529', 'Article_530', 'Article_531', 'Article_532', 'Article_533', 'Article_534', 'Article_535', 'Article_536', 'Article_537', 'Article_538', 'Article_539', 'Article_540', 'Article_541', 'Article_542', 'Article_543', 'Article_544', 'Article_545', 'Article_546', 'Article_547', 'Article_548', 'Article_549', 'Article_550', 'Article_551', 'Article_552', 'Article_553', 'Article_554', 'Article_555', 'Article_556', 'Article_557', 'Article_558', 'Article_559', 'Article_560', 'Article_561', 'Article_562', 'Article_563', 'Article_564', 'Article_565', 'Article_566', 'Article_567', 'Article_568', 'Article_569', 'Article_570', 'Article_571', 'Article_572', 'Article_573', 'Article_574', 'Article_575', 'Article_576', 'Article_577', 'Article_578', 'Article_579', 'Article_580', 'Article_581', 'Article_582', 'Article_583', 'Article_584', 'Article_585', 'Article_586', 'Article_587', 'Article_588', 'Article_589', 'Article_591', 'Article_592', 'Article_593', 'Article_594', 'Article_596', 'Article_597', 'Article_598', 'Article_599', 'Article_600', 'Article_601', 'Article_602', 'Article_603', 'Article_604', 'Article_605', 'Article_606', 'Article_607', 'Article_609', 'Article_610', 'Article_611', 'Article_613', 'Article_614', 'Article_616', 'Article_617', 'Article_618', 'Article_619', 'Article_620', 'Article_621', 'Article_622', 'Article_623', 'Article_624', 'Article_625', 'Article_626', 'Article_627', 'Article_628', 'Article_629', 'Article_630', 'Article_631', 'Article_632', 'Article_633', 'Article_634']\n",
      "✅ 總共有 584 篇文章\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from typing import Dict, Any\n",
    "from openai import OpenAI\n",
    "\n",
    "# ==============================\n",
    "# OpenAI 設定與呼叫\n",
    "# ==============================\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "def get_response(prompt: str, temperature: float = 0.0) -> str:\n",
    "    \"\"\"呼叫 OpenAI API，回傳模型輸出（應該是 JSON 字串）\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperature,\n",
    "            stream=False,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"API 調用錯誤: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# ==============================\n",
    "# 讀取文章（只為了保留順序，不丟進模型）\n",
    "# ==============================\n",
    "import pandas as pd\n",
    "\n",
    "# 讀取 CSV\n",
    "df = pd.read_csv(\"./articles_584.csv\", encoding=\"ISO-8859-1\")\n",
    "\n",
    "# 只保留有文章的資料\n",
    "df = df.dropna(subset=[\"ARTICLE_TEXT\"])\n",
    "\n",
    "# 確保 id 欄位是整數型態（避免後面拼接出問題）\n",
    "df[\"id\"] = df[\"id\"].astype(int)\n",
    "\n",
    "# 建立 dict：用 CSV 裡的 id 當編號\n",
    "articles = {f\"Article_{row['id']}\": row[\"ARTICLE_TEXT\"] for _, row in df.iterrows()}\n",
    "\n",
    "# 列出全部文章編號\n",
    "all_articles = list(articles.keys())\n",
    "print(\"📄 CSV 中的文章編號：\")\n",
    "print(all_articles)  # 先只印前 20 筆\n",
    "print(f\"✅ 總共有 {len(all_articles)} 篇文章\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c453f1f-c7bb-4338-8446-6a9e374f52db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 實際文章數量: 584 / 584\n",
      "❌ 缺少的 Article 編號（共 45 篇）:\n",
      "Article_7, Article_29, Article_41, Article_51, Article_54, Article_55, Article_63, Article_64, Article_77, Article_78, Article_95, Article_106, Article_111, Article_135, Article_138, Article_146, Article_215, Article_229, Article_234, Article_254, Article_261, Article_269, Article_271, Article_287, Article_289, Article_295, Article_353, Article_354, Article_362, Article_372, Article_378, Article_407, Article_408, Article_432, Article_444, Article_447, Article_462, Article_464, Article_472, Article_482, Article_491, Article_497, Article_498, Article_502, Article_509\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 輸入你的 CSV 檔案名稱\n",
    "csv_file = \"articles_584.csv\"\n",
    "\n",
    "# 假設你知道總共應該有多少篇（例：584）\n",
    "expected_total = 584\n",
    "\n",
    "# 讀取 CSV\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# 取得實際出現過的 id（轉成 int）\n",
    "existing_numbers = set(df[\"id\"].dropna().astype(int))\n",
    "\n",
    "# 建立完整應有的 ID 集合\n",
    "# ⚠️ 假設 id 從 1 開始編號\n",
    "expected_numbers = set(range(1, expected_total + 1))\n",
    "\n",
    "# 找出缺少的編號\n",
    "missing = sorted(expected_numbers - existing_numbers)\n",
    "\n",
    "print(f\"✅ 實際文章數量: {len(existing_numbers)} / {expected_total}\")\n",
    "print(f\"❌ 缺少的 Article 編號（共 {len(missing)} 篇）:\")\n",
    "print(\", \".join(f\"Article_{i}\" for i in missing))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4936bcaa-d529-457a-92ad-55356de3f848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 開始處理批次 1/12...\n",
      "✅ 批次 1/12 已完成，儲存至 step1_batches\\step1_batch_1.json\n",
      "\n",
      "🚀 開始處理批次 2/12...\n",
      "✅ 批次 2/12 已完成，儲存至 step1_batches\\step1_batch_2.json\n",
      "\n",
      "🚀 開始處理批次 3/12...\n",
      "✅ 批次 3/12 已完成，儲存至 step1_batches\\step1_batch_3.json\n",
      "\n",
      "🚀 開始處理批次 4/12...\n",
      "✅ 批次 4/12 已完成，儲存至 step1_batches\\step1_batch_4.json\n",
      "\n",
      "🚀 開始處理批次 5/12...\n",
      "✅ 批次 5/12 已完成，儲存至 step1_batches\\step1_batch_5.json\n",
      "\n",
      "🚀 開始處理批次 6/12...\n",
      "✅ 批次 6/12 已完成，儲存至 step1_batches\\step1_batch_6.json\n",
      "\n",
      "🚀 開始處理批次 7/12...\n",
      "✅ 批次 7/12 已完成，儲存至 step1_batches\\step1_batch_7.json\n",
      "\n",
      "🚀 開始處理批次 8/12...\n",
      "✅ 批次 8/12 已完成，儲存至 step1_batches\\step1_batch_8.json\n",
      "\n",
      "🚀 開始處理批次 9/12...\n",
      "✅ 批次 9/12 已完成，儲存至 step1_batches\\step1_batch_9.json\n",
      "\n",
      "🚀 開始處理批次 10/12...\n",
      "✅ 批次 10/12 已完成，儲存至 step1_batches\\step1_batch_10.json\n",
      "\n",
      "🚀 開始處理批次 11/12...\n",
      "✅ 批次 11/12 已完成，儲存至 step1_batches\\step1_batch_11.json\n",
      "\n",
      "🚀 開始處理批次 12/12...\n",
      "✅ 批次 12/12 已完成，儲存至 step1_batches\\step1_batch_12.json\n",
      "\n",
      "🎉 所有批次處理完成！\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Colab 程式碼區塊 1: 初始化和步驟1\n",
    "# ==========================================\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Step 1 的提示詞\n",
    "step1_prompt = \"\"\"You are a sociology professor with 30 years of experience analyzing Asian racism in the United States.\n",
    "Your task is to analyze the following article by identifying named entities and classifying them into the appropriate social roles and institutional categories. Focus especially on people or groups reacting to or affected by anti-Asian incidents.\n",
    "\n",
    "Step 1: Named Entity Recognition and Categorization\n",
    "\n",
    "1. Identify named entities.\n",
    "2. Classify them into appropriate social roles or institutional categories.\n",
    "3. Determine whether each **individual** is **Asian**, **Non-Asian**, or **Cannot be inferred** based on the text.\n",
    "4. For each entity, include a `\"reference\"` field that reflects **the exact phrase or wording** used in the article to refer to them.\n",
    "\n",
    "Please extract all named entities from the text and categorize them according to the following schema:\n",
    "\n",
    "---\n",
    "\n",
    "**INDIVIDUALS** (Specific persons or actors representing individual agency)\n",
    "\n",
    "1. politicians\n",
    "   - Elected officials acting in an individual capacity.\n",
    "   - Examples: senators, representatives, mayors, governors\n",
    "\n",
    "2. professionals\n",
    "   - Individuals recognized by their expertise or institutional role.\n",
    "   - Examples: professors, doctors, lawyers, foundation presidents\n",
    "\n",
    "3. celebrities\n",
    "   - Public figures in entertainment or sports (e.g., actors, athletes) unless clearly acting in a professional or political role.\n",
    "   - If overlapping with another role, assign to the more institutionally grounded category.\n",
    "\n",
    "4. perpetrators\n",
    "   - Individuals directly identified as committing or responsible for anti-Asian actions.\n",
    "   - Do not include vague or generalized public unless clearly specified.\n",
    "\n",
    "5. victims\n",
    "   - Individuals or racial/ethnic groups explicitly targeted by anti-Asian acts.\n",
    "   - Examples: “a woman attacked on the subway,” “Japanese Americans during WWII”\n",
    "\n",
    "6. other_individuals\n",
    "   - All other named or unnamed individuals who do not fall into the above categories.\n",
    "   - Includes the general public, community members, business owners, or relatives (e.g., “my mom,” “a neighbor”).\n",
    "\n",
    "---\n",
    "\n",
    "**ORGANIZATIONS** (Named institutions or collectives)\n",
    "\n",
    "1. law_enforcement_agencies\n",
    "   - Official police or investigative institutions.\n",
    "   - Examples: Chicago Police Department, FBI, local sheriff’s office\n",
    "\n",
    "2. government_bodies\n",
    "   - Government agencies, departments, or offices at any level (local/state/federal).\n",
    "   - Examples: CDC, Department of Justice, City Council\n",
    "\n",
    "3. ngo_or_advocacy_groups\n",
    "   - Civil rights organizations, foundations, or advocacy nonprofits.\n",
    "   - Examples: Stop AAPI Hate, Robert Wood Johnson Foundation\n",
    "\n",
    "4. business_entities\n",
    "   - Named companies, hotels, restaurants, or stores.\n",
    "   - Examples: Wrap-on Tools, Edgewater Beach Hotel\n",
    "\n",
    "5. community_groups\n",
    "   - Named cultural, ethnic, or neighborhood associations.\n",
    "   - Examples: Chinatown Association, Asian-American Coalition\n",
    "\n",
    "---\n",
    "\n",
    "**ETHNICITY INFERENCE RULES:**\n",
    "\n",
    "- For each **individual**, determine whether they are **Asian**, **Non-Asian**, or **Cannot be inferred**.\n",
    "- Use contextual clues such as ethnicity indicators, names, or explicit mentions.\n",
    "- If ethnicity is ambiguous or not stated, return `\"Cannot be inferred\"`.\n",
    "\n",
    "---\n",
    "\n",
    "**ADDITIONAL INSTRUCTIONS:**\n",
    "\n",
    "- Use `\"reference\"` to capture how the person/group was referred to in the original article (e.g., `\"an 80-year-old woman\"`, `\"Lee\"`, `\"the attacker\"`).\n",
    "- Normalize all name variants to a canonical form (e.g., “Dr. Church,” “J. Church,” and “Church” → “Jacqueline Church”).\n",
    "- If an individual belongs to multiple roles, assign them to the most institutionally specific one (e.g., categorize a lawyer-celebrity as a professional).\n",
    "- Include only individuals explicitly involved in specific incidents under “victims” and “perpetrators.”\n",
    "- Do not classify individual police officers or sheriffs as individuals—assign them under law_enforcement_agencies.\n",
    "- Classify individual owners under “business_actors” and company names under “business_entities.”\n",
    "\n",
    "---\n",
    "\n",
    "**Output format (in JSON):**\n",
    "\n",
    "For all individuals, return an object with \"name\" and \"asian_status\" fields.\n",
    "For all organizations, return an object with \"name\" and \"asian_status\": \"Not applicable\".\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"individuals\": {\n",
    "    \"politicians\": [\n",
    "      {\n",
    "        \"name\": \"Tammy Duckworth\",\n",
    "        \"reference\": \"Senator Tammy Duckworth\",\n",
    "        \"asian_status\": \"Asian\"\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"Joe Biden\",\n",
    "        \"reference\": \"President Joe Biden\",\n",
    "        \"asian_status\": \"Non-Asian\"\n",
    "      }\n",
    "    ],\n",
    "    \"professionals\": [\n",
    "      {\n",
    "        \"name\": \"Julie Morita\",\n",
    "        \"reference\": \"Julie Morita\",\n",
    "        \"asian_status\": \"Asian\"\n",
    "      }\n",
    "    ],\n",
    "    \"celebrities\": [\n",
    "      {\n",
    "        \"name\": \"Awkwafina\",\n",
    "        \"reference\": \"Awkwafina\",\n",
    "        \"asian_status\": \"Asian\"\n",
    "      }\n",
    "    ],\n",
    "    \"perpetrators\": [\n",
    "      {\n",
    "        \"name\": \"Unknown Attacker\",\n",
    "        \"reference\": \"the attacker\",\n",
    "        \"asian_status\": \"Non-Asian\"\n",
    "      }\n",
    "    ],\n",
    "    \"victims\": [\n",
    "      {\n",
    "        \"name\": \"Asian Elderly Woman\",\n",
    "        \"reference\": \"an 80-year-old woman\",\n",
    "        \"asian_status\": \"Asian\"\n",
    "      }\n",
    "    ],\n",
    "    \"other_individuals\": [\n",
    "      {\n",
    "        \"name\": \"my mom\",\n",
    "        \"reference\": \"my mom\",\n",
    "        \"asian_status\": \"Asian\"\n",
    "      }\n",
    "    ]\n",
    "    \n",
    "  },\n",
    "  \"organizations\": {\n",
    "    \"law_enforcement_agencies\": [\n",
    "      {\n",
    "        \"name\": \"Chicago Police Department\",\n",
    "        \"reference\": \"Chicago Police Department\",\n",
    "        \"asian_status\": \"Not applicable\"\n",
    "      }\n",
    "    ],\n",
    "    \"government_bodies\": [\n",
    "      {\n",
    "        \"name\": \"City Council\",\n",
    "        \"reference\": \"City Council\",\n",
    "        \"asian_status\": \"Not applicable\"\n",
    "      }\n",
    "    ],\n",
    "    \"ngo_or_advocacy_groups\": [\n",
    "      {\n",
    "        \"name\": \"Stop AAPI Hate\",\n",
    "        \"reference\": \"Stop AAPI Hate\",\n",
    "        \"asian_status\": \"Not applicable\"\n",
    "      }\n",
    "    ],\n",
    "    \"business_entities\": [\n",
    "      {\n",
    "        \"name\": \"Edgewater Beach Hotel\",\n",
    "        \"reference\": \"Edgewater Beach Hotel\",\n",
    "        \"asian_status\": \"Not applicable\"\n",
    "      }\n",
    "    ],\n",
    "    \"community_groups\": [\n",
    "      {\n",
    "        \"name\": \"Chinatown Association\",\n",
    "        \"reference\": \"Chinatown Association\",\n",
    "        \"asian_status\": \"Not applicable\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 呼叫 Step 1\n",
    "\n",
    "# 建立輸出資料夾\n",
    "output_dir = \"step1_batches\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 每批文章數量\n",
    "batch_size = 50\n",
    "article_items = list(articles.items())\n",
    "total_articles = len(article_items)\n",
    "total_batches = (total_articles + batch_size - 1) // batch_size  # 無條件進位\n",
    "\n",
    "# 逐批處理\n",
    "for batch_idx in range(0, total_articles, batch_size):\n",
    "    batch_number = batch_idx // batch_size + 1\n",
    "    filename = os.path.join(output_dir, f\"step1_batch_{batch_number}.json\")\n",
    "\n",
    "    # 如果檔案已經存在，就跳過這批\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"⏭️ 批次 {batch_number}/{total_batches} 已存在，跳過\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n🚀 開始處理批次 {batch_number}/{total_batches}...\")\n",
    "\n",
    "    step1_result = {}\n",
    "    batch = article_items[batch_idx: batch_idx + batch_size]\n",
    "\n",
    "    for title, content in batch:\n",
    "        full_prompt = step1_prompt + \"\\n\\nArticle Text:\\n\" + content\n",
    "        response = get_response(full_prompt)\n",
    "        step1_result[title] = response\n",
    "\n",
    "    # 儲存這一批結果\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(step1_result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"✅ 批次 {batch_number}/{total_batches} 已完成，儲存至 {filename}\")\n",
    "\n",
    "print(\"\\n🎉 所有批次處理完成！\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97f46f96-40d0-4d23-a71d-fdf5e7931277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 開始處理 step1_batches\\step1_batch_1.json → step2_batches\\step2_batch_1.json\n",
      "✔️ 已處理 Article_1\n",
      "✔️ 已處理 Article_2\n",
      "✔️ 已處理 Article_3\n",
      "✔️ 已處理 Article_4\n",
      "✔️ 已處理 Article_5\n",
      "✔️ 已處理 Article_6\n",
      "✔️ 已處理 Article_8\n",
      "✔️ 已處理 Article_9\n",
      "✔️ 已處理 Article_10\n",
      "✔️ 已處理 Article_11\n",
      "✔️ 已處理 Article_12\n",
      "✔️ 已處理 Article_13\n",
      "✔️ 已處理 Article_14\n",
      "✔️ 已處理 Article_15\n",
      "✔️ 已處理 Article_16\n",
      "✔️ 已處理 Article_17\n",
      "✔️ 已處理 Article_18\n",
      "✔️ 已處理 Article_19\n",
      "✔️ 已處理 Article_20\n",
      "✔️ 已處理 Article_21\n",
      "✔️ 已處理 Article_22\n",
      "✔️ 已處理 Article_23\n",
      "✔️ 已處理 Article_24\n",
      "✔️ 已處理 Article_25\n",
      "✔️ 已處理 Article_26\n",
      "✔️ 已處理 Article_27\n",
      "✔️ 已處理 Article_28\n",
      "✔️ 已處理 Article_30\n",
      "✔️ 已處理 Article_31\n",
      "✔️ 已處理 Article_32\n",
      "✔️ 已處理 Article_33\n",
      "✔️ 已處理 Article_34\n",
      "✔️ 已處理 Article_35\n",
      "✔️ 已處理 Article_36\n",
      "✔️ 已處理 Article_37\n",
      "✔️ 已處理 Article_38\n",
      "✔️ 已處理 Article_39\n",
      "✔️ 已處理 Article_40\n",
      "✔️ 已處理 Article_42\n",
      "✔️ 已處理 Article_43\n",
      "✔️ 已處理 Article_44\n",
      "✔️ 已處理 Article_45\n",
      "✔️ 已處理 Article_46\n",
      "✔️ 已處理 Article_47\n",
      "✔️ 已處理 Article_48\n",
      "✔️ 已處理 Article_49\n",
      "✔️ 已處理 Article_50\n",
      "✔️ 已處理 Article_52\n",
      "✔️ 已處理 Article_53\n",
      "✔️ 已處理 Article_56\n",
      "✅ step2_batch_1.json 已完成並儲存至 step2_batches\\step2_batch_1.json\n",
      "\n",
      "🚀 開始處理 step1_batches\\step1_batch_10.json → step2_batches\\step2_batch_10.json\n",
      "✔️ 已處理 Article_492\n",
      "✔️ 已處理 Article_493\n",
      "✔️ 已處理 Article_494\n",
      "✔️ 已處理 Article_495\n",
      "✔️ 已處理 Article_496\n",
      "✔️ 已處理 Article_499\n",
      "✔️ 已處理 Article_500\n",
      "✔️ 已處理 Article_501\n",
      "✔️ 已處理 Article_503\n",
      "✔️ 已處理 Article_504\n",
      "✔️ 已處理 Article_505\n",
      "✔️ 已處理 Article_506\n",
      "✔️ 已處理 Article_507\n",
      "✔️ 已處理 Article_508\n",
      "✔️ 已處理 Article_510\n",
      "✔️ 已處理 Article_511\n",
      "✔️ 已處理 Article_512\n",
      "✔️ 已處理 Article_513\n",
      "✔️ 已處理 Article_514\n",
      "✔️ 已處理 Article_515\n",
      "✔️ 已處理 Article_516\n",
      "✔️ 已處理 Article_517\n",
      "✔️ 已處理 Article_518\n",
      "✔️ 已處理 Article_519\n",
      "✔️ 已處理 Article_520\n",
      "✔️ 已處理 Article_521\n",
      "✔️ 已處理 Article_522\n",
      "✔️ 已處理 Article_523\n",
      "✔️ 已處理 Article_524\n",
      "✔️ 已處理 Article_525\n",
      "✔️ 已處理 Article_526\n",
      "✔️ 已處理 Article_527\n",
      "✔️ 已處理 Article_528\n",
      "✔️ 已處理 Article_529\n",
      "✔️ 已處理 Article_530\n",
      "✔️ 已處理 Article_531\n",
      "✔️ 已處理 Article_532\n",
      "✔️ 已處理 Article_533\n",
      "✔️ 已處理 Article_534\n",
      "✔️ 已處理 Article_535\n",
      "✔️ 已處理 Article_536\n",
      "✔️ 已處理 Article_537\n",
      "✔️ 已處理 Article_538\n",
      "✔️ 已處理 Article_539\n",
      "✔️ 已處理 Article_540\n",
      "✔️ 已處理 Article_541\n",
      "✔️ 已處理 Article_542\n",
      "✔️ 已處理 Article_543\n",
      "✔️ 已處理 Article_544\n",
      "✔️ 已處理 Article_545\n",
      "✅ step2_batch_10.json 已完成並儲存至 step2_batches\\step2_batch_10.json\n",
      "\n",
      "🚀 開始處理 step1_batches\\step1_batch_11.json → step2_batches\\step2_batch_11.json\n",
      "✔️ 已處理 Article_546\n",
      "✔️ 已處理 Article_547\n",
      "✔️ 已處理 Article_548\n",
      "✔️ 已處理 Article_549\n",
      "✔️ 已處理 Article_550\n",
      "✔️ 已處理 Article_551\n",
      "✔️ 已處理 Article_552\n",
      "✔️ 已處理 Article_553\n",
      "✔️ 已處理 Article_554\n",
      "✔️ 已處理 Article_555\n",
      "✔️ 已處理 Article_556\n",
      "✔️ 已處理 Article_557\n",
      "✔️ 已處理 Article_558\n",
      "✔️ 已處理 Article_559\n",
      "✔️ 已處理 Article_560\n",
      "✔️ 已處理 Article_561\n",
      "✔️ 已處理 Article_562\n",
      "✔️ 已處理 Article_563\n",
      "✔️ 已處理 Article_564\n",
      "✔️ 已處理 Article_565\n",
      "✔️ 已處理 Article_566\n",
      "✔️ 已處理 Article_567\n",
      "✔️ 已處理 Article_568\n",
      "✔️ 已處理 Article_569\n",
      "✔️ 已處理 Article_570\n",
      "✔️ 已處理 Article_571\n",
      "✔️ 已處理 Article_572\n",
      "✔️ 已處理 Article_573\n",
      "✔️ 已處理 Article_574\n",
      "✔️ 已處理 Article_575\n",
      "✔️ 已處理 Article_576\n",
      "✔️ 已處理 Article_577\n",
      "✔️ 已處理 Article_578\n",
      "✔️ 已處理 Article_579\n",
      "✔️ 已處理 Article_580\n",
      "✔️ 已處理 Article_581\n",
      "✔️ 已處理 Article_582\n",
      "✔️ 已處理 Article_583\n",
      "✔️ 已處理 Article_584\n",
      "✔️ 已處理 Article_585\n",
      "✔️ 已處理 Article_586\n",
      "✔️ 已處理 Article_587\n",
      "✔️ 已處理 Article_588\n",
      "✔️ 已處理 Article_589\n",
      "✔️ 已處理 Article_591\n",
      "✔️ 已處理 Article_592\n",
      "✔️ 已處理 Article_593\n",
      "✔️ 已處理 Article_594\n",
      "✔️ 已處理 Article_596\n",
      "✔️ 已處理 Article_597\n",
      "✅ step2_batch_11.json 已完成並儲存至 step2_batches\\step2_batch_11.json\n",
      "\n",
      "🚀 開始處理 step1_batches\\step1_batch_12.json → step2_batches\\step2_batch_12.json\n",
      "✔️ 已處理 Article_598\n",
      "✔️ 已處理 Article_599\n",
      "✔️ 已處理 Article_600\n",
      "✔️ 已處理 Article_601\n",
      "✔️ 已處理 Article_602\n",
      "✔️ 已處理 Article_603\n",
      "✔️ 已處理 Article_604\n",
      "✔️ 已處理 Article_605\n",
      "✔️ 已處理 Article_606\n",
      "✔️ 已處理 Article_607\n",
      "✔️ 已處理 Article_609\n",
      "✔️ 已處理 Article_610\n",
      "✔️ 已處理 Article_611\n",
      "✔️ 已處理 Article_613\n",
      "✔️ 已處理 Article_614\n",
      "✔️ 已處理 Article_616\n",
      "✔️ 已處理 Article_617\n",
      "✔️ 已處理 Article_618\n",
      "✔️ 已處理 Article_619\n",
      "✔️ 已處理 Article_620\n",
      "✔️ 已處理 Article_621\n",
      "✔️ 已處理 Article_622\n",
      "✔️ 已處理 Article_623\n",
      "✔️ 已處理 Article_624\n",
      "✔️ 已處理 Article_625\n",
      "✔️ 已處理 Article_626\n",
      "✔️ 已處理 Article_627\n",
      "✔️ 已處理 Article_628\n",
      "✔️ 已處理 Article_629\n",
      "✔️ 已處理 Article_630\n",
      "✔️ 已處理 Article_631\n",
      "✔️ 已處理 Article_632\n",
      "✔️ 已處理 Article_633\n",
      "✔️ 已處理 Article_634\n",
      "✅ step2_batch_12.json 已完成並儲存至 step2_batches\\step2_batch_12.json\n",
      "\n",
      "🚀 開始處理 step1_batches\\step1_batch_2.json → step2_batches\\step2_batch_2.json\n",
      "✔️ 已處理 Article_57\n",
      "✔️ 已處理 Article_58\n",
      "✔️ 已處理 Article_59\n",
      "✔️ 已處理 Article_60\n",
      "✔️ 已處理 Article_61\n",
      "✔️ 已處理 Article_62\n",
      "✔️ 已處理 Article_65\n",
      "✔️ 已處理 Article_66\n",
      "✔️ 已處理 Article_67\n",
      "✔️ 已處理 Article_68\n",
      "✔️ 已處理 Article_69\n",
      "✔️ 已處理 Article_70\n",
      "✔️ 已處理 Article_71\n",
      "✔️ 已處理 Article_72\n",
      "✔️ 已處理 Article_73\n",
      "✔️ 已處理 Article_74\n",
      "✔️ 已處理 Article_75\n",
      "✔️ 已處理 Article_76\n",
      "✔️ 已處理 Article_79\n",
      "✔️ 已處理 Article_80\n",
      "✔️ 已處理 Article_81\n",
      "✔️ 已處理 Article_82\n",
      "✔️ 已處理 Article_83\n",
      "✔️ 已處理 Article_84\n",
      "✔️ 已處理 Article_85\n",
      "✔️ 已處理 Article_86\n",
      "✔️ 已處理 Article_87\n",
      "✔️ 已處理 Article_88\n",
      "✔️ 已處理 Article_89\n",
      "✔️ 已處理 Article_90\n",
      "✔️ 已處理 Article_91\n",
      "✔️ 已處理 Article_92\n",
      "✔️ 已處理 Article_93\n",
      "✔️ 已處理 Article_94\n",
      "✔️ 已處理 Article_96\n",
      "✔️ 已處理 Article_97\n",
      "✔️ 已處理 Article_98\n",
      "✔️ 已處理 Article_99\n",
      "✔️ 已處理 Article_100\n",
      "✔️ 已處理 Article_101\n",
      "✔️ 已處理 Article_102\n",
      "✔️ 已處理 Article_103\n",
      "✔️ 已處理 Article_104\n",
      "✔️ 已處理 Article_105\n",
      "✔️ 已處理 Article_107\n",
      "✔️ 已處理 Article_108\n",
      "✔️ 已處理 Article_109\n",
      "✔️ 已處理 Article_110\n",
      "✔️ 已處理 Article_112\n",
      "✔️ 已處理 Article_113\n",
      "✅ step2_batch_2.json 已完成並儲存至 step2_batches\\step2_batch_2.json\n",
      "\n",
      "🚀 開始處理 step1_batches\\step1_batch_3.json → step2_batches\\step2_batch_3.json\n",
      "✔️ 已處理 Article_114\n",
      "✔️ 已處理 Article_115\n",
      "✔️ 已處理 Article_116\n",
      "✔️ 已處理 Article_117\n",
      "✔️ 已處理 Article_118\n",
      "✔️ 已處理 Article_119\n",
      "✔️ 已處理 Article_120\n",
      "✔️ 已處理 Article_121\n",
      "✔️ 已處理 Article_122\n",
      "✔️ 已處理 Article_123\n",
      "✔️ 已處理 Article_124\n",
      "✔️ 已處理 Article_125\n",
      "✔️ 已處理 Article_126\n",
      "✔️ 已處理 Article_127\n",
      "✔️ 已處理 Article_128\n",
      "✔️ 已處理 Article_129\n",
      "✔️ 已處理 Article_130\n",
      "✔️ 已處理 Article_131\n",
      "✔️ 已處理 Article_132\n",
      "✔️ 已處理 Article_133\n",
      "✔️ 已處理 Article_134\n",
      "✔️ 已處理 Article_136\n",
      "✔️ 已處理 Article_137\n",
      "✔️ 已處理 Article_139\n",
      "✔️ 已處理 Article_140\n",
      "✔️ 已處理 Article_141\n",
      "✔️ 已處理 Article_142\n",
      "✔️ 已處理 Article_143\n",
      "✔️ 已處理 Article_144\n",
      "✔️ 已處理 Article_145\n",
      "✔️ 已處理 Article_147\n",
      "✔️ 已處理 Article_148\n",
      "✔️ 已處理 Article_149\n",
      "✔️ 已處理 Article_150\n",
      "✔️ 已處理 Article_151\n",
      "✔️ 已處理 Article_152\n",
      "✔️ 已處理 Article_153\n",
      "✔️ 已處理 Article_154\n",
      "✔️ 已處理 Article_155\n",
      "✔️ 已處理 Article_156\n",
      "✔️ 已處理 Article_157\n",
      "✔️ 已處理 Article_158\n",
      "✔️ 已處理 Article_159\n",
      "✔️ 已處理 Article_160\n",
      "✔️ 已處理 Article_161\n",
      "✔️ 已處理 Article_162\n",
      "✔️ 已處理 Article_163\n",
      "✔️ 已處理 Article_164\n",
      "✔️ 已處理 Article_165\n",
      "✔️ 已處理 Article_166\n",
      "✅ step2_batch_3.json 已完成並儲存至 step2_batches\\step2_batch_3.json\n",
      "\n",
      "🚀 開始處理 step1_batches\\step1_batch_4.json → step2_batches\\step2_batch_4.json\n",
      "✔️ 已處理 Article_167\n",
      "✔️ 已處理 Article_168\n",
      "✔️ 已處理 Article_169\n",
      "✔️ 已處理 Article_170\n",
      "✔️ 已處理 Article_171\n",
      "✔️ 已處理 Article_172\n",
      "✔️ 已處理 Article_173\n",
      "✔️ 已處理 Article_174\n",
      "✔️ 已處理 Article_175\n",
      "✔️ 已處理 Article_176\n",
      "✔️ 已處理 Article_177\n",
      "✔️ 已處理 Article_178\n",
      "✔️ 已處理 Article_179\n",
      "✔️ 已處理 Article_180\n",
      "✔️ 已處理 Article_181\n",
      "✔️ 已處理 Article_182\n",
      "✔️ 已處理 Article_183\n",
      "✔️ 已處理 Article_184\n",
      "✔️ 已處理 Article_185\n",
      "✔️ 已處理 Article_186\n",
      "✔️ 已處理 Article_187\n",
      "✔️ 已處理 Article_188\n",
      "✔️ 已處理 Article_189\n",
      "✔️ 已處理 Article_190\n",
      "✔️ 已處理 Article_191\n",
      "✔️ 已處理 Article_192\n",
      "✔️ 已處理 Article_193\n",
      "✔️ 已處理 Article_194\n",
      "✔️ 已處理 Article_195\n",
      "✔️ 已處理 Article_196\n",
      "✔️ 已處理 Article_197\n",
      "✔️ 已處理 Article_198\n",
      "✔️ 已處理 Article_199\n",
      "✔️ 已處理 Article_200\n",
      "✔️ 已處理 Article_201\n",
      "✔️ 已處理 Article_202\n",
      "✔️ 已處理 Article_203\n",
      "✔️ 已處理 Article_204\n",
      "✔️ 已處理 Article_205\n",
      "✔️ 已處理 Article_206\n",
      "✔️ 已處理 Article_207\n",
      "✔️ 已處理 Article_208\n",
      "✔️ 已處理 Article_209\n",
      "✔️ 已處理 Article_210\n",
      "✔️ 已處理 Article_211\n",
      "✔️ 已處理 Article_212\n",
      "✔️ 已處理 Article_213\n",
      "✔️ 已處理 Article_214\n",
      "✔️ 已處理 Article_216\n",
      "✔️ 已處理 Article_217\n",
      "✅ step2_batch_4.json 已完成並儲存至 step2_batches\\step2_batch_4.json\n",
      "\n",
      "🚀 開始處理 step1_batches\\step1_batch_5.json → step2_batches\\step2_batch_5.json\n",
      "✔️ 已處理 Article_218\n",
      "✔️ 已處理 Article_219\n",
      "✔️ 已處理 Article_220\n",
      "✔️ 已處理 Article_221\n",
      "✔️ 已處理 Article_222\n",
      "✔️ 已處理 Article_223\n",
      "✔️ 已處理 Article_224\n",
      "✔️ 已處理 Article_225\n",
      "✔️ 已處理 Article_226\n",
      "✔️ 已處理 Article_227\n",
      "✔️ 已處理 Article_228\n",
      "✔️ 已處理 Article_230\n",
      "✔️ 已處理 Article_231\n",
      "✔️ 已處理 Article_232\n",
      "✔️ 已處理 Article_233\n",
      "✔️ 已處理 Article_235\n",
      "✔️ 已處理 Article_236\n",
      "✔️ 已處理 Article_237\n",
      "✔️ 已處理 Article_238\n",
      "✔️ 已處理 Article_239\n",
      "✔️ 已處理 Article_240\n",
      "✔️ 已處理 Article_241\n",
      "✔️ 已處理 Article_242\n",
      "✔️ 已處理 Article_243\n",
      "✔️ 已處理 Article_244\n",
      "✔️ 已處理 Article_245\n",
      "✔️ 已處理 Article_246\n",
      "✔️ 已處理 Article_247\n",
      "✔️ 已處理 Article_248\n",
      "✔️ 已處理 Article_249\n",
      "✔️ 已處理 Article_250\n",
      "✔️ 已處理 Article_251\n",
      "✔️ 已處理 Article_252\n",
      "✔️ 已處理 Article_253\n",
      "✔️ 已處理 Article_255\n",
      "✔️ 已處理 Article_256\n",
      "✔️ 已處理 Article_257\n",
      "✔️ 已處理 Article_258\n",
      "✔️ 已處理 Article_259\n",
      "✔️ 已處理 Article_260\n",
      "✔️ 已處理 Article_262\n",
      "✔️ 已處理 Article_263\n",
      "✔️ 已處理 Article_264\n",
      "✔️ 已處理 Article_265\n",
      "✔️ 已處理 Article_266\n",
      "✔️ 已處理 Article_267\n",
      "✔️ 已處理 Article_268\n",
      "✔️ 已處理 Article_270\n",
      "✔️ 已處理 Article_272\n",
      "✔️ 已處理 Article_273\n",
      "✅ step2_batch_5.json 已完成並儲存至 step2_batches\\step2_batch_5.json\n",
      "\n",
      "🚀 開始處理 step1_batches\\step1_batch_6.json → step2_batches\\step2_batch_6.json\n",
      "✔️ 已處理 Article_274\n",
      "✔️ 已處理 Article_275\n",
      "✔️ 已處理 Article_276\n",
      "✔️ 已處理 Article_277\n",
      "✔️ 已處理 Article_278\n",
      "✔️ 已處理 Article_279\n",
      "✔️ 已處理 Article_280\n",
      "✔️ 已處理 Article_281\n",
      "✔️ 已處理 Article_282\n",
      "✔️ 已處理 Article_283\n",
      "✔️ 已處理 Article_284\n",
      "✔️ 已處理 Article_285\n",
      "✔️ 已處理 Article_286\n",
      "✔️ 已處理 Article_288\n",
      "✔️ 已處理 Article_290\n",
      "✔️ 已處理 Article_291\n",
      "✔️ 已處理 Article_292\n",
      "✔️ 已處理 Article_293\n",
      "✔️ 已處理 Article_294\n",
      "✔️ 已處理 Article_296\n",
      "✔️ 已處理 Article_297\n",
      "✔️ 已處理 Article_298\n",
      "✔️ 已處理 Article_299\n",
      "✔️ 已處理 Article_300\n",
      "✔️ 已處理 Article_301\n",
      "✔️ 已處理 Article_302\n",
      "✔️ 已處理 Article_303\n",
      "✔️ 已處理 Article_304\n",
      "✔️ 已處理 Article_305\n",
      "✔️ 已處理 Article_306\n",
      "✔️ 已處理 Article_307\n",
      "✔️ 已處理 Article_308\n",
      "✔️ 已處理 Article_309\n",
      "✔️ 已處理 Article_310\n",
      "✔️ 已處理 Article_311\n",
      "✔️ 已處理 Article_312\n",
      "✔️ 已處理 Article_313\n",
      "✔️ 已處理 Article_314\n",
      "✔️ 已處理 Article_315\n",
      "✔️ 已處理 Article_316\n",
      "✔️ 已處理 Article_317\n",
      "✔️ 已處理 Article_318\n",
      "✔️ 已處理 Article_319\n",
      "✔️ 已處理 Article_320\n",
      "✔️ 已處理 Article_321\n",
      "✔️ 已處理 Article_322\n",
      "✔️ 已處理 Article_323\n",
      "✔️ 已處理 Article_324\n",
      "✔️ 已處理 Article_325\n",
      "✔️ 已處理 Article_326\n",
      "✅ step2_batch_6.json 已完成並儲存至 step2_batches\\step2_batch_6.json\n",
      "\n",
      "🚀 開始處理 step1_batches\\step1_batch_7.json → step2_batches\\step2_batch_7.json\n",
      "✔️ 已處理 Article_327\n",
      "✔️ 已處理 Article_328\n",
      "✔️ 已處理 Article_329\n",
      "✔️ 已處理 Article_330\n",
      "✔️ 已處理 Article_331\n",
      "✔️ 已處理 Article_332\n",
      "✔️ 已處理 Article_333\n",
      "✔️ 已處理 Article_334\n",
      "✔️ 已處理 Article_335\n",
      "✔️ 已處理 Article_336\n",
      "✔️ 已處理 Article_337\n",
      "✔️ 已處理 Article_338\n",
      "✔️ 已處理 Article_339\n",
      "✔️ 已處理 Article_340\n",
      "✔️ 已處理 Article_341\n",
      "✔️ 已處理 Article_342\n",
      "✔️ 已處理 Article_343\n",
      "✔️ 已處理 Article_344\n",
      "✔️ 已處理 Article_345\n",
      "✔️ 已處理 Article_346\n",
      "✔️ 已處理 Article_347\n",
      "✔️ 已處理 Article_348\n",
      "✔️ 已處理 Article_349\n",
      "✔️ 已處理 Article_350\n",
      "✔️ 已處理 Article_351\n",
      "✔️ 已處理 Article_352\n",
      "✔️ 已處理 Article_355\n",
      "✔️ 已處理 Article_356\n",
      "✔️ 已處理 Article_357\n",
      "✔️ 已處理 Article_358\n",
      "✔️ 已處理 Article_359\n",
      "✔️ 已處理 Article_360\n",
      "✔️ 已處理 Article_361\n",
      "✔️ 已處理 Article_363\n",
      "✔️ 已處理 Article_364\n",
      "✔️ 已處理 Article_365\n",
      "✔️ 已處理 Article_366\n",
      "✔️ 已處理 Article_367\n",
      "✔️ 已處理 Article_368\n",
      "✔️ 已處理 Article_369\n",
      "✔️ 已處理 Article_370\n",
      "✔️ 已處理 Article_371\n",
      "✔️ 已處理 Article_373\n",
      "✔️ 已處理 Article_374\n",
      "✔️ 已處理 Article_375\n",
      "✔️ 已處理 Article_376\n",
      "✔️ 已處理 Article_377\n",
      "✔️ 已處理 Article_379\n",
      "✔️ 已處理 Article_380\n",
      "✔️ 已處理 Article_381\n",
      "✅ step2_batch_7.json 已完成並儲存至 step2_batches\\step2_batch_7.json\n",
      "\n",
      "🚀 開始處理 step1_batches\\step1_batch_8.json → step2_batches\\step2_batch_8.json\n",
      "✔️ 已處理 Article_382\n",
      "✔️ 已處理 Article_383\n",
      "✔️ 已處理 Article_384\n",
      "✔️ 已處理 Article_385\n",
      "✔️ 已處理 Article_386\n",
      "✔️ 已處理 Article_387\n",
      "✔️ 已處理 Article_388\n",
      "✔️ 已處理 Article_389\n",
      "✔️ 已處理 Article_390\n",
      "✔️ 已處理 Article_391\n",
      "✔️ 已處理 Article_392\n",
      "✔️ 已處理 Article_393\n",
      "✔️ 已處理 Article_394\n",
      "✔️ 已處理 Article_395\n",
      "✔️ 已處理 Article_396\n",
      "✔️ 已處理 Article_397\n",
      "✔️ 已處理 Article_398\n",
      "✔️ 已處理 Article_399\n",
      "✔️ 已處理 Article_400\n",
      "✔️ 已處理 Article_401\n",
      "✔️ 已處理 Article_402\n",
      "✔️ 已處理 Article_403\n",
      "✔️ 已處理 Article_404\n",
      "✔️ 已處理 Article_405\n",
      "✔️ 已處理 Article_406\n",
      "✔️ 已處理 Article_409\n",
      "✔️ 已處理 Article_410\n",
      "✔️ 已處理 Article_411\n",
      "✔️ 已處理 Article_412\n",
      "✔️ 已處理 Article_413\n",
      "✔️ 已處理 Article_414\n",
      "✔️ 已處理 Article_415\n",
      "✔️ 已處理 Article_416\n",
      "✔️ 已處理 Article_417\n",
      "✔️ 已處理 Article_418\n",
      "✔️ 已處理 Article_419\n",
      "✔️ 已處理 Article_420\n",
      "✔️ 已處理 Article_421\n",
      "✔️ 已處理 Article_422\n",
      "✔️ 已處理 Article_423\n",
      "✔️ 已處理 Article_424\n",
      "✔️ 已處理 Article_425\n",
      "✔️ 已處理 Article_426\n",
      "✔️ 已處理 Article_427\n",
      "✔️ 已處理 Article_428\n",
      "✔️ 已處理 Article_429\n",
      "✔️ 已處理 Article_430\n",
      "✔️ 已處理 Article_431\n",
      "✔️ 已處理 Article_433\n",
      "✔️ 已處理 Article_434\n",
      "✅ step2_batch_8.json 已完成並儲存至 step2_batches\\step2_batch_8.json\n",
      "\n",
      "🚀 開始處理 step1_batches\\step1_batch_9.json → step2_batches\\step2_batch_9.json\n",
      "✔️ 已處理 Article_435\n",
      "✔️ 已處理 Article_436\n",
      "✔️ 已處理 Article_437\n",
      "✔️ 已處理 Article_438\n",
      "✔️ 已處理 Article_439\n",
      "✔️ 已處理 Article_440\n",
      "✔️ 已處理 Article_441\n",
      "✔️ 已處理 Article_442\n",
      "✔️ 已處理 Article_443\n",
      "✔️ 已處理 Article_445\n",
      "✔️ 已處理 Article_446\n",
      "✔️ 已處理 Article_448\n",
      "✔️ 已處理 Article_449\n",
      "✔️ 已處理 Article_450\n",
      "✔️ 已處理 Article_451\n",
      "✔️ 已處理 Article_452\n",
      "✔️ 已處理 Article_453\n",
      "✔️ 已處理 Article_454\n",
      "✔️ 已處理 Article_455\n",
      "✔️ 已處理 Article_456\n",
      "✔️ 已處理 Article_457\n",
      "✔️ 已處理 Article_458\n",
      "✔️ 已處理 Article_459\n",
      "✔️ 已處理 Article_460\n",
      "✔️ 已處理 Article_461\n",
      "✔️ 已處理 Article_463\n",
      "✔️ 已處理 Article_465\n",
      "✔️ 已處理 Article_466\n",
      "✔️ 已處理 Article_467\n",
      "✔️ 已處理 Article_468\n",
      "✔️ 已處理 Article_469\n",
      "✔️ 已處理 Article_470\n",
      "✔️ 已處理 Article_471\n",
      "✔️ 已處理 Article_473\n",
      "✔️ 已處理 Article_474\n",
      "✔️ 已處理 Article_475\n",
      "✔️ 已處理 Article_476\n",
      "✔️ 已處理 Article_477\n",
      "✔️ 已處理 Article_478\n",
      "✔️ 已處理 Article_479\n",
      "✔️ 已處理 Article_480\n",
      "✔️ 已處理 Article_481\n",
      "✔️ 已處理 Article_483\n",
      "✔️ 已處理 Article_484\n",
      "API 調用錯誤: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "✔️ 已處理 Article_485\n",
      "API 調用錯誤: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "✔️ 已處理 Article_486\n",
      "API 調用錯誤: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "✔️ 已處理 Article_487\n",
      "API 調用錯誤: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "✔️ 已處理 Article_488\n",
      "API 調用錯誤: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "✔️ 已處理 Article_489\n",
      "API 調用錯誤: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "✔️ 已處理 Article_490\n",
      "✅ step2_batch_9.json 已完成並儲存至 step2_batches\\step2_batch_9.json\n",
      "\n",
      "🎉 Step 2 全部批次處理完成！\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "step2_prompt = \"\"\"You are a sociology professor with 30 years of experience analyzing Asian racism in the United States.\n",
    "\n",
    "**Step 2: Extract Related Sentences**\n",
    "\n",
    "Based on the named entities identified in **Step 1**, extract **all relevant complete sentences** from the text for each entity.\n",
    "\n",
    "**For each named entity:**\n",
    "1. Use the `\"reference\"` field (the phrase as it appears in the article) to identify relevant sentences.\n",
    "2. Identify **all complete, verbatim sentences** in the text that mention or describe the entity’s **involvement, action, reaction, statement, or experience** related to anti-Asian hate (directly or indirectly).\n",
    "3. Do **not paraphrase** or summarize. Use the **exact wording** from the text.\n",
    "4. If no relevant sentence is found, set `\"relevant_sentences\": []`.\n",
    "5. Return results **grouped by entity name**, exactly matching the names used in Step 1.\n",
    "6. Include the following structured metadata for each entity:\n",
    "   - `\"entity_type\"`:  \n",
    "     - For individuals: the **social role** (e.g., \"politician\", \"celebrity\", \"victim\")  \n",
    "     - For organizations: the **institutional category** (e.g., \"law_enforcement_agency\", \"ngo_or_advocacy_group\")\n",
    "   - `\"asian_status\"`:  \n",
    "     - For individuals: **\"Asian\"**, **\"Non-Asian\"**, or **\"Cannot be inferred\"**  \n",
    "     - For organizations: Always **\"Not applicable\"**\n",
    "\n",
    "**Note:** These sentences will later be used to infer **behavioral reactions** and **emotional responses**, so include **any sentence** that provides context about what the entity did, said, or experienced.\n",
    "\n",
    "### Output format (JSON):\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"Entity Name\": {\n",
    "    \"entity_type\": \"e.g., politician, law_enforcement_agency\",\n",
    "    \"asian_status\": \"Asian / Non-Asian / Cannot be inferred / Not applicable\",\n",
    "    \"relevant_sentences\": [\n",
    "      \"Sentence 1 from the article.\",\n",
    "      \"Sentence 2 from the article.\"\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 建立輸出資料夾\n",
    "output_dir = \"step2_batches\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 找出 Step 1 的所有批次檔案\n",
    "step1_files = sorted(glob.glob(\"step1_batches/step1_batch_*.json\"))\n",
    "\n",
    "for step1_file in step1_files:\n",
    "    batch_name = os.path.basename(step1_file).replace(\"step1_\", \"step2_\")\n",
    "    output_file = os.path.join(output_dir, batch_name)\n",
    "\n",
    "    # 如果已經有 Step 2 的結果，就跳過\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"⏭️ {batch_name} 已存在，跳過\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n🚀 開始處理 {step1_file} → {output_file}\")\n",
    "\n",
    "    # 讀取 Step 1 的結果\n",
    "    with open(step1_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        step1_result = json.load(f)\n",
    "\n",
    "    step2_result = {}\n",
    "\n",
    "    # 逐篇文章處理\n",
    "    for title, step1_text in step1_result.items():\n",
    "        content = articles.get(title, \"\")  # 從原始文章 dict 拿內容\n",
    "\n",
    "        full_prompt = (\n",
    "            step2_prompt +\n",
    "            f\"\\n\\nStep 1 Results:\\n{step1_text}\" +\n",
    "            f\"\\n\\nOriginal Article Text:\\n{content}\"\n",
    "        )\n",
    "\n",
    "        response = get_response(full_prompt)\n",
    "        step2_result[title] = response\n",
    "        print(f\"✔️ 已處理 {title}\")\n",
    "\n",
    "    # 儲存這一批的 Step 2 結果\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(step2_result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"✅ {batch_name} 已完成並儲存至 {output_file}\")\n",
    "\n",
    "print(\"\\n🎉 Step 2 全部批次處理完成！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc2b10c2-8265-45e8-8c31-e98d7f31ab67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 重新處理 Article_485\n",
      "🔄 重新處理 Article_486\n",
      "🔄 重新處理 Article_487\n",
      "🔄 重新處理 Article_488\n",
      "🔄 重新處理 Article_489\n",
      "🔄 重新處理 Article_490\n",
      "✅ 已更新 step2_batches/step2_batch_9.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def rerun_incomplete_step2(step1_file, step2_file):\n",
    "    # 讀取 Step1 的結果\n",
    "    with open(step1_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        step1_result = json.load(f)\n",
    "\n",
    "    # 如果 step2_file 已存在，先讀取；否則新建\n",
    "    if os.path.exists(step2_file):\n",
    "        with open(step2_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            step2_result = json.load(f)\n",
    "    else:\n",
    "        step2_result = {}\n",
    "\n",
    "    updated = False\n",
    "\n",
    "    for title, step1_text in step1_result.items():\n",
    "        # 只處理空的 or 缺失的\n",
    "        if not step2_result.get(title):\n",
    "            content = articles.get(title, \"\")  # 原始文章\n",
    "            full_prompt = (\n",
    "                step2_prompt +\n",
    "                f\"\\n\\nStep 1 Results:\\n{step1_text}\" +\n",
    "                f\"\\n\\nOriginal Article Text:\\n{content}\"\n",
    "            )\n",
    "            response = get_response(full_prompt)\n",
    "            step2_result[title] = response\n",
    "            print(f\"🔄 重新處理 {title}\")\n",
    "            updated = True\n",
    "\n",
    "    if updated:\n",
    "        with open(step2_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(step2_result, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"✅ 已更新 {step2_file}\")\n",
    "    else:\n",
    "        print(\"👌 沒有需要補跑的文章，全數完成\")\n",
    "\n",
    "# 使用範例\n",
    "rerun_incomplete_step2(\n",
    "    step1_file=\"step1_batches/step1_batch_9.json\",\n",
    "    step2_file=\"step2_batches/step2_batch_9.json\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e1c2ee9-87b3-42bb-8bef-d271575ff76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 開始處理批次 1/12: step2_batches\\step2_batch_1.json\n",
      "✅ 批次 1/12 已完成並儲存至 step3_batches\\step3_batch_1.json\n",
      "   Gate stats: {'no': 426, 'yes': 174}\n",
      "   Label stats: {'Cannot be inferred': 466, 'Support Asian Americans': 86, 'Government takes actions to stop AAPI hate': 30, 'Advocacy/take actions for changes': 4, 'Attending marches/rallies': 1, 'Politicians initiated anti-Asian hate and racism': 4, 'Color blind/minimizing racism': 1, 'Fostering conversations about anti-Asian hate': 2, 'Useless law enforcement': 2, 'Hiring security guards': 2, 'Educating students': 1, 'Videotaping/confronting harasser/attacker': 1}\n",
      "\n",
      "🚀 開始處理批次 2/12: step2_batches\\step2_batch_10.json\n",
      "✅ 批次 2/12 已完成並儲存至 step3_batches\\step3_batch_10.json\n",
      "   Gate stats: {'no': 285, 'yes': 131}\n",
      "   Label stats: {'Cannot be inferred': 296, 'Support Asian Americans': 56, 'Government takes actions to stop AAPI hate': 51, 'Attending marches/rallies': 2, 'Advocacy/take actions for changes': 5, 'Politicians initiated anti-Asian hate and racism': 2, 'Color blind/minimizing racism': 1, 'Videotaping/confronting harasser/attacker': 2, 'Educating students': 1}\n",
      "\n",
      "🚀 開始處理批次 3/12: step2_batches\\step2_batch_11.json\n",
      "✅ 批次 3/12 已完成並儲存至 step3_batches\\step3_batch_11.json\n",
      "   Gate stats: {'yes': 164, 'no': 411}\n",
      "   Label stats: {'Support Asian Americans': 80, 'Cannot be inferred': 431, 'Government takes actions to stop AAPI hate': 46, 'Videotaping/confronting harasser/attacker': 5, 'Hiring security guards': 1, 'Politicians initiated anti-Asian hate and racism': 1, 'Speaking up on social media': 2, 'Advocacy/take actions for changes': 5, 'Attending marches/rallies': 1, 'Useless law enforcement': 1, 'Color blind/minimizing racism': 1, 'Feeling hopeless or support AAPI being not enough': 1}\n",
      "\n",
      "🚀 開始處理批次 4/12: step2_batches\\step2_batch_12.json\n",
      "✅ 批次 4/12 已完成並儲存至 step3_batches\\step3_batch_12.json\n",
      "   Gate stats: {'yes': 105, 'no': 237}\n",
      "   Label stats: {'Government takes actions to stop AAPI hate': 34, 'Support Asian Americans': 52, 'Cannot be inferred': 246, 'Advocacy/take actions for changes': 7, 'Videotaping/confronting harasser/attacker': 2, 'Politicians initiated anti-Asian hate and racism': 1}\n",
      "\n",
      "🚀 開始處理批次 5/12: step2_batches\\step2_batch_2.json\n",
      "✅ 批次 5/12 已完成並儲存至 step3_batches\\step3_batch_2.json\n",
      "   Gate stats: {'no': 343, 'yes': 123}\n",
      "   Label stats: {'Cannot be inferred': 369, 'Support Asian Americans': 49, 'Useless law enforcement': 1, 'Government takes actions to stop AAPI hate': 29, 'Politicians initiated anti-Asian hate and racism': 2, 'Attending marches/rallies': 2, 'Advocacy/take actions for changes': 9, 'Feeling hopeless or support AAPI being not enough': 2, 'Educating students': 1, 'Not confronting attacker/harasser or not reporting': 1, 'Color blind/minimizing racism': 1}\n",
      "\n",
      "🚀 開始處理批次 6/12: step2_batches\\step2_batch_3.json\n",
      "✅ 批次 6/12 已完成並儲存至 step3_batches\\step3_batch_3.json\n",
      "   Gate stats: {'no': 376, 'yes': 129}\n",
      "   Label stats: {'Cannot be inferred': 394, 'Support Asian Americans': 72, 'Advocacy/take actions for changes': 3, 'Government takes actions to stop AAPI hate': 24, 'Educating students': 2, 'Calling for being united': 2, 'Videotaping/confronting harasser/attacker': 1, 'Politicians initiated anti-Asian hate and racism': 4, 'Fostering conversations about anti-Asian hate': 2, 'Color blind/minimizing racism': 1}\n",
      "\n",
      "🚀 開始處理批次 7/12: step2_batches\\step2_batch_4.json\n",
      "✅ 批次 7/12 已完成並儲存至 step3_batches\\step3_batch_4.json\n",
      "   Gate stats: {'no': 329, 'yes': 154}\n",
      "   Label stats: {'Cannot be inferred': 361, 'Support Asian Americans': 79, 'Fostering conversations about anti-Asian hate': 2, 'Government takes actions to stop AAPI hate': 26, 'Color blind/minimizing racism': 2, 'Advocacy/take actions for changes': 3, 'Speaking up on social media': 2, 'Useless law enforcement': 1, 'Politicians initiated anti-Asian hate and racism': 4, 'Educating students': 3}\n",
      "\n",
      "🚀 開始處理批次 8/12: step2_batches\\step2_batch_5.json\n",
      "✅ 批次 8/12 已完成並儲存至 step3_batches\\step3_batch_5.json\n",
      "   Gate stats: {'yes': 139, 'no': 335}\n",
      "   Label stats: {'Support Asian Americans': 77, 'Cannot be inferred': 350, 'Useless law enforcement': 1, 'Government takes actions to stop AAPI hate': 20, 'Videotaping/confronting harasser/attacker': 3, 'Advocacy/take actions for changes': 10, 'Attending marches/rallies': 5, 'Educating students': 2, 'Politicians initiated anti-Asian hate and racism': 2, 'Hiring security guards': 1, 'Fostering conversations about anti-Asian hate': 2, 'Feeling hopeless or support AAPI being not enough': 1}\n",
      "\n",
      "🚀 開始處理批次 9/12: step2_batches\\step2_batch_6.json\n",
      "✅ 批次 9/12 已完成並儲存至 step3_batches\\step3_batch_6.json\n",
      "   Gate stats: {'yes': 141, 'no': 368}\n",
      "   Label stats: {'Support Asian Americans': 90, 'Cannot be inferred': 397, 'Attending marches/rallies': 2, 'Advocacy/take actions for changes': 2, 'Government takes actions to stop AAPI hate': 14, 'Politicians initiated anti-Asian hate and racism': 1, 'Useless law enforcement': 2, 'Fostering conversations about anti-Asian hate': 1}\n",
      "\n",
      "🚀 開始處理批次 10/12: step2_batches\\step2_batch_7.json\n",
      "✅ 批次 10/12 已完成並儲存至 step3_batches\\step3_batch_7.json\n",
      "   Gate stats: {'yes': 87, 'no': 272}\n",
      "   Label stats: {'Cannot be inferred': 291, 'Support Asian Americans': 31, 'Government takes actions to stop AAPI hate': 28, 'Politicians initiated anti-Asian hate and racism': 2, 'Advocacy/take actions for changes': 6, 'Fostering conversations about anti-Asian hate': 1}\n",
      "\n",
      "🚀 開始處理批次 11/12: step2_batches\\step2_batch_8.json\n",
      "✅ 批次 11/12 已完成並儲存至 step3_batches\\step3_batch_8.json\n",
      "   Gate stats: {'no': 344, 'yes': 113}\n",
      "   Label stats: {'Cannot be inferred': 376, 'Advocacy/take actions for changes': 11, 'Government takes actions to stop AAPI hate': 27, 'Support Asian Americans': 36, 'Educating students': 3, 'Useless law enforcement': 1, 'Politicians initiated anti-Asian hate and racism': 1, 'Videotaping/confronting harasser/attacker': 2}\n",
      "\n",
      "🚀 開始處理批次 12/12: step2_batches\\step2_batch_9.json\n",
      "✅ 批次 12/12 已完成並儲存至 step3_batches\\step3_batch_9.json\n",
      "   Gate stats: {'no': 392, 'yes': 115}\n",
      "   Label stats: {'Cannot be inferred': 422, 'Support Asian Americans': 35, 'Government takes actions to stop AAPI hate': 34, 'Advocacy/take actions for changes': 6, 'Useless law enforcement': 4, 'Educating students': 3, 'Fostering conversations about anti-Asian hate': 1, 'Attending marches/rallies': 1, 'Videotaping/confronting harasser/attacker': 1}\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "from collections import Counter\n",
    "from typing import Dict, Any\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 工具：JSON 寬鬆解析 + 句子正規化\n",
    "# ==============================\n",
    "def strip_code_fences(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return s\n",
    "    s = s.strip()\n",
    "    m = re.search(r\"```(?:json)?\\s*(.*?)```\", s, flags=re.S | re.I)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "    return s\n",
    "\n",
    "def parse_json_loose(s: str):\n",
    "    if not isinstance(s, str):\n",
    "        return None\n",
    "    s = strip_code_fences(s).strip()\n",
    "    if not s:\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        pass\n",
    "    m = re.search(r\"\\{.*\\}\", s, flags=re.S)\n",
    "    if m:\n",
    "        try:\n",
    "            return json.loads(m.group(0))\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def to_text(x: Any) -> str:\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    if isinstance(x, str):\n",
    "        return x\n",
    "    if isinstance(x, list):\n",
    "        return \"\\n\".join(str(item) for item in x)\n",
    "    if isinstance(x, dict):\n",
    "        if \"relevant_sentences\" in x:\n",
    "            return to_text(x[\"relevant_sentences\"])\n",
    "        return \"\\n\".join(to_text(v) for v in x.values())\n",
    "    return str(x)\n",
    "\n",
    "def parse_model_json(s: str, default: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return default\n",
    "    s2 = strip_code_fences(s)\n",
    "    try:\n",
    "        return json.loads(s2)\n",
    "    except Exception:\n",
    "        m = re.search(r\"\\{.*\\}\", s2, flags=re.S)\n",
    "        if m:\n",
    "            try:\n",
    "                return json.loads(m.group(0))\n",
    "            except Exception:\n",
    "                return default\n",
    "        return default\n",
    "\n",
    "# ==============================\n",
    "# Step2 結果正規化\n",
    "# ==============================\n",
    "def normalize_step2_result(title: str, raw_obj) -> Dict[str, Dict[str, str]]:\n",
    "    if isinstance(raw_obj, dict):\n",
    "        if all(isinstance(v, dict) for v in raw_obj.values()):\n",
    "            out = {}\n",
    "            for k, v in raw_obj.items():\n",
    "                out[k] = {\n",
    "                    \"entity_type\": v.get(\"entity_type\", \"\"),\n",
    "                    \"asian_status\": v.get(\"asian_status\", \"\"),\n",
    "                    \"relevant_sentences\": to_text(v.get(\"relevant_sentences\"))\n",
    "                }\n",
    "            return out\n",
    "    if isinstance(raw_obj, str):\n",
    "        parsed = parse_json_loose(raw_obj)\n",
    "        if parsed:\n",
    "            return normalize_step2_result(title, parsed)\n",
    "    return {}\n",
    "\n",
    "# ==============================\n",
    "# Prompts\n",
    "# ==============================\n",
    "def build_gate_prompt(step2_sentences: str) -> str:\n",
    "    return f\"\"\"You are a sociology professor analyzing reactions to anti-Asian hate.\n",
    "\n",
    "Task: Decide if the extracted sentences show any OBSERVABLE reaction (action or inaction) to anti-Asian hate by the entity.\n",
    "\n",
    "Rules:\n",
    "- Observable = concrete action/inaction or explicit public stance (e.g., speaking up, condemning, organizing, reporting, policy ask, government action, refusing to act).\n",
    "- Pure emotions/concerns are NOT reactions.\n",
    "- Pure incident descriptions are NOT reactions.\n",
    "- Use ONLY the exact `relevant_sentences`.\n",
    "\n",
    "Output JSON:\n",
    "{{\n",
    "  \"has_reaction\": \"yes\" | \"no\",\n",
    "  \"evidence\": \"Exact sentence(s) from relevant_sentences (or empty if no).\"\n",
    "}}\n",
    "\n",
    "relevant_sentences:\n",
    "{step2_sentences}\n",
    "\"\"\"\n",
    "\n",
    "def build_classifier_prompt(entity_type: str, asian_status: str, step2_sentences: str) -> str:\n",
    "    return f\"\"\"You are a sociology professor with 30 years of experience analyzing anti-Asian racism.\n",
    "\n",
    "Task: Classify the entity’s REACTION strictly using the Reaction Concept Tree. Use ONLY `relevant_sentences` as evidence. Do NOT infer emotions. Do NOT paraphrase.\n",
    "\n",
    "Reaction Concept Tree:\n",
    "- Support Asian Americans\n",
    "  - Attending marches/rallies\n",
    "  - Speaking up on social media\n",
    "  - Calling for being united\n",
    "  - Educating students\n",
    "  - Fostering conversations about anti-Asian hate\n",
    "  - Hiring security guards\n",
    "  - Providing shopkeepers with air horns\n",
    "  - Rewarding the public to report the info about the suspects\n",
    "- Advocacy/take actions for changes\n",
    "- Politicians initiated anti-Asian hate and racism\n",
    "- Undermining human rights\n",
    "- Color blind/minimizing racism\n",
    "- Youth as not an excuse\n",
    "- Videotaping/confronting harasser/attacker\n",
    "- Sex (sexual) addiction\n",
    "- Religion as a reason\n",
    "- Feeling hopeless or support AAPI being not enough\n",
    "- Not confronting attacker/harasser or not reporting\n",
    "- Useless law enforcement\n",
    "  - Did not take a report on Anti-Asian hate crime\n",
    "  - Did not often patrol the streets\n",
    "- Government takes actions to stop AAPI hate\n",
    "  - Installing hotlines\n",
    "  - Launching a hate-crime task force\n",
    "  - Increasing patrols\n",
    "  - Organizing a town hall\n",
    "\n",
    "Strict Rules:\n",
    "1) Pure concerns/worries ≠ reaction; return \"Cannot be inferred\".\n",
    "2) Arrests/charges/prosecutions ⇒ “Government takes actions…”.\n",
    "3) Explicit condemnation ⇒ “Support Asian Americans”.\n",
    "4) If no clear reaction, return \"Cannot be inferred\".\n",
    "5) Do NOT invent labels.\n",
    "6) Always choose the most specific subcategory.\n",
    "\n",
    "entity_type: {entity_type}\n",
    "asian_status: {asian_status}\n",
    "\n",
    "relevant_sentences:\n",
    "{step2_sentences}\n",
    "\n",
    "Output JSON:\n",
    "{{\n",
    "  \"reaction\": \"<one label from the tree or 'Cannot be inferred'>\",\n",
    "  \"reaction_reason\": \"Exact sentence(s) from relevant_sentences\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "# ==============================\n",
    "# 主流程：從 Step2 → Step3\n",
    "# ==============================\n",
    "def run_step3_from_step2(step2_dir=\"step2_batches\", step3_dir=\"step3_batches\"):\n",
    "    os.makedirs(step3_dir, exist_ok=True)\n",
    "    step2_files = sorted(glob.glob(os.path.join(step2_dir, \"step2_batch_*.json\")))\n",
    "    total_batches = len(step2_files)\n",
    "\n",
    "    for idx, step2_file in enumerate(step2_files, start=1):\n",
    "        batch_name = os.path.basename(step2_file).replace(\"step2_\", \"step3_\")\n",
    "        out_path = os.path.join(step3_dir, batch_name)\n",
    "\n",
    "        # 斷點續跑\n",
    "        if os.path.exists(out_path):\n",
    "            print(f\"⏭️ 批次 {idx}/{total_batches} {batch_name} 已存在，跳過\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n🚀 開始處理批次 {idx}/{total_batches}: {step2_file}\")\n",
    "\n",
    "        with open(step2_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            step2_batch_result = json.load(f)\n",
    "\n",
    "        step3_batch_result = {}\n",
    "        gate_stats = Counter()\n",
    "        label_stats = Counter()\n",
    "\n",
    "        for title, raw in step2_batch_result.items():\n",
    "            entities = normalize_step2_result(title, raw)\n",
    "            entity_outputs = {}\n",
    "\n",
    "            if not entities:\n",
    "                step3_batch_result[title] = entity_outputs\n",
    "                continue\n",
    "\n",
    "            for entity, meta in entities.items():\n",
    "                entity_type = meta.get(\"entity_type\", \"\")\n",
    "                asian_status = meta.get(\"asian_status\", \"\")\n",
    "                relevant_sentences = to_text(meta.get(\"relevant_sentences\", \"\")).strip()\n",
    "\n",
    "                # --- 3A: Gate ---\n",
    "                gate_prompt = build_gate_prompt(relevant_sentences)\n",
    "                gate_resp = get_response(gate_prompt, temperature=0.0)\n",
    "                gate_json = parse_model_json(gate_resp, default={\"has_reaction\": \"no\", \"evidence\": \"\"})\n",
    "\n",
    "                has_reaction = str(gate_json.get(\"has_reaction\", \"no\")).lower()\n",
    "                gate_stats[has_reaction] += 1\n",
    "\n",
    "                if has_reaction != \"yes\":\n",
    "                    out = {\n",
    "                        \"entity_type\": entity_type,\n",
    "                        \"asian_status\": asian_status,\n",
    "                        \"reaction\": \"Cannot be inferred\",\n",
    "                        \"reaction_reason\": \"\"\n",
    "                    }\n",
    "                    entity_outputs[entity] = out\n",
    "                    label_stats[\"Cannot be inferred\"] += 1\n",
    "                    continue\n",
    "\n",
    "                # --- 3B: Classifier ---\n",
    "                cls_prompt = build_classifier_prompt(entity_type, asian_status, relevant_sentences)\n",
    "                cls_resp = get_response(cls_prompt, temperature=0.0)\n",
    "                cls_json = parse_model_json(cls_resp, default={\"reaction\": \"Cannot be inferred\", \"reaction_reason\": \"\"})\n",
    "\n",
    "                label = cls_json.get(\"reaction\", \"Cannot be inferred\")\n",
    "                reason = cls_json.get(\"reaction_reason\", \"\")\n",
    "\n",
    "                out = {\n",
    "                    \"entity_type\": entity_type,\n",
    "                    \"asian_status\": asian_status,\n",
    "                    \"reaction\": label,\n",
    "                    \"reaction_reason\": reason\n",
    "                }\n",
    "                entity_outputs[entity] = out\n",
    "                label_stats[label] += 1\n",
    "\n",
    "            step3_batch_result[title] = entity_outputs\n",
    "\n",
    "        # 儲存\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(step3_batch_result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"✅ 批次 {idx}/{total_batches} 已完成並儲存至 {out_path}\")\n",
    "        print(\"   Gate stats:\", dict(gate_stats))\n",
    "        print(\"   Label stats:\", dict(label_stats))\n",
    "\n",
    "# ==============================\n",
    "# 執行\n",
    "# ==============================\n",
    "if __name__ == \"__main__\":\n",
    "    run_step3_from_step2(\"step2_batches\", \"step3_batches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33bca6c5-678f-4f0b-83b5-4892945dd504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 開始處理批次 1/12: step2_batches\\step2_batch_1.json\n",
      "✅ 批次 1/12 已完成並儲存至 step3_batches_new\\step3_batch_1.json\n",
      "   Gate stats: {'no': 472, 'yes': 185}\n",
      "   Label stats: {'Cannot be inferred': 504, 'Support Asian Americans': 77, 'Fostering conversations about anti-Asian hate': 14, 'Speaking up on social media': 4, 'Government takes actions for changes': 2, 'Attending marches/rallies': 8, 'Calling for being united': 7, 'Increasing patrols': 4, 'Takes actions to stop AAPI hate': 12, 'Politicians initiated anti-Asian hate and racism': 5, 'Feeling hopeless or support AAPI being not enough': 11, 'Videotaping/confronting harasser/attacker': 2, 'Advocacy/take actions for changes': 4, 'Government takes actions to stop AAPI hate': 1, 'Government takes actions…': 1, 'Launching a hate-crime task force': 1}\n",
      "\n",
      "🚀 開始處理批次 2/12: step2_batches\\step2_batch_10.json\n",
      "✅ 批次 2/12 已完成並儲存至 step3_batches_new\\step3_batch_10.json\n",
      "   Gate stats: {'no': 314, 'yes': 136}\n",
      "   Label stats: {'Cannot be inferred': 332, 'Support Asian Americans': 53, 'Takes actions to stop AAPI hate': 26, 'Attending marches/rallies': 6, 'Advocacy/take actions for changes': 6, 'Government takes actions…': 3, 'Increasing patrols': 3, 'Government takes actions to stop AAPI hate': 1, 'Fostering conversations about anti-Asian hate': 5, 'Videotaping/confronting harasser/attacker': 1, 'Speaking up on social media': 7, 'Educating students': 2, 'Installing hotlines': 1, 'Calling for being united': 1, 'Making an announcement to condemn anti-Asian hate': 1, 'Politicians initiated anti-Asian hate and racism': 1, 'Feeling hopeless or support AAPI being not enough': 1}\n",
      "\n",
      "🚀 開始處理批次 3/12: step2_batches\\step2_batch_11.json\n",
      "✅ 批次 3/12 已完成並儲存至 step3_batches_new\\step3_batch_11.json\n",
      "   Gate stats: {'yes': 170, 'no': 428}\n",
      "   Label stats: {'Support Asian Americans': 75, 'Cannot be inferred': 455, 'Takes actions to stop AAPI hate': 15, 'Government takes actions…': 6, 'Launching a hate-crime task force': 2, 'Speaking up on social media': 10, 'Feeling hopeless or support AAPI being not enough': 4, 'Advocacy/take actions for changes': 7, 'Videotaping/confronting harasser/attacker': 4, 'Fostering conversations about anti-Asian hate': 4, 'Increasing patrols': 7, 'Hiring security guards': 1, 'Calling for being united': 2, 'Attending marches/rallies': 1, 'Making an announcement to condemn anti-Asian hate': 1, 'Installing hotlines': 1, 'Government takes actions to stop AAPI hate': 2, 'Politicians initiated anti-Asian hate and racism': 1}\n",
      "\n",
      "🚀 開始處理批次 4/12: step2_batches\\step2_batch_12.json\n",
      "✅ 批次 4/12 已完成並儲存至 step3_batches_new\\step3_batch_12.json\n",
      "   Gate stats: {'yes': 105, 'no': 237}\n",
      "   Label stats: {'Support Asian Americans': 39, 'Takes actions to stop AAPI hate': 16, 'Cannot be inferred': 251, 'Government takes actions…': 3, 'Speaking up on social media': 8, 'Videotaping/confronting harasser/attacker': 2, 'Installing hotlines': 1, 'Fostering conversations about anti-Asian hate': 7, 'Advocacy/take actions for changes': 6, 'Feeling hopeless or support AAPI being not enough': 1, 'Politicians initiated anti-Asian hate and racism': 1, 'Attending marches/rallies': 1, 'Rewarding the public to report the info about the suspects': 4, 'Calling for being united': 2}\n",
      "\n",
      "🚀 開始處理批次 5/12: step2_batches\\step2_batch_2.json\n",
      "✅ 批次 5/12 已完成並儲存至 step3_batches_new\\step3_batch_2.json\n",
      "   Gate stats: {'no': 372, 'yes': 129}\n",
      "   Label stats: {'Cannot be inferred': 402, 'Fostering conversations about anti-Asian hate': 13, 'Support Asian Americans': 43, 'Useless law enforcement': 1, 'Increasing patrols': 1, 'Politicians initiated anti-Asian hate and racism': 1, 'Launching a hate-crime task force': 4, 'Attending marches/rallies': 5, 'Takes actions to stop AAPI hate': 10, 'Advocacy/take actions for changes': 5, 'Speaking up on social media': 1, 'Rewarding the public to report the info about the suspects': 1, 'Feeling hopeless or support AAPI being not enough': 2, 'Calling for being united': 1, 'Government takes actions to stop AAPI hate': 6, 'Government takes actions…': 1, 'Installing hotlines': 1, 'Not confronting attacker/harasser or not reporting': 1, 'Organizing a town hall': 1, 'Color blind/minimizing racism': 1}\n",
      "\n",
      "🚀 開始處理批次 6/12: step2_batches\\step2_batch_3.json\n",
      "✅ 批次 6/12 已完成並儲存至 step3_batches_new\\step3_batch_3.json\n",
      "   Gate stats: {'no': 407, 'yes': 130}\n",
      "   Label stats: {'Cannot be inferred': 426, 'Support Asian Americans': 58, 'Advocacy/take actions for changes': 2, 'Fostering conversations about anti-Asian hate': 16, 'Takes actions to stop AAPI hate': 5, 'Calling for being united': 6, 'Useless law enforcement': 1, 'Did not take a report on Anti-Asian hate crime': 1, 'Speaking up on social media': 8, 'Attending marches/rallies': 2, 'Organizing a town hall': 1, 'Government takes actions…': 2, 'Politicians initiated anti-Asian hate and racism': 2, 'Installing hotlines': 6, 'Rewarding the public to report the info about the suspects': 1}\n",
      "\n",
      "🚀 開始處理批次 7/12: step2_batches\\step2_batch_4.json\n",
      "✅ 批次 7/12 已完成並儲存至 step3_batches_new\\step3_batch_4.json\n",
      "   Gate stats: {'no': 345, 'yes': 152}\n",
      "   Label stats: {'Cannot be inferred': 370, 'Fostering conversations about anti-Asian hate': 9, 'Calling for being united': 3, 'Support Asian Americans': 75, 'Politicians initiated anti-Asian hate and racism': 4, 'Government takes actions…': 2, 'Takes actions to stop AAPI hate': 8, 'Speaking up on social media': 12, 'Installing hotlines': 6, 'Attending marches/rallies': 3, 'Feeling hopeless or support AAPI being not enough': 1, 'Educating students': 1, 'Government takes actions to stop AAPI hate': 1, 'Advocacy/take actions for changes': 1, 'Launching a hate-crime task force': 1}\n",
      "\n",
      "🚀 開始處理批次 8/12: step2_batches\\step2_batch_5.json\n",
      "✅ 批次 8/12 已完成並儲存至 step3_batches_new\\step3_batch_5.json\n",
      "   Gate stats: {'yes': 154, 'no': 376}\n",
      "   Label stats: {'Support Asian Americans': 76, 'Cannot be inferred': 392, 'Takes actions to stop AAPI hate': 10, 'Videotaping/confronting harasser/attacker': 3, 'Advocacy/take actions for changes': 9, 'Calling for being united': 4, 'Attending marches/rallies': 9, 'Fostering conversations about anti-Asian hate': 13, 'Speaking up on social media': 3, 'Increasing patrols': 2, 'Feeling hopeless or support AAPI being not enough': 2, 'Organizing a town hall': 3, 'Launching a hate-crime task force': 1, 'Educating students': 1, 'Rewarding the public to report the info about the suspects': 2}\n",
      "\n",
      "🚀 開始處理批次 9/12: step2_batches\\step2_batch_6.json\n",
      "✅ 批次 9/12 已完成並儲存至 step3_batches_new\\step3_batch_6.json\n",
      "   Gate stats: {'yes': 156, 'no': 389}\n",
      "   Label stats: {'Attending marches/rallies': 12, 'Fostering conversations about anti-Asian hate': 4, 'Support Asian Americans': 81, 'Cannot be inferred': 416, 'Calling for being united': 7, 'Feeling hopeless or support AAPI being not enough': 2, 'Advocacy/take actions for changes': 2, 'Useless law enforcement': 1, 'Launching a hate-crime task force': 1, 'Increasing patrols': 2, 'Videotaping/confronting harasser/attacker': 1, 'Takes actions to stop AAPI hate': 6, 'Government takes actions…': 1, 'Government takes actions to stop AAPI hate': 2, 'Speaking up on social media': 6, 'Color blind/minimizing racism': 1}\n",
      "\n",
      "🚀 開始處理批次 10/12: step2_batches\\step2_batch_7.json\n",
      "✅ 批次 10/12 已完成並儲存至 step3_batches_new\\step3_batch_7.json\n",
      "   Gate stats: {'yes': 95, 'no': 302}\n",
      "   Label stats: {'Fostering conversations about anti-Asian hate': 9, 'Cannot be inferred': 317, 'Support Asian Americans': 34, 'Increasing patrols': 3, 'Takes actions to stop AAPI hate': 15, 'Politicians initiated anti-Asian hate and racism': 3, 'Installing hotlines': 1, 'Advocacy/take actions for changes': 4, 'Government takes actions…': 4, 'Government takes actions to stop AAPI hate': 2, 'Launching a hate-crime task force': 1, 'Attending marches/rallies': 1, 'Feeling hopeless or support AAPI being not enough': 1, 'Videotaping/confronting harasser/attacker': 1, 'Speaking up on social media': 1}\n",
      "\n",
      "🚀 開始處理批次 11/12: step2_batches\\step2_batch_8.json\n",
      "✅ 批次 11/12 已完成並儲存至 step3_batches_new\\step3_batch_8.json\n",
      "   Gate stats: {'no': 370, 'yes': 121}\n",
      "   Label stats: {'Cannot be inferred': 405, 'Fostering conversations about anti-Asian hate': 12, 'Government takes actions to stop AAPI hate': 2, 'Support Asian Americans': 43, 'Advocacy/take actions for changes': 10, 'Speaking up on social media': 3, 'Government takes actions…': 1, 'Launching a hate-crime task force': 1, 'Takes actions to stop AAPI hate': 9, 'Calling for being united': 1, 'Videotaping/confronting harasser/attacker': 2, 'Installing hotlines': 2}\n",
      "\n",
      "🚀 開始處理批次 12/12: step2_batches\\step2_batch_9.json\n",
      "✅ 批次 12/12 已完成並儲存至 step3_batches_new\\step3_batch_9.json\n",
      "   Gate stats: {'no': 395, 'yes': 123}\n",
      "   Label stats: {'Cannot be inferred': 438, 'Speaking up on social media': 3, 'Advocacy/take actions for changes': 6, 'Support Asian Americans': 31, 'Fostering conversations about anti-Asian hate': 12, 'Takes actions to stop AAPI hate': 15, 'Attending marches/rallies': 5, 'Feeling hopeless or support AAPI being not enough': 1, 'Color blind/minimizing racism': 1, 'Government takes actions…': 1, 'Increasing patrols': 2, 'Videotaping/confronting harasser/attacker': 1, 'Useless law enforcement': 2}\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "from collections import Counter\n",
    "from typing import Dict, Any\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 工具：JSON 寬鬆解析 + 句子正規化\n",
    "# ==============================\n",
    "def strip_code_fences(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return s\n",
    "    s = s.strip()\n",
    "    m = re.search(r\"```(?:json)?\\s*(.*?)```\", s, flags=re.S | re.I)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "    return s\n",
    "\n",
    "def parse_json_loose(s: str):\n",
    "    if not isinstance(s, str):\n",
    "        return None\n",
    "    s = strip_code_fences(s).strip()\n",
    "    if not s:\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        pass\n",
    "    m = re.search(r\"\\{.*\\}\", s, flags=re.S)\n",
    "    if m:\n",
    "        try:\n",
    "            return json.loads(m.group(0))\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def to_text(x: Any) -> str:\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    if isinstance(x, str):\n",
    "        return x\n",
    "    if isinstance(x, list):\n",
    "        return \"\\n\".join(str(item) for item in x)\n",
    "    if isinstance(x, dict):\n",
    "        if \"relevant_sentences\" in x:\n",
    "            return to_text(x[\"relevant_sentences\"])\n",
    "        return \"\\n\".join(to_text(v) for v in x.values())\n",
    "    return str(x)\n",
    "\n",
    "def parse_model_json(s: str, default: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return default\n",
    "    s2 = strip_code_fences(s)\n",
    "    try:\n",
    "        return json.loads(s2)\n",
    "    except Exception:\n",
    "        m = re.search(r\"\\{.*\\}\", s2, flags=re.S)\n",
    "        if m:\n",
    "            try:\n",
    "                return json.loads(m.group(0))\n",
    "            except Exception:\n",
    "                return default\n",
    "        return default\n",
    "\n",
    "# ==============================\n",
    "# Step2 結果正規化\n",
    "# ==============================\n",
    "def normalize_step2_result(title: str, raw_obj) -> Dict[str, Dict[str, str]]:\n",
    "    if isinstance(raw_obj, dict):\n",
    "        if all(isinstance(v, dict) for v in raw_obj.values()):\n",
    "            out = {}\n",
    "            for k, v in raw_obj.items():\n",
    "                out[k] = {\n",
    "                    \"entity_type\": v.get(\"entity_type\", \"\"),\n",
    "                    \"asian_status\": v.get(\"asian_status\", \"\"),\n",
    "                    \"relevant_sentences\": to_text(v.get(\"relevant_sentences\"))\n",
    "                }\n",
    "            return out\n",
    "    if isinstance(raw_obj, str):\n",
    "        parsed = parse_json_loose(raw_obj)\n",
    "        if parsed:\n",
    "            return normalize_step2_result(title, parsed)\n",
    "    return {}\n",
    "\n",
    "# ==============================\n",
    "# Prompts\n",
    "# ==============================\n",
    "def build_gate_prompt(step2_sentences: str) -> str:\n",
    "    return f\"\"\"You are a sociology professor analyzing reactions to anti-Asian hate.\n",
    "\n",
    "Task: Decide if the extracted sentences show any OBSERVABLE reaction (action or inaction) to anti-Asian hate by the entity.\n",
    "\n",
    "Rules:\n",
    "- Observable = concrete action/inaction or explicit public stance (e.g., speaking up, condemning, organizing, reporting, policy ask, government action, refusing to act).\n",
    "- Pure emotions/concerns are NOT reactions.\n",
    "- Pure incident descriptions are NOT reactions.\n",
    "- Use ONLY the exact `relevant_sentences`.\n",
    "\n",
    "Output JSON:\n",
    "{{\n",
    "  \"has_reaction\": \"yes\" | \"no\",\n",
    "  \"evidence\": \"Exact sentence(s) from relevant_sentences (or empty if no).\"\n",
    "}}\n",
    "\n",
    "relevant_sentences:\n",
    "{step2_sentences}\n",
    "\"\"\"\n",
    "\n",
    "def build_classifier_prompt(entity_type: str, asian_status: str, step2_sentences: str) -> str:\n",
    "    return f\"\"\"You are a sociology professor with 30 years of experience analyzing anti-Asian racism.\n",
    "\n",
    "Task: Classify the entity’s REACTION strictly using the Reaction Concept Tree. Use ONLY `relevant_sentences` as evidence. Do NOT infer emotions. Do NOT paraphrase.\n",
    "\n",
    "Reaction Concept Tree:\n",
    "- Support Asian Americans:People or organizations condemned/do not want Anti-Asian incidents to happen, including all forms of crimes, attacks, violence, assaults, physical, verbal, and online harassment. This kind of support is at a conscientious (cognitive) level, not yet taking concrete actions to stop AAPI hate. \n",
    "  - Attending marches/rallies:People or organizations actively attended marches/rallies for supporting Asian American communities.\n",
    "  - Speaking up on social media:People or organizations spoke up in public, such as via social media, to condemn Anti-Asian hate.\n",
    "  - Calling for being united:Asians/Asian Americans become united to support each other to overcome Anti-Asian hate.\n",
    "  - Fostering conversations about anti-Asian hate:The public fostered conversations regarding the Anti-Asian hate issues. Some organizations/groups (e.g., Asian American communities) also think it’s important to have conversations to address the root cause behind the Anti-Asian hate incidents so this will help us to make change.\n",
    "  - Providing shopkeepers with air horns:Some stores or groups provide shopkeepers with air horns. If any anti-Asian hate crimes happened, they can use the air horns to draw everyone’s attention\n",
    "- Advocacy/take actions for changes:Individuals, groups, or organizations want or advocate for changing the current situation where Asians/Asian Americans suffered from Anti-Asian hate, such as physical, verbal, and online harassment, attacks, violence, assaults, and hate crimes. They want cultural shift, open dialogue and listening sessions regarding incidents, practical change in racial stereotypes\n",
    "and perceptions, more awareness about Anti-Asian hate, and human rights.\n",
    "- Politicians initiated anti-Asian hate and racism:Trump and some republican politicians made a lot of comments on the COVID-19 pandemic. One kind of comment is that he used/dubbed Asian-related objects to combine with disease (virus/flu/covid) or directly calling coronavirus/covid (e.g., country’s or region’s names or Kung flu[modified by Kung Fu], or races) to verbally attack Asians/Asian Americans, such as  “China/Chinese virus” or “Kung flu.” Another type of comment is that he blamed China for causing the pandemic. Such kinds of comments initiated racism, Anti-Asian bigotry/hate, and Anti-Asian hate incidents in the US society. Such comments also led some Americans to blame Asians/Asian Americans for causing the pandemic.\n",
    "- Undermining human rights:People want to dehumanize and to undermine the fundamental rights, dignity and belonging of those they target.\n",
    "- Color blind/minimizing racism:Some Americans do not believe discrimination, racism, or racist bias/bigotry against Asians/Asian Americans exists in the community. Incidents of anti-Asian hate (including physical, verbal, and online harassment, attacks, violence, assaults, and Anti-Asian crimes) were downplayed, ignored, or perceived as not existing by the public, the law enforcement system (e.g., the police), and governors (e.g., Mr. Donald Trump). When Asians/Asian Americans were attacked, Anti-Asian hate or racism was not perceived as the perpetrators’ motives/motivations by the police or the perpetrators said their motivations were not triggered by Anti-Asian bigotry or racism.\n",
    "- Youth as not an excuse:Robert Aaron Long murdered eight people in the incident of the 2021 Atlanta Spa Shootings. In news reports, he was called “the 21-year-old.” Some comments advocated stopping calling him “the 21-year-old” as if his youth is an excuse to murder others because of their race, ethnicities, and sex.\n",
    "- Videotaping/confronting harasser/attacker:Asians/Asian Americans or bystanders videotaped/recorded the incidents of physical or verbal harassment; Anti-Asian attacks, assaults, or violence; and Anti-Asian crimes. Asians/Asian Americans who suffered physical harassment, attacks, assaults, violence attacked back to the harassers or attackers. Bystanders’ behaviors aim to defense those who were attacked. Cell phones and survelliance system can be used for videotaping or recording. Additionally, those who experienced verbal harassment speak out to the harassers to let them know their thoughts were biased, offensive, and unjust and tell them to stop. \n",
    "- Sex (sexual) addiction:Excessive sexual thoughts, desires, urges or behaviors that can’t be controlled and cause distress and harm to your relationships, finances and other aspects of life. It is also called hypersexuality or compulsive sexual behavior. It is what the Atlanta shooter claimed as a motivation that led to his senseless killings of the victims.\n",
    "- Religion as a reason:In the 2021 Atlanta Spa Shootings, Robert Aaron Long was the killer who murdered eight people. He told the police that his motive was religious guilt about his sexuality. He said he had sexual desire so he wanted to eliminate it. That’s why he went to the spa to skill women of Asian descent. Asian advocacy groups mentioned whether the killer’s motive was religious guilt about his sexuality, no one should ignore the broader context of Anti-Asian violence and hate crimes. Asian advocacy groups tend to attribute the killer’s motive stems from racism or xenophobia, misogyny, and gendered racism\n",
    "- Feeling hopeless or support AAPI being not enough:Asians/Asian Americans felt worried, frustrated, anxious, and afraid that they may experience Anti-Asian hate crimes, attacks, assaults, and violence. But they felt that nothing happened to stop them. Support for Asian American communities is not enough.\n",
    "- Not confronting attacker/harasser or not reporting:Asians/Asian Americans did not want to confront attackers/harassers/bullies who physically or verbally harassed or attacked them. They thought it is not worthy of reporting the incidents. They did not want to confront because they were afraid of their safety. They just wanted to leave from the incidents soon.\n",
    "- Useless law enforcement:Police did not take a police report and denied there was an Anti-Asian hate crime for the incidents of physical, verbal, or online harassment, attacks, assaults, violence, and Anti-Asian crimes. Another situation is that police affirmed there was a crime, but the motivation did not come from Anti-Asian hate or bigotry/prejudice or racism. Additionally, Asian Americans thought if police often patrolled the streets, a lot of Anti-Asian hate crimes, attacks, assaults, and violence would not happen. But in reality, policy did not do so. \n",
    "  - Did not take a report on Anti-Asian hate crime:police did not take a report on Anti-Asian hate crime, including physical, verbal, or online harassment, attacks, assaults, and violence.\n",
    "  - Did not often patrol the streets:police affirmed there was a crime, but police did not often patrol the streets so that there were a lot of Anti-Asian hate crimes (e.g., physical, verbal, or online harassment, attacks, assaults, and violence) happened.\n",
    "- Takes actions to stop AAPI hate:After the incidents of Anti-Asian hate crimes, attacks, assaults, and violence, state or city government or individuals take concrete actions that aim to stop AAPI hate.\n",
    "  - Installing hotlines:This is a type of action to stop AAPI hate. Some organizations (e.g., city and state governments) install hotlines for victims or people who witness Anti-Asian incidents to report.\n",
    "  - Launching a hate-crime task force:This is a type of action to stop AAPI hate. Some organizations (e.g., city and state governments) launched an Asian hate crime task force to develop approaches to stopping anti-Asian hate crimes.\n",
    "  - Making an announcement to condemn anti-Asian hate:This is a type of action to stop AAPI hate. Some organizations (e.g., city and state governments)  made an open announcement to condemn anti-Asian hate.\n",
    "  - Increasing patrols:Some organizations (e.g., city and state governments) increased patrolling the streets to ensure the safety of Asian Americans. \n",
    "  - Organizing a town hall:Some organizations (e.g., city and state governments) organized a town hall meeting to discuss how to stop anti-Asian hate racism.\n",
    "  - Hiring security guards:Some stores or groups hired security guards to increase safety for Asian Americans and prevent anti-Asian hate crimes or racism.\n",
    "  - Educating students:Schoolteachers and university faculty took actions to educate students on current social and political issues on Anti-Asian hate. They aim to use education to change the public’s view about Asian Americans/Asians and increase the awareness of respecting Asian Americans/Asians\n",
    "  - Rewarding the public to report the info about the suspects:Individuals, groups, or organizations provide rewards to the public when they report any information regarding the suspects who may commit anti-Asian hate crimes.\n",
    "\n",
    "\n",
    "Strict Rules:\n",
    "1) Pure concerns/worries ≠ reaction; return \"Cannot be inferred\".\n",
    "2) Arrests/charges/prosecutions ⇒ “Government takes actions…”.\n",
    "3) Explicit condemnation ⇒ “Support Asian Americans”.\n",
    "4) If no clear reaction, return \"Cannot be inferred\".\n",
    "5) Do NOT invent labels.\n",
    "6) Always choose the most specific subcategory.\n",
    "\n",
    "entity_type: {entity_type}\n",
    "asian_status: {asian_status}\n",
    "\n",
    "relevant_sentences:\n",
    "{step2_sentences}\n",
    "\n",
    "Output JSON:\n",
    "{{\n",
    "  \"reaction\": \"<one label from the tree or 'Cannot be inferred'>\",\n",
    "  \"reaction_reason\": \"Exact sentence(s) from relevant_sentences\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "# ==============================\n",
    "# 主流程：從 Step2 → Step3\n",
    "# ==============================\n",
    "def run_step3_from_step2(step2_dir=\"step2_batches\", step3_dir=\"step3_batches\"):\n",
    "    os.makedirs(step3_dir, exist_ok=True)\n",
    "    step2_files = sorted(glob.glob(os.path.join(step2_dir, \"step2_batch_*.json\")))\n",
    "    total_batches = len(step2_files)\n",
    "\n",
    "    for idx, step2_file in enumerate(step2_files, start=1):\n",
    "        batch_name = os.path.basename(step2_file).replace(\"step2_\", \"step3_\")\n",
    "        out_path = os.path.join(step3_dir, batch_name)\n",
    "\n",
    "        # 斷點續跑\n",
    "        if os.path.exists(out_path):\n",
    "            print(f\"⏭️ 批次 {idx}/{total_batches} {batch_name} 已存在，跳過\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n🚀 開始處理批次 {idx}/{total_batches}: {step2_file}\")\n",
    "\n",
    "        with open(step2_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            step2_batch_result = json.load(f)\n",
    "\n",
    "        step3_batch_result = {}\n",
    "        gate_stats = Counter()\n",
    "        label_stats = Counter()\n",
    "\n",
    "        for title, raw in step2_batch_result.items():\n",
    "            entities = normalize_step2_result(title, raw)\n",
    "            entity_outputs = {}\n",
    "\n",
    "            if not entities:\n",
    "                step3_batch_result[title] = entity_outputs\n",
    "                continue\n",
    "\n",
    "            for entity, meta in entities.items():\n",
    "                entity_type = meta.get(\"entity_type\", \"\")\n",
    "                asian_status = meta.get(\"asian_status\", \"\")\n",
    "                relevant_sentences = to_text(meta.get(\"relevant_sentences\", \"\")).strip()\n",
    "\n",
    "                # --- 3A: Gate ---\n",
    "                gate_prompt = build_gate_prompt(relevant_sentences)\n",
    "                gate_resp = get_response(gate_prompt, temperature=0.0)\n",
    "                gate_json = parse_model_json(gate_resp, default={\"has_reaction\": \"no\", \"evidence\": \"\"})\n",
    "\n",
    "                has_reaction = str(gate_json.get(\"has_reaction\", \"no\")).lower()\n",
    "                gate_stats[has_reaction] += 1\n",
    "\n",
    "                if has_reaction != \"yes\":\n",
    "                    out = {\n",
    "                        \"entity_type\": entity_type,\n",
    "                        \"asian_status\": asian_status,\n",
    "                        \"reaction\": \"Cannot be inferred\",\n",
    "                        \"reaction_reason\": \"\"\n",
    "                    }\n",
    "                    entity_outputs[entity] = out\n",
    "                    label_stats[\"Cannot be inferred\"] += 1\n",
    "                    continue\n",
    "\n",
    "                # --- 3B: Classifier ---\n",
    "                cls_prompt = build_classifier_prompt(entity_type, asian_status, relevant_sentences)\n",
    "                cls_resp = get_response(cls_prompt, temperature=0.0)\n",
    "                cls_json = parse_model_json(cls_resp, default={\"reaction\": \"Cannot be inferred\", \"reaction_reason\": \"\"})\n",
    "\n",
    "                label = cls_json.get(\"reaction\", \"Cannot be inferred\")\n",
    "                reason = cls_json.get(\"reaction_reason\", \"\")\n",
    "\n",
    "                out = {\n",
    "                    \"entity_type\": entity_type,\n",
    "                    \"asian_status\": asian_status,\n",
    "                    \"reaction\": label,\n",
    "                    \"reaction_reason\": reason\n",
    "                }\n",
    "                entity_outputs[entity] = out\n",
    "                label_stats[label] += 1\n",
    "\n",
    "            step3_batch_result[title] = entity_outputs\n",
    "\n",
    "        # 儲存\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(step3_batch_result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"✅ 批次 {idx}/{total_batches} 已完成並儲存至 {out_path}\")\n",
    "        print(\"   Gate stats:\", dict(gate_stats))\n",
    "        print(\"   Label stats:\", dict(label_stats))\n",
    "\n",
    "# ==============================\n",
    "# 執行\n",
    "# ==============================\n",
    "if __name__ == \"__main__\":\n",
    "    run_step3_from_step2(\"step2_batches\", \"step3_batches_new\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "235f9159-8986-4aaa-8c91-8533ad327d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 開始處理批次 1/12: step2_batches\\step2_batch_1.json\n",
      "✅ 批次 1/12 已完成並儲存至 step4_batches\\step4_batch_1.json\n",
      "   Stats: {'done': 50}\n",
      "\n",
      "🚀 開始處理批次 2/12: step2_batches\\step2_batch_10.json\n",
      "✅ 批次 2/12 已完成並儲存至 step4_batches\\step4_batch_10.json\n",
      "   Stats: {'done': 50}\n",
      "\n",
      "🚀 開始處理批次 3/12: step2_batches\\step2_batch_11.json\n",
      "✅ 批次 3/12 已完成並儲存至 step4_batches\\step4_batch_11.json\n",
      "   Stats: {'done': 50}\n",
      "\n",
      "🚀 開始處理批次 4/12: step2_batches\\step2_batch_12.json\n",
      "✅ 批次 4/12 已完成並儲存至 step4_batches\\step4_batch_12.json\n",
      "   Stats: {'done': 34}\n",
      "\n",
      "🚀 開始處理批次 5/12: step2_batches\\step2_batch_2.json\n",
      "✅ 批次 5/12 已完成並儲存至 step4_batches\\step4_batch_2.json\n",
      "   Stats: {'done': 50}\n",
      "\n",
      "🚀 開始處理批次 6/12: step2_batches\\step2_batch_3.json\n",
      "✅ 批次 6/12 已完成並儲存至 step4_batches\\step4_batch_3.json\n",
      "   Stats: {'done': 50}\n",
      "\n",
      "🚀 開始處理批次 7/12: step2_batches\\step2_batch_4.json\n",
      "✅ 批次 7/12 已完成並儲存至 step4_batches\\step4_batch_4.json\n",
      "   Stats: {'done': 50}\n",
      "\n",
      "🚀 開始處理批次 8/12: step2_batches\\step2_batch_5.json\n",
      "✅ 批次 8/12 已完成並儲存至 step4_batches\\step4_batch_5.json\n",
      "   Stats: {'done': 50}\n",
      "\n",
      "🚀 開始處理批次 9/12: step2_batches\\step2_batch_6.json\n",
      "✅ 批次 9/12 已完成並儲存至 step4_batches\\step4_batch_6.json\n",
      "   Stats: {'done': 50}\n",
      "\n",
      "🚀 開始處理批次 10/12: step2_batches\\step2_batch_7.json\n",
      "✅ 批次 10/12 已完成並儲存至 step4_batches\\step4_batch_7.json\n",
      "   Stats: {'done': 50}\n",
      "\n",
      "🚀 開始處理批次 11/12: step2_batches\\step2_batch_8.json\n",
      "✅ 批次 11/12 已完成並儲存至 step4_batches\\step4_batch_8.json\n",
      "   Stats: {'done': 50}\n",
      "\n",
      "🚀 開始處理批次 12/12: step2_batches\\step2_batch_9.json\n",
      "✅ 批次 12/12 已完成並儲存至 step4_batches\\step4_batch_9.json\n",
      "   Stats: {'done': 50}\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Step 4 Prompt\n",
    "# ==============================\n",
    "step4_prompt = \"\"\"You are a sociology professor with 30 years of experience analyzing racial dynamics and anti-Asian racism in the United States.\n",
    "\n",
    "### Step 4: Infer **Emotions and Their Intensity**\n",
    "\n",
    "Your task is to analyze the **extracted sentences** from **Step 2** and infer each entity's **emotional stance** toward anti-Asian hate.\n",
    "\n",
    "You will also receive metadata from Step 2, including:\n",
    "- `entity_type`: for individuals use their social role; for organizations use institutional category.\n",
    "- `asian_status`: \"Asian\", \"Non-Asian\", \"Cannot be inferred\", or \"Not applicable\"\n",
    "\n",
    "Use only the exact `relevant_sentences` from Step 2 as your source — do NOT paraphrase or add your own wording.\n",
    "\n",
    "---\n",
    "\n",
    "## Emotion Concept Tree\n",
    "- Love \n",
    "- Joy \n",
    "- Anger \n",
    "- Sadness \n",
    "- Fear \n",
    "- Surprise \n",
    "\n",
    "---\n",
    "\n",
    "## Instructions\n",
    "1. Focus only on emotions — do NOT infer actions.\n",
    "2. If no emotion is expressed, output `\"emotion\": \"Cannot be inferred\".\n",
    "3. If multiple emotions appear, list multiple objects.\n",
    "4. Use the exact sentence(s) as `\"emotion_reason\"`.\n",
    "\n",
    "---\n",
    "\n",
    "## Output format\n",
    "{\n",
    "  \"Entity Name\": {\n",
    "    \"entity_type\": \"...\",\n",
    "    \"asian_status\": \"...\",\n",
    "    \"emotions\": [\n",
    "      {\n",
    "        \"emotion\": \"deepest matched term or Cannot be inferred\",\n",
    "        \"emotion_reason\": \"Exact sentence(s)\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# ==============================\n",
    "# 主流程 Step2 → Step4\n",
    "# ==============================\n",
    "def run_step4_from_step2(step2_dir=\"step2_batches\", step4_dir=\"step4_batches\"):\n",
    "    os.makedirs(step4_dir, exist_ok=True)\n",
    "    step2_files = sorted(glob.glob(os.path.join(step2_dir, \"step2_batch_*.json\")))\n",
    "    total_batches = len(step2_files)\n",
    "\n",
    "    for idx, step2_file in enumerate(step2_files, start=1):\n",
    "        batch_name = os.path.basename(step2_file).replace(\"step2_\", \"step4_\")\n",
    "        out_path = os.path.join(step4_dir, batch_name)\n",
    "\n",
    "        # 斷點續跑\n",
    "        if os.path.exists(out_path):\n",
    "            print(f\"⏭️ 批次 {idx}/{total_batches} {batch_name} 已存在，跳過\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n🚀 開始處理批次 {idx}/{total_batches}: {step2_file}\")\n",
    "\n",
    "        with open(step2_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            step2_batch_result = json.load(f)\n",
    "\n",
    "        step4_batch_result = {}\n",
    "        emo_stats = Counter()\n",
    "\n",
    "        for title, step2_text in step2_batch_result.items():\n",
    "            full_prompt = (\n",
    "                step4_prompt +\n",
    "                f\"\\n\\nStep 2 Results (Extracted Sentences):\\n{step2_text}\"\n",
    "            )\n",
    "\n",
    "            response = get_response(full_prompt)\n",
    "            step4_batch_result[title] = response\n",
    "            emo_stats[\"done\"] += 1\n",
    "\n",
    "        # 儲存\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(step4_batch_result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"✅ 批次 {idx}/{total_batches} 已完成並儲存至 {out_path}\")\n",
    "        print(\"   Stats:\", dict(emo_stats))\n",
    "\n",
    "# ==============================\n",
    "# 執行\n",
    "# ==============================\n",
    "if __name__ == \"__main__\":\n",
    "    run_step4_from_step2(\"step2_batches\", \"step4_batches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8509778e-2d88-4fd6-9130-661ad83aaa9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Step 3 已合併 12 個批次檔 → step3_all_new.csv，共 6063 筆\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def merge_step3_with_id(input_dir=\"step3_batches\", prefix=\"step3_batch_\", \n",
    "                        output_json=\"step3_all.json\", output_csv=\"step3_all.csv\"):\n",
    "    batch_files = sorted(glob.glob(os.path.join(input_dir, f\"{prefix}*.json\")))\n",
    "\n",
    "    merged_result = {}\n",
    "    for file in batch_files:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            merged_result.update(data)\n",
    "\n",
    "    with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(merged_result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    rows = []\n",
    "    idx = 1\n",
    "    for article_id, entities in merged_result.items():\n",
    "        for entity, meta in entities.items():\n",
    "            rows.append({\n",
    "                \"reaction_id\": f\"reaction_{idx}\",\n",
    "                \"article_id\": article_id,\n",
    "                \"entity\": entity,\n",
    "                \"entity_type\": meta.get(\"entity_type\", \"\"),\n",
    "                \"asian_status\": meta.get(\"asian_status\", \"\"),\n",
    "                \"reaction\": meta.get(\"reaction\", \"\"),\n",
    "                \"reaction_reason\": meta.get(\"reaction_reason\", \"\")\n",
    "            })\n",
    "            idx += 1\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"✅ Step 3 已合併 {len(batch_files)} 個批次檔 → {output_csv}，共 {len(df)} 筆\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    merge_step3_with_id(\n",
    "        input_dir=\"step3_batches_new\",\n",
    "        prefix=\"step3_batch_\",\n",
    "        output_json=\"step3_all_new.json\",\n",
    "        output_csv=\"step3_all_new.csv\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "094b81c0-66dd-4e5b-8e48-f663b5550e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Step 4 已合併 12 個批次檔 → step4_all.csv，共 8011 筆\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def parse_json_loose(s: str):\n",
    "    \"\"\"嘗試從字串裡解析 JSON\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return None\n",
    "    s = s.strip()\n",
    "    if not s:\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        # 嘗試抓第一個 {...}\n",
    "        m = re.search(r\"\\{.*\\}\", s, flags=re.S)\n",
    "        if m:\n",
    "            try:\n",
    "                return json.loads(m.group(0))\n",
    "            except Exception:\n",
    "                return None\n",
    "    return None\n",
    "\n",
    "def merge_step4_with_id(input_dir=\"step4_batches\", prefix=\"step4_batch_\", \n",
    "                        output_json=\"step4_all.json\", output_csv=\"step4_all.csv\"):\n",
    "    batch_files = sorted(glob.glob(os.path.join(input_dir, f\"{prefix}*.json\")))\n",
    "    all_articles = {}\n",
    "\n",
    "    for file in batch_files:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # ⚠️ 不用 update，逐篇展開\n",
    "        for article_id, raw in data.items():\n",
    "            if isinstance(raw, dict):\n",
    "                all_articles[article_id] = raw\n",
    "            elif isinstance(raw, str):\n",
    "                parsed = parse_json_loose(raw)\n",
    "                if isinstance(parsed, dict):\n",
    "                    all_articles[article_id] = parsed\n",
    "\n",
    "    # 存 JSON\n",
    "    with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_articles, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # 轉成 CSV\n",
    "    rows = []\n",
    "    idx = 1\n",
    "    for article_id, entities in all_articles.items():\n",
    "        for entity, meta in entities.items():\n",
    "            emotions = meta.get(\"emotions\", [])\n",
    "            \n",
    "            # 統一處理格式\n",
    "            if isinstance(emotions, str):\n",
    "                emotions = [{\n",
    "                    \"emotion\": emotions,\n",
    "                    \"emotion_path\": None,\n",
    "                    \"emotion_reason\": \"\"\n",
    "                }]\n",
    "            elif not isinstance(emotions, list):\n",
    "                emotions = []\n",
    "    \n",
    "            for emo in emotions:\n",
    "                if not isinstance(emo, dict):  # 再保險一次\n",
    "                    emo = {\"emotion\": str(emo), \"emotion_path\": None, \"emotion_reason\": \"\"}\n",
    "                rows.append({\n",
    "                    \"emotion_id\": f\"emotion_{idx}\",\n",
    "                    \"article_id\": article_id,\n",
    "                    \"entity\": entity,\n",
    "                    \"entity_type\": meta.get(\"entity_type\", \"\"),\n",
    "                    \"asian_status\": meta.get(\"asian_status\", \"\"),\n",
    "                    \"emotion\": emo.get(\"emotion\", \"\"),\n",
    "                    \"emotion_path\": emo.get(\"emotion_path\", \"\"),\n",
    "                    \"emotion_reason\": emo.get(\"emotion_reason\", \"\")\n",
    "                })\n",
    "                idx += 1\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"✅ Step 4 已合併 {len(batch_files)} 個批次檔 → {output_csv}，共 {len(df)} 筆\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    merge_step4_with_id(\n",
    "        input_dir=\"step4_batches\",\n",
    "        prefix=\"step4_batch_\",\n",
    "        output_json=\"step4_all.json\",\n",
    "        output_csv=\"step4_all.csv\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b719cfd3-826a-4595-a245-3dbb745cf6d6",
   "metadata": {},
   "source": [
    "# 處理錯誤文章"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "7778de67-dfe2-41b7-9c7c-b4ba74fa673f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Debugging Article_440\n",
      "\n",
      "🔎 Debugging Article_465\n",
      "✅ 已完成 ['Article_440', 'Article_465']，輸出 step3_batches_debug\\step3_batch_9__subset_debug.json\n",
      "   Gate stats: {}\n",
      "   Label stats: {'Politicians initiated anti-Asian hate and racism': 1, 'Color blind/minimizing racism': 2, 'Support Asian Americans': 2, 'Feeling hopeless or support AAPI being not enough': 2, 'Advocacy/take actions for changes': 1, 'Cannot be inferred': 3}\n"
     ]
    }
   ],
   "source": [
    "import os, json, re\n",
    "from collections import Counter\n",
    "from typing import Any, Dict\n",
    "from openai import OpenAI\n",
    "\n",
    "# ==============================\n",
    "# OpenAI client (替換成你自己的 key)\n",
    "# ==============================\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "def get_response(prompt: str, temperature: float = 0.0) -> str:\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        return resp.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(\"❌ API error:\", e)\n",
    "        return \"\"\n",
    "\n",
    "# ==============================\n",
    "# JSON utilities (寬鬆 parser)\n",
    "# ==============================\n",
    "def strip_code_fences(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return s\n",
    "    s = s.strip()\n",
    "    m = re.search(r\"```(?:json)?\\s*([\\s\\S]*?)\\s*```\", s, flags=re.I)\n",
    "    return m.group(1).strip() if m else s\n",
    "\n",
    "def _sanitize_json_like(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = strip_code_fences(s).lstrip(\"\\ufeff\")\n",
    "    # 移除不可見控制字元（保留換行/縮排）\n",
    "    s = \"\".join(ch for ch in s if ch.isprintable() or ch in \"\\n\\r\\t\")\n",
    "    # 嘗試修正常見 mojibake\n",
    "    if \"â€\" in s or \"Ã\" in s:\n",
    "        try:\n",
    "            s = s.encode(\"latin1\").decode(\"utf-8\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    # 取最外層 {...} 或 [...]\n",
    "    m = re.search(r\"(\\{[\\s\\S]*\\}|\\[[\\s\\S]*\\])\", s)\n",
    "    if m:\n",
    "        s = m.group(1)\n",
    "    # 移除尾逗號\n",
    "    s = re.sub(r\",(\\s*[}\\]])\", r\"\\1\", s)\n",
    "    return s.strip()\n",
    "\n",
    "def parse_json_loose(s: str):\n",
    "    if not isinstance(s, str):\n",
    "        return None\n",
    "    base = _sanitize_json_like(s)\n",
    "    if not base:\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(base)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # 單引號 pseudo-JSON\n",
    "    if base.startswith(\"{\") and base.endswith(\"}\") and '\"' not in base and \"'\" in base:\n",
    "        try:\n",
    "            return json.loads(re.sub(r\"'\", '\"', base))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def parse_model_json(s: str, default: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return default\n",
    "    cleaned = _sanitize_json_like(s)\n",
    "    try:\n",
    "        return json.loads(cleaned)\n",
    "    except Exception:\n",
    "        m = re.search(r\"\\{[\\s\\S]*\\}\", cleaned)\n",
    "        if m:\n",
    "            try:\n",
    "                return json.loads(m.group(0))\n",
    "            except Exception:\n",
    "                return default\n",
    "        return default\n",
    "\n",
    "def to_text(x: Any) -> str:\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    if isinstance(x, str):\n",
    "        return x\n",
    "    if isinstance(x, list):\n",
    "        return \"\\n\".join(str(item) for item in x)\n",
    "    if isinstance(x, dict):\n",
    "        if \"relevant_sentences\" in x:\n",
    "            return to_text(x[\"relevant_sentences\"])\n",
    "        return \"\\n\".join(to_text(v) for v in x.values())\n",
    "    return str(x)\n",
    "\n",
    "def normalize_step2_result(title: str, raw_obj) -> Dict[str, Dict[str, str]]:\n",
    "    if isinstance(raw_obj, dict) and all(isinstance(v, dict) for v in raw_obj.values()):\n",
    "        out = {}\n",
    "        for k, v in raw_obj.items():\n",
    "            out[k] = {\n",
    "                \"entity_type\": v.get(\"entity_type\", \"\"),\n",
    "                \"asian_status\": v.get(\"asian_status\", \"\"),\n",
    "                \"relevant_sentences\": to_text(v.get(\"relevant_sentences\"))\n",
    "            }\n",
    "        return out\n",
    "    if isinstance(raw_obj, str):\n",
    "        parsed = parse_json_loose(raw_obj)\n",
    "        if parsed is not None: \n",
    "            return normalize_step2_result(title, parsed)\n",
    "    return {}\n",
    "\n",
    "def debug_json_failure(s: str, context=2):\n",
    "    cleaned = _sanitize_json_like(s)\n",
    "    try:\n",
    "        json.loads(cleaned)\n",
    "        print(\"✅ JSON OK\")\n",
    "        return\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"❌ JSON error @ line {e.lineno}, col {e.colno}: {e.msg}\")\n",
    "        lines = cleaned.splitlines()\n",
    "        i = e.lineno - 1\n",
    "        lo, hi = max(0, i-context), min(len(lines), i+context+1)\n",
    "        for idx in range(lo, hi):\n",
    "            mark = \">>\" if idx == i else \"  \"\n",
    "            print(f\"{mark} {idx+1:4d}: {lines[idx]}\")\n",
    "\n",
    "# ==============================\n",
    "# Prompts\n",
    "# ==============================\n",
    "def build_gate_prompt(step2_sentences: str) -> str:\n",
    "    return f\"\"\"Task: Decide if the extracted sentences show any observable reaction.\n",
    "\n",
    "Rules:\n",
    "- Observable = action/inaction or explicit stance\n",
    "- Pure emotions or incident descriptions ≠ reaction\n",
    "\n",
    "Output JSON:\n",
    "{{\n",
    "  \"has_reaction\": \"yes\" | \"no\",\n",
    "  \"evidence\": \"Exact sentence(s) from relevant_sentences\"\n",
    "}}\n",
    "\n",
    "relevant_sentences:\n",
    "{step2_sentences}\n",
    "\"\"\"\n",
    "\n",
    "def build_classifier_prompt(entity_type: str, asian_status: str, step2_sentences: str) -> str:\n",
    "    return f\"\"\"Task: Classify the entity’s REACTION strictly using the Reaction Concept Tree.\n",
    "\n",
    "Reaction Concept Tree:\n",
    "- Support Asian Americans\n",
    "  - Attending marches/rallies\n",
    "  - Speaking up on social media\n",
    "  - Calling for being united\n",
    "  - Educating students\n",
    "  - Fostering conversations about anti-Asian hate\n",
    "  - Hiring security guards\n",
    "  - Providing shopkeepers with air horns\n",
    "  - Rewarding the public to report suspects\n",
    "- Advocacy/take actions for changes\n",
    "- Politicians initiated anti-Asian hate and racism\n",
    "- Undermining human rights\n",
    "- Color blind/minimizing racism\n",
    "- Youth as not an excuse\n",
    "- Videotaping/confronting harasser/attacker\n",
    "- Sex (sexual) addiction\n",
    "- Religion as a reason\n",
    "- Feeling hopeless or support AAPI being not enough\n",
    "- Not confronting attacker/harasser or not reporting\n",
    "- Useless law enforcement\n",
    "  - Did not take a report\n",
    "  - Did not patrol the streets\n",
    "- Government takes actions to stop AAPI hate\n",
    "  - Installing hotlines\n",
    "  - Launching a hate-crime task force\n",
    "  - Increasing patrols\n",
    "  - Organizing a town hall\n",
    "\n",
    "Rules:\n",
    "- If no clear reaction → \"Cannot be inferred\"\n",
    "- Always choose the most specific subcategory\n",
    "\n",
    "entity_type: {entity_type}\n",
    "asian_status: {asian_status}\n",
    "\n",
    "relevant_sentences:\n",
    "{step2_sentences}\n",
    "\n",
    "Output JSON:\n",
    "{{\n",
    "  \"reaction\": \"<one label or 'Cannot be inferred'>\",\n",
    "  \"reaction_reason\": \"Exact sentence(s)\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "# ==============================\n",
    "# Step3 主流程 + Debug\n",
    "# ==============================\n",
    "def run_step3_for_titles(step2_file=\"step2_batches/step2_batch_1.json\",\n",
    "                         titles=None,\n",
    "                         out_dir=\"step3_batches_debug\",\n",
    "                         out_suffix=\"__subset_debug\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    with open(step2_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        step2_batch_result = json.load(f)\n",
    "\n",
    "    if titles is None:\n",
    "        titles = [next(iter(step2_batch_result.keys()), None)]\n",
    "\n",
    "    filtered = {t: step2_batch_result[t] for t in titles if t in step2_batch_result}\n",
    "    if not filtered:\n",
    "        print(\"找不到指定的 title\")\n",
    "        return\n",
    "\n",
    "    gate_stats = Counter()\n",
    "    label_stats = Counter()\n",
    "    step3_batch_result = {}\n",
    "\n",
    "    for title, raw in filtered.items():\n",
    "        print(f\"\\n🔎 Debugging {title}\")\n",
    "        entities = normalize_step2_result(title, raw)\n",
    "        if not entities:\n",
    "            print(\"⚠️ 無法解析，呼叫 debug_json_failure\")\n",
    "            debug_json_failure(raw)\n",
    "            continue\n",
    "\n",
    "        entity_outputs = {}\n",
    "        for entity, meta in entities.items():\n",
    "            entity_type = meta.get(\"entity_type\", \"\")\n",
    "            asian_status = meta.get(\"asian_status\", \"\")\n",
    "            relevant_sentences = to_text(meta.get(\"relevant_sentences\", \"\")).strip()\n",
    "\n",
    "            # --- Gate\n",
    "            gate_resp = get_response(build_gate_prompt(relevant_sentences))\n",
    "            gate_json = parse_model_json(gate_resp, default={\"has_reaction\": \"no\", \"evidence\": \"\"})\n",
    "            if str(gate_json.get(\"has_reaction\", \"no\")).lower() != \"yes\":\n",
    "                out = {\n",
    "                    \"entity_type\": entity_type,\n",
    "                    \"asian_status\": asian_status,\n",
    "                    \"reaction\": \"Cannot be inferred\",\n",
    "                    \"reaction_reason\": \"\"\n",
    "                }\n",
    "                entity_outputs[entity] = out\n",
    "                label_stats[\"Cannot be inferred\"] += 1\n",
    "                continue\n",
    "\n",
    "            # --- Classifier\n",
    "            cls_resp = get_response(build_classifier_prompt(entity_type, asian_status, relevant_sentences))\n",
    "            cls_json = parse_model_json(cls_resp, default={\"reaction\": \"Cannot be inferred\", \"reaction_reason\": \"\"})\n",
    "            out = {\n",
    "                \"entity_type\": entity_type,\n",
    "                \"asian_status\": asian_status,\n",
    "                \"reaction\": cls_json.get(\"reaction\", \"Cannot be inferred\"),\n",
    "                \"reaction_reason\": cls_json.get(\"reaction_reason\", \"\")\n",
    "            }\n",
    "            entity_outputs[entity] = out\n",
    "            label_stats[out[\"reaction\"]] += 1\n",
    "\n",
    "        step3_batch_result[title] = entity_outputs\n",
    "\n",
    "    base = os.path.basename(step2_file).replace(\"step2_\", \"step3_\").replace(\".json\", f\"{out_suffix}.json\")\n",
    "    out_path = os.path.join(out_dir, base)\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(step3_batch_result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"✅ 已完成 {list(step3_batch_result.keys())}，輸出 {out_path}\")\n",
    "    print(\"   Gate stats:\", dict(gate_stats))\n",
    "    print(\"   Label stats:\", dict(label_stats))\n",
    "\n",
    "\n",
    "# 或跑多篇\n",
    "run_step3_for_titles(\"step2_batches/step2_batch_9.json\", titles=[\"Article_440\",\n",
    "    \"Article_465\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "b9f70092-e754-4bdb-95e5-c5116e53a868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Debugging Article_40\n",
      "✅ 已完成 ['Article_40']，輸出 step4_batches_debug\\step4_batch_1__subset_debug.json\n",
      "   Stats: {'done': 1}\n",
      "\n",
      "🔎 Debugging Article_255\n",
      "✅ 已完成 ['Article_255']，輸出 step4_batches_debug\\step4_batch_5__subset_debug.json\n",
      "   Stats: {'done': 1}\n",
      "\n",
      "🔎 Debugging Article_349\n",
      "\n",
      "🔎 Debugging Article_350\n",
      "\n",
      "🔎 Debugging Article_379\n",
      "✅ 已完成 ['Article_349', 'Article_350', 'Article_379']，輸出 step4_batches_debug\\step4_batch_7__subset_debug.json\n",
      "   Stats: {'done': 3}\n",
      "\n",
      "🔎 Debugging Article_429\n",
      "✅ 已完成 ['Article_429']，輸出 step4_batches_debug\\step4_batch_8__subset_debug.json\n",
      "   Stats: {'done': 1}\n",
      "\n",
      "🔎 Debugging Article_524\n",
      "✅ 已完成 ['Article_524']，輸出 step4_batches_debug\\step4_batch_10__subset_debug.json\n",
      "   Stats: {'done': 1}\n",
      "\n",
      "🔎 Debugging Article_553\n",
      "✅ 已完成 ['Article_553']，輸出 step4_batches_debug\\step4_batch_11__subset_debug.json\n",
      "   Stats: {'done': 1}\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os, json, re\n",
    "from collections import Counter\n",
    "from typing import Any, Dict\n",
    "from openai import OpenAI\n",
    "\n",
    "# ==============================\n",
    "# OpenAI client\n",
    "# ==============================\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "def get_response(prompt: str, temperature: float = 0.0) -> str:\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        return resp.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(\"❌ API error:\", e)\n",
    "        return \"{}\"\n",
    "\n",
    "# ==============================\n",
    "# JSON utilities\n",
    "# ==============================\n",
    "def strip_code_fences(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return s\n",
    "    s = s.strip()\n",
    "    m = re.search(r\"```(?:json)?\\s*([\\s\\S]*?)\\s*```\", s, flags=re.I)\n",
    "    return m.group(1).strip() if m else s\n",
    "\n",
    "def _sanitize_json_like(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = strip_code_fences(s).lstrip(\"\\ufeff\")\n",
    "    s = \"\".join(ch for ch in s if ch.isprintable() or ch in \"\\n\\r\\t\")\n",
    "    m = re.search(r\"(\\{[\\s\\S]*\\}|\\[[\\s\\S]*\\])\", s)\n",
    "    if m:\n",
    "        s = m.group(1)\n",
    "    s = re.sub(r\",(\\s*[}\\]])\", r\"\\1\", s)\n",
    "    return s.strip()\n",
    "\n",
    "def parse_model_json(s: str, default: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return default\n",
    "    cleaned = _sanitize_json_like(s)\n",
    "    try:\n",
    "        return json.loads(cleaned)\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "def to_text(x: Any) -> str:\n",
    "    if x is None: return \"\"\n",
    "    if isinstance(x, str): return x\n",
    "    if isinstance(x, list): return \"\\n\".join(str(item) for item in x)\n",
    "    if isinstance(x, dict):\n",
    "        if \"relevant_sentences\" in x:\n",
    "            return to_text(x[\"relevant_sentences\"])\n",
    "        return \"\\n\".join(to_text(v) for v in x.values())\n",
    "    return str(x)\n",
    "\n",
    "def normalize_step2_result(title: str, raw_obj) -> Dict[str, Dict[str, str]]:\n",
    "    if isinstance(raw_obj, dict) and all(isinstance(v, dict) for v in raw_obj.values()):\n",
    "        out = {}\n",
    "        for k, v in raw_obj.items():\n",
    "            out[k] = {\n",
    "                \"entity_type\": v.get(\"entity_type\", \"\"),\n",
    "                \"asian_status\": v.get(\"asian_status\", \"\"),\n",
    "                \"relevant_sentences\": to_text(v.get(\"relevant_sentences\"))\n",
    "            }\n",
    "        return out\n",
    "    return {}\n",
    "\n",
    "# ==============================\n",
    "# Step4 Prompt\n",
    "# ==============================\n",
    "step4_prompt = \"\"\"You are a sociology professor with 30 years of experience analyzing racial dynamics and anti-Asian racism in the United States.\n",
    "\n",
    "### Step 4: Infer **Emotions and Their Intensity**\n",
    "\n",
    "Your task is to analyze the **extracted sentences** from **Step 2** and infer each entity's **emotional stance** toward anti-Asian hate.\n",
    "\n",
    "You will also receive metadata from Step 2, including:\n",
    "- `entity_type`\n",
    "- `asian_status`\n",
    "\n",
    "Use only the exact `relevant_sentences` from Step 2 as your source.\n",
    "\n",
    "---\n",
    "\n",
    "## Emotion Concept Tree\n",
    "- Love \n",
    "- Joy \n",
    "- Anger \n",
    "- Sadness \n",
    "- Fear \n",
    "- Surprise \n",
    "\n",
    "---\n",
    "\n",
    "## Instructions\n",
    "1. Focus only on emotions — do NOT infer actions.\n",
    "2. If no emotion is expressed, output `\"emotion\": \"Cannot be inferred\"`.\n",
    "3. If multiple emotions appear, list multiple objects.\n",
    "4. Use the exact sentence(s) as `\"emotion_reason\"`.\n",
    "\n",
    "---\n",
    "\n",
    "## Output format\n",
    "{\n",
    "  \"Entity Name\": {\n",
    "    \"entity_type\": \"...\",\n",
    "    \"asian_status\": \"...\",\n",
    "    \"emotions\": [\n",
    "      {\n",
    "        \"emotion\": \"deepest matched term or Cannot be inferred\",\n",
    "        \"emotion_reason\": \"Exact sentence(s)\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# ==============================\n",
    "# Step4 主流程 + Debug\n",
    "# ==============================\n",
    "def run_step4_for_titles(step2_file=\"step2_batches/step2_batch_1.json\",\n",
    "                         titles=None,\n",
    "                         out_dir=\"step4_batches_debug\",\n",
    "                         out_suffix=\"__subset_debug\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    with open(step2_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        step2_batch_result = json.load(f)\n",
    "\n",
    "    if titles is None:\n",
    "        titles = [next(iter(step2_batch_result.keys()), None)]\n",
    "\n",
    "    filtered = {t: step2_batch_result[t] for t in titles if t in step2_batch_result}\n",
    "    if not filtered:\n",
    "        print(\"找不到指定的 title\")\n",
    "        return\n",
    "\n",
    "    step4_batch_result = {}\n",
    "    emo_stats = Counter()\n",
    "\n",
    "    for title, raw in filtered.items():\n",
    "        print(f\"\\n🔎 Debugging {title}\")\n",
    "        entities = normalize_step2_result(title, raw)\n",
    "        if not entities:\n",
    "            print(f\"⚠️ {title} 無法解析 step2 結果\")\n",
    "            continue\n",
    "\n",
    "        entity_outputs = {}\n",
    "        for entity, meta in entities.items():\n",
    "            entity_type = meta.get(\"entity_type\", \"\")\n",
    "            asian_status = meta.get(\"asian_status\", \"\")\n",
    "            relevant_sentences = to_text(meta.get(\"relevant_sentences\", \"\")).strip()\n",
    "\n",
    "            full_prompt = (\n",
    "                step4_prompt +\n",
    "                f\"\\n\\nEntity: {entity}\\nentity_type: {entity_type}\\nasian_status: {asian_status}\\nrelevant_sentences:\\n{relevant_sentences}\"\n",
    "            )\n",
    "\n",
    "            resp = get_response(full_prompt)\n",
    "            parsed = parse_model_json(resp, default={\n",
    "                entity: {\n",
    "                    \"entity_type\": entity_type,\n",
    "                    \"asian_status\": asian_status,\n",
    "                    \"emotions\": [{\"emotion\": \"Cannot be inferred\", \"emotion_reason\": \"\"}]\n",
    "                }\n",
    "            })\n",
    "            entity_outputs[entity] = parsed.get(entity, parsed)\n",
    "\n",
    "        step4_batch_result[title] = entity_outputs\n",
    "        emo_stats[\"done\"] += 1\n",
    "\n",
    "    # === 輸出 ===\n",
    "    base = os.path.basename(step2_file).replace(\"step2_\", \"step4_\").replace(\".json\", f\"{out_suffix}.json\")\n",
    "    out_path = os.path.join(out_dir, base)\n",
    "\n",
    "    # 如果檔案已經存在 → 合併舊結果\n",
    "    if os.path.exists(out_path):\n",
    "        with open(out_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            old_data = json.load(f)\n",
    "        old_data.update(step4_batch_result)\n",
    "        step4_batch_result = old_data\n",
    "\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(step4_batch_result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"✅ 已完成 {list(filtered.keys())}，輸出 {out_path}\")\n",
    "    print(\"   Stats:\", dict(emo_stats))\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 執行範例\n",
    "# ==============================\n",
    "run_step4_for_titles(\"step2_batches/step2_batch_1.json\", titles=[\"Article_40\"])\n",
    "run_step4_for_titles(\"step2_batches/step2_batch_5.json\", titles=[\"Article_255\"])\n",
    "run_step4_for_titles(\"step2_batches/step2_batch_7.json\", titles=[\"Article_349\", \"Article_350\", \"Article_379\"])\n",
    "run_step4_for_titles(\"step2_batches/step2_batch_8.json\", titles=[\"Article_429\"])\n",
    "run_step4_for_titles(\"step2_batches/step2_batch_10.json\", titles=[\"Article_524\"])\n",
    "run_step4_for_titles(\"step2_batches/step2_batch_11.json\", titles=[\"Article_553\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "7495c32c-97c5-4f61-aa7b-fb9db37cc517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Re-running Article_220\n",
      "✅ Article_220 已完成\n",
      "💾 已寫回 step3_batches_debug\\step3_batch_5__subset_debug.json，目前共 7 篇\n"
     ]
    }
   ],
   "source": [
    "def rerun_step3_to_subset(step2_file, titles, out_dir=\"step3_batches_debug\", out_suffix=\"__subset_debug\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # 讀 step2\n",
    "    with open(step2_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        step2_batch_result = json.load(f)\n",
    "\n",
    "    # 過濾出要跑的文章\n",
    "    filtered = {t: step2_batch_result[t] for t in titles if t in step2_batch_result}\n",
    "    if not filtered:\n",
    "        print(\"⚠️ 找不到指定的 titles\")\n",
    "        return\n",
    "\n",
    "    step3_batch_result = {}\n",
    "\n",
    "    for title, raw in filtered.items():\n",
    "        print(f\"\\n🔎 Re-running {title}\")\n",
    "        entities = normalize_step2_result(title, raw)\n",
    "        if not entities:\n",
    "            print(f\"⚠️ {title} step2 無法解析\")\n",
    "            debug_json_failure(raw)\n",
    "            continue\n",
    "\n",
    "        entity_outputs = {}\n",
    "        for entity, meta in entities.items():\n",
    "            entity_type = meta.get(\"entity_type\", \"\")\n",
    "            asian_status = meta.get(\"asian_status\", \"\")\n",
    "            relevant_sentences = to_text(meta.get(\"relevant_sentences\", \"\")).strip()\n",
    "\n",
    "            # --- Gate\n",
    "            gate_resp = get_response(build_gate_prompt(relevant_sentences))\n",
    "            gate_json = parse_model_json(gate_resp, default={\"has_reaction\": \"no\", \"evidence\": \"\"})\n",
    "            if str(gate_json.get(\"has_reaction\", \"no\")).lower() != \"yes\":\n",
    "                entity_outputs[entity] = {\n",
    "                    \"entity_type\": entity_type,\n",
    "                    \"asian_status\": asian_status,\n",
    "                    \"reaction\": \"Cannot be inferred\",\n",
    "                    \"reaction_reason\": \"\"\n",
    "                }\n",
    "                continue\n",
    "\n",
    "            # --- Classifier\n",
    "            cls_resp = get_response(build_classifier_prompt(entity_type, asian_status, relevant_sentences))\n",
    "            cls_json = parse_model_json(cls_resp, default={\"reaction\": \"Cannot be inferred\", \"reaction_reason\": \"\"})\n",
    "            entity_outputs[entity] = {\n",
    "                \"entity_type\": entity_type,\n",
    "                \"asian_status\": asian_status,\n",
    "                \"reaction\": cls_json.get(\"reaction\", \"Cannot be inferred\"),\n",
    "                \"reaction_reason\": cls_json.get(\"reaction_reason\", \"\")\n",
    "            }\n",
    "\n",
    "        step3_batch_result[title] = entity_outputs\n",
    "        print(f\"✅ {title} 已完成\")\n",
    "\n",
    "    # 輸出到 subset 檔案\n",
    "    base = os.path.basename(step2_file).replace(\"step2_\", \"step3_\").replace(\".json\", f\"{out_suffix}.json\")\n",
    "    out_path = os.path.join(out_dir, base)\n",
    "\n",
    "    # 如果 subset 檔案已存在 → 合併\n",
    "    if os.path.exists(out_path):\n",
    "        with open(out_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            existing = json.load(f)\n",
    "    else:\n",
    "        existing = {}\n",
    "\n",
    "    existing.update(step3_batch_result)\n",
    "\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(existing, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"💾 已寫回 {out_path}，目前共 {len(existing)} 篇\")\n",
    "\n",
    "rerun_step3_to_subset(\n",
    "    step2_file=\"step2_batches/step2_batch_5.json\",\n",
    "    titles=[\"Article_220\"],\n",
    "    out_dir=\"step3_batches_debug\",\n",
    "    out_suffix=\"__subset_debug\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "8d13f2b8-486d-43b5-bd9e-b573fb898179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 載入既有 step3_all.json，已有 584 篇文章\n",
      "🔍 偵測到 11 個 subset 檔案\n",
      "✅ 合併 step3_batch_10__subset_debug.json，+0 篇\n",
      "✅ 合併 step3_batch_11__subset_debug.json，+0 篇\n",
      "✅ 合併 step3_batch_1__subset_debug.json，+0 篇\n",
      "✅ 合併 step3_batch_2__subset_debug.json，+0 篇\n",
      "✅ 合併 step3_batch_3__subset_debug.json，+0 篇\n",
      "✅ 合併 step3_batch_4__subset_debug.json，+0 篇\n",
      "✅ 合併 step3_batch_5__subset_debug.json，+0 篇\n",
      "✅ 合併 step3_batch_6__subset_debug.json，+0 篇\n",
      "✅ 合併 step3_batch_7__subset_debug.json，+0 篇\n",
      "✅ 合併 step3_batch_8__subset_debug.json，+0 篇\n",
      "✅ 合併 step3_batch_9__subset_debug.json，+0 篇\n",
      "💾 已更新 step3_all.json 和 step3_all.csv，共 6063 筆 reactions\n"
     ]
    }
   ],
   "source": [
    "# 修復後的結果合併到step3_all\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def merge_step3_with_existing(all_json=\"step3_all.json\", all_csv=\"step3_all.csv\",\n",
    "                              subset_dir=\"step3_batches_debug\",\n",
    "                              prefix=\"step3_batch_\", suffix=\"__subset_debug.json\"):\n",
    "    # 先讀舊的 all.json\n",
    "    if os.path.exists(all_json):\n",
    "        with open(all_json, \"r\", encoding=\"utf-8\") as f:\n",
    "            merged_result = json.load(f)\n",
    "        print(f\"📂 載入既有 {all_json}，已有 {len(merged_result)} 篇文章\")\n",
    "    else:\n",
    "        merged_result = {}\n",
    "        print(f\"⚠️ 找不到 {all_json}，建立新檔案\")\n",
    "\n",
    "    # 找 subset 檔案\n",
    "    subset_files = sorted(glob.glob(os.path.join(subset_dir, f\"{prefix}*{suffix}\")))\n",
    "    print(f\"🔍 偵測到 {len(subset_files)} 個 subset 檔案\")\n",
    "\n",
    "    # 合併 subset → all\n",
    "    for file in subset_files:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            before = len(merged_result)\n",
    "            merged_result.update(data)  # ⚠️ 若有同一篇文章，會覆蓋\n",
    "            after = len(merged_result)\n",
    "        print(f\"✅ 合併 {os.path.basename(file)}，+{after-before} 篇\")\n",
    "\n",
    "    # 存回 all.json\n",
    "    with open(all_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(merged_result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # 轉成 CSV\n",
    "    rows = []\n",
    "    idx = 1\n",
    "    for article_id, entities in merged_result.items():\n",
    "        for entity, meta in entities.items():\n",
    "            rows.append({\n",
    "                \"reaction_id\": f\"reaction_{idx}\",\n",
    "                \"article_id\": article_id,\n",
    "                \"entity\": entity,\n",
    "                \"entity_type\": meta.get(\"entity_type\", \"\"),\n",
    "                \"asian_status\": meta.get(\"asian_status\", \"\"),\n",
    "                \"reaction\": meta.get(\"reaction\", \"\"),\n",
    "                \"reaction_reason\": meta.get(\"reaction_reason\", \"\")\n",
    "            })\n",
    "            idx += 1\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(all_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"💾 已更新 {all_json} 和 {all_csv}，共 {len(df)} 筆 reactions\")\n",
    "\n",
    "# 使用範例\n",
    "if __name__ == \"__main__\":\n",
    "    merge_step3_with_existing(\n",
    "        all_json=\"step3_all.json\",\n",
    "        all_csv=\"step3_all.csv\",\n",
    "        subset_dir=\"step3_batches_debug\",\n",
    "        prefix=\"step3_batch_\",\n",
    "        suffix=\"__subset_debug.json\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b0ecdafc-1264-4916-a3e1-8d703b730f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 載入既有 step4_all.json，已有 584 篇文章\n",
      "🔍 偵測到 6 個 subset 檔案\n",
      "✅ 合併 step4_batch_10__subset_debug.json，+0 篇\n",
      "✅ 合併 step4_batch_11__subset_debug.json，+0 篇\n",
      "✅ 合併 step4_batch_1__subset_debug.json，+0 篇\n",
      "✅ 合併 step4_batch_5__subset_debug.json，+0 篇\n",
      "✅ 合併 step4_batch_7__subset_debug.json，+0 篇\n",
      "✅ 合併 step4_batch_8__subset_debug.json，+0 篇\n",
      "💾 已更新 step4_all.json 和 step4_all.csv，共 8127 筆 emotions\n"
     ]
    }
   ],
   "source": [
    "# 修復後的結果合併到step4_all\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def merge_step4_with_existing(all_json=\"step4_all.json\", all_csv=\"step4_all.csv\",\n",
    "                              subset_dir=\"step4_batches_debug\",\n",
    "                              prefix=\"step4_batch_\", suffix=\"__subset_debug.json\"):\n",
    "    # 先讀舊的 all.json\n",
    "    if os.path.exists(all_json):\n",
    "        with open(all_json, \"r\", encoding=\"utf-8\") as f:\n",
    "            merged_result = json.load(f)\n",
    "        print(f\"📂 載入既有 {all_json}，已有 {len(merged_result)} 篇文章\")\n",
    "    else:\n",
    "        merged_result = {}\n",
    "        print(f\"⚠️ 找不到 {all_json}，建立新檔案\")\n",
    "\n",
    "    # 找 subset 檔案\n",
    "    subset_files = sorted(glob.glob(os.path.join(subset_dir, f\"{prefix}*{suffix}\")))\n",
    "    print(f\"🔍 偵測到 {len(subset_files)} 個 subset 檔案\")\n",
    "\n",
    "    # 合併 subset → all\n",
    "    for file in subset_files:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            before = len(merged_result)\n",
    "            merged_result.update(data)  # ⚠️ 若有同一篇文章，會覆蓋\n",
    "            after = len(merged_result)\n",
    "        print(f\"✅ 合併 {os.path.basename(file)}，+{after-before} 篇\")\n",
    "\n",
    "    # 存回 all.json\n",
    "    with open(all_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(merged_result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # 轉成 CSV\n",
    "    rows = []\n",
    "    idx = 1\n",
    "    for article_id, entities in merged_result.items():\n",
    "        for entity, meta in entities.items():\n",
    "            emotions = meta.get(\"emotions\", [])\n",
    "\n",
    "            # emotions 可能是字串或 list\n",
    "            if isinstance(emotions, str):\n",
    "                rows.append({\n",
    "                    \"emotion_id\": f\"emotion_{idx}\",\n",
    "                    \"article_id\": article_id,\n",
    "                    \"entity\": entity,\n",
    "                    \"entity_type\": meta.get(\"entity_type\", \"\"),\n",
    "                    \"asian_status\": meta.get(\"asian_status\", \"\"),\n",
    "                    \"emotion\": emotions,\n",
    "                    \"emotion_reason\": \"\"\n",
    "                })\n",
    "                idx += 1\n",
    "\n",
    "            elif isinstance(emotions, list):\n",
    "                if not emotions:  # 空 list\n",
    "                    rows.append({\n",
    "                        \"emotion_id\": f\"emotion_{idx}\",\n",
    "                        \"article_id\": article_id,\n",
    "                        \"entity\": entity,\n",
    "                        \"entity_type\": meta.get(\"entity_type\", \"\"),\n",
    "                        \"asian_status\": meta.get(\"asian_status\", \"\"),\n",
    "                        \"emotion\": \"Cannot be inferred\",\n",
    "                        \"emotion_reason\": \"\"\n",
    "                    })\n",
    "                    idx += 1\n",
    "                else:\n",
    "                    for emo in emotions:\n",
    "                        if isinstance(emo, dict):\n",
    "                            rows.append({\n",
    "                                \"emotion_id\": f\"emotion_{idx}\",\n",
    "                                \"article_id\": article_id,\n",
    "                                \"entity\": entity,\n",
    "                                \"entity_type\": meta.get(\"entity_type\", \"\"),\n",
    "                                \"asian_status\": meta.get(\"asian_status\", \"\"),\n",
    "                                \"emotion\": emo.get(\"emotion\", \"Cannot be inferred\"),\n",
    "                                \"emotion_reason\": emo.get(\"emotion_reason\", \"\")\n",
    "                            })\n",
    "                        else:  # list 裡還是字串\n",
    "                            rows.append({\n",
    "                                \"emotion_id\": f\"emotion_{idx}\",\n",
    "                                \"article_id\": article_id,\n",
    "                                \"entity\": entity,\n",
    "                                \"entity_type\": meta.get(\"entity_type\", \"\"),\n",
    "                                \"asian_status\": meta.get(\"asian_status\", \"\"),\n",
    "                                \"emotion\": str(emo),\n",
    "                                \"emotion_reason\": \"\"\n",
    "                            })\n",
    "                        idx += 1\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(all_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"💾 已更新 {all_json} 和 {all_csv}，共 {len(df)} 筆 emotions\")\n",
    "\n",
    "\n",
    "# 使用範例\n",
    "if __name__ == \"__main__\":\n",
    "    merge_step4_with_existing(\n",
    "        all_json=\"step4_all.json\",\n",
    "        all_csv=\"step4_all.csv\",\n",
    "        subset_dir=\"step4_batches_debug\",\n",
    "        prefix=\"step4_batch_\",\n",
    "        suffix=\"__subset_debug.json\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "f21615a2-63d8-476f-aa5c-61a2ecfd3787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Article_349 已修復並寫回 step2_batches/step2_batch_7.json\n"
     ]
    }
   ],
   "source": [
    "# 自動修復\n",
    "\n",
    "import json, re\n",
    "\n",
    "def fix_relevant_sentences_block(txt: str) -> str:\n",
    "    \"\"\"\n",
    "    找出 relevant_sentences 區塊，把裡面的句子逐條修正\n",
    "    \"\"\"\n",
    "    def fix_sentence_block(match):\n",
    "        block = match.group(0)\n",
    "        # 抽取句子內容（刪掉 JSON 格式）\n",
    "        sentences = re.findall(r'\"(.*?)\"', block, flags=re.S)\n",
    "        fixed = []\n",
    "        for s in sentences:\n",
    "            # 合併報導式引號\n",
    "            s = re.sub(r',\"\\s*([A-Z][^\"]+?\\s+said)', r', \\1', s)\n",
    "            s = re.sub(r'\"\\s*([A-Z][^\"]+?\\s+said)', r' \\1', s)\n",
    "            # 刪掉句子中殘留的裸引號\n",
    "            s = s.replace('\\\\\"', '\"')  # 避免重複 escape\n",
    "            s = re.sub(r'(?<!\\\\)\"', \"'\", s)  # 把內部裸引號換成單引號\n",
    "            fixed.append(s.strip())\n",
    "        # 重建成合法 JSON 陣列\n",
    "        rebuilt = \"[\\n      \" + \",\\n      \".join(json.dumps(s) for s in fixed) + \"\\n    ]\"\n",
    "        return rebuilt\n",
    "\n",
    "    # 找 relevant_sentences block\n",
    "    return re.sub(r'\\[\\s*(\".*?\")\\s*\\]', fix_sentence_block, txt, flags=re.S)\n",
    "\n",
    "\n",
    "# def fix_unclosed_blocks(txt: str) -> str:\n",
    "#     \"\"\"\n",
    "#     嘗試修復未關閉的 relevant_sentences 區塊\n",
    "#     \"\"\"\n",
    "#     # 如果有 relevant_sentences: [ 但後面沒有 ]\n",
    "#     if '\"relevant_sentences\": [' in txt and not re.search(r'\\]\\s*\\}', txt):\n",
    "#         print(\"🔧 偵測到 relevant_sentences 未關閉，補上 ] }\")\n",
    "#         # 補上缺失的結尾\n",
    "#         txt = re.sub(r'(\"relevant_sentences\": \\[[^\\]]+)$',\n",
    "#                      r'\\1\\n    ]\\n  }',\n",
    "#                      txt, flags=re.S)\n",
    "#     return txt\n",
    "    \n",
    "def remove_trailing_commas(txt: str) -> str:\n",
    "    \"\"\"\n",
    "    移除 JSON 中不合法的尾逗號\n",
    "    \"\"\"\n",
    "    # 陣列或物件結尾前的逗號\n",
    "    txt = re.sub(r\",(\\s*[\\]}])\", r\"\\1\", txt)\n",
    "    return txt\n",
    "\n",
    "def fix_outer_braces(txt: str) -> str:\n",
    "    \"\"\"\n",
    "    確保 JSON 以 { 開頭，以 } 結尾\n",
    "    \"\"\"\n",
    "    txt = txt.strip()\n",
    "    if not txt.startswith(\"{\"):\n",
    "        txt = \"{\\n\" + txt\n",
    "    if not txt.endswith(\"}\"):\n",
    "        txt = txt + \"\\n}\"\n",
    "    return txt\n",
    "\n",
    "def fix_unclosed_blocks(txt: str) -> str:\n",
    "    if '\"relevant_sentences\": [' in txt and not re.search(r'\\]\\s*[\\},]', txt):\n",
    "        print(\"🔧 偵測到 relevant_sentences 未關閉，補上 ] }\")\n",
    "        txt = re.sub(r'(\"relevant_sentences\": \\[[^\\]]+)$',\n",
    "                     r'\\1\\n    ]\\n  }',\n",
    "                     txt, flags=re.S)\n",
    "    return txt\n",
    "\n",
    "def sanitize_relevant_sentences(txt: str) -> str:\n",
    "    def fix_block(match):\n",
    "        block = match.group(0)\n",
    "        sentences = re.findall(r'\"(.*?)\"', block, flags=re.S)\n",
    "\n",
    "        fixed = []\n",
    "        for s in sentences:\n",
    "            s = s.replace(\"\\n\", \" \").strip()   # 清理換行\n",
    "            fixed.append(s)\n",
    "\n",
    "        return \"[\\n      \" + \",\\n      \".join(json.dumps(s) for s in fixed) + \"\\n    ]\"\n",
    "\n",
    "    return re.sub(r'\\[\\s*(\".*?\")\\s*\\]', fix_block, txt, flags=re.S)\n",
    "\n",
    "\n",
    "\n",
    "def fix_single_article(step2_file, article_id):\n",
    "    with open(step2_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    raw = data[article_id]\n",
    "\n",
    "    if isinstance(raw, str):\n",
    "        txt = raw.strip()\n",
    "        if txt.startswith(\"```\"):\n",
    "            txt = re.sub(r\"^```json\", \"\", txt, flags=re.I).strip()\n",
    "            txt = re.sub(r\"```$\", \"\", txt).strip()\n",
    "\n",
    "        # 修 relevant_sentences\n",
    "        txt = fix_relevant_sentences_block(txt)\n",
    "        txt = sanitize_relevant_sentences(txt)\n",
    "        txt = fix_unclosed_blocks(txt)\n",
    "        txt = remove_trailing_commas(txt)\n",
    "        txt = fix_outer_braces(txt)\n",
    "\n",
    "        try:\n",
    "            parsed = json.loads(txt)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"❌ 修復後仍無法 parse: {e}\")\n",
    "            print(\"⚠️ 修復後片段:\\n\", \"\\n\".join(txt.splitlines()[:20]))\n",
    "            return\n",
    "\n",
    "        # ✅ 放回 Article_x\n",
    "        data[article_id] = parsed\n",
    "\n",
    "    # ✅ 存回完整 batch\n",
    "    with open(step2_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"✅ {article_id} 已修復並寫回 {step2_file}\")\n",
    "\n",
    "\n",
    "\n",
    "# 測試修復 batch_2 的幾篇文章\n",
    "# fix_single_article(\"step2_batches/step2_batch_2.json\", \"Article_59\")\n",
    "# fix_single_article(\"step2_batches/step2_batch_4.json\", \"Article_197\")\n",
    "fix_single_article(\"step2_batches/step2_batch_7.json\", \"Article_349\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c598583f-420c-4e92-a27b-a30c5ada2927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Article_220 已手動修復\n",
      "💾 已寫回 step2_batches/step2_batch_5.json\n"
     ]
    }
   ],
   "source": [
    "# 手動修復\n",
    "\n",
    "import json\n",
    "\n",
    "def manual_fix_articles(step2_file, fixes: dict):\n",
    "    \"\"\"\n",
    "    fixes: dict，key = Article_x，value = Python dict（修正版 JSON）\n",
    "    \"\"\"\n",
    "    with open(step2_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for article_id, fixed_content in fixes.items():\n",
    "        if article_id in data:\n",
    "            data[article_id] = fixed_content\n",
    "            print(f\"✅ {article_id} 已手動修復\")\n",
    "        else:\n",
    "            print(f\"⚠️ {article_id} 不存在於 {step2_file}\")\n",
    "\n",
    "    with open(step2_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"💾 已寫回 {step2_file}\")\n",
    "\n",
    "\n",
    "# =======================\n",
    "# 直接手動修復三篇文章\n",
    "# =======================\n",
    "fixes_batch5 = {\n",
    "    \"Article_220\": {\n",
    "        \"Sen. Kevin Thomas (D-Levittown)\": {\n",
    "            \"entity_type\": \"politician\",\n",
    "            \"asian_status\": \"Asian\",\n",
    "            \"relevant_sentences\": [\n",
    "                \"We are not looking for any different way of treating us, said Sen. Kevin Thomas (D-Levittown), who arrived at the United States at the age of 10.\",\n",
    "                \"Just treat us the same as everyone else.\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "manual_fix_articles(\"step2_batches/step2_batch_5.json\", fixes_batch5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5da521f-6d0d-4e01-b97a-b3da2327e4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"step2_batches\\\\step2_batch_10.json\": [\n",
      "    \"Article_492\",\n",
      "    \"Article_493\",\n",
      "    \"Article_524\"\n",
      "  ],\n",
      "  \"step2_batches\\\\step2_batch_11.json\": [\n",
      "    \"Article_553\"\n",
      "  ],\n",
      "  \"step2_batches\\\\step2_batch_5.json\": [\n",
      "    \"Article_218\"\n",
      "  ],\n",
      "  \"step2_batches\\\\step2_batch_7.json\": [\n",
      "    \"Article_350\",\n",
      "    \"Article_379\"\n",
      "  ],\n",
      "  \"step2_batches\\\\step2_batch_8.json\": [\n",
      "    \"Article_429\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import glob, json\n",
    "\n",
    "missing_articles = [\n",
    "    \"Article_218\", \"Article_492\", \"Article_493\", \"Article_350\", \"Article_379\", \"Article_429\", \"Article_524\", \"Article_553\"\n",
    "]\n",
    "\n",
    "step2_files = sorted(glob.glob(\"step2_batches/step2_batch_*.json\"))\n",
    "missing_map = {}\n",
    "\n",
    "for f in step2_files:\n",
    "    with open(f, \"r\", encoding=\"utf-8\") as fh:   # ✅ 用 fh\n",
    "        data = json.load(fh)                    # ✅ json.load(fh) 而不是 f\n",
    "    for art in missing_articles:\n",
    "        if art in data:\n",
    "            missing_map.setdefault(f, []).append(art)\n",
    "\n",
    "print(json.dumps(missing_map, indent=2, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02abf62-6b50-4158-96e5-e868a510cf72",
   "metadata": {},
   "source": [
    "# 重分 entity_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28dbb82d-0a39-4fe6-a614-2b3379ab7f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ finish\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === 1. 讀取原始資料 ===\n",
    "df = pd.read_csv(\"step3_all_new.csv\")  # 換成你的檔案名稱\n",
    "\n",
    "# === 2. 定義標準化對應表 ===\n",
    "entity_type_mapping = {\n",
    "    # Individuals\n",
    "    \"victim\": \"victims\",\n",
    "    \"victims\": \"victims\",\n",
    "    \"perpetrator\": \"perpetrators\",\n",
    "    \"perpetrators\": \"perpetrators\",\n",
    "    \"politician\": \"politicians\",\n",
    "    \"politicians\": \"politicians\",\n",
    "    \"professional\": \"professionals\",\n",
    "    \"professionals\": \"professionals\",\n",
    "    \"celebrity\": \"celebrities\",\n",
    "    \"musician\": \"celebrities\",\n",
    "    \"actor\": \"celebrities\",\n",
    "    \"actress\": \"celebrities\",\n",
    "    \"journalist\": \"professionals\",\n",
    "    \"reporter\": \"professionals\",\n",
    "    \"professor\": \"professionals\",\n",
    "    \"student\": \"professionals\",\n",
    "    \"educator\": \"professionals\",\n",
    "    \"scholar\": \"professionals\",\n",
    "    \"adjunct instructor\": \"professionals\",\n",
    "    \"attorney\": \"professionals\",\n",
    "    \"director\": \"professionals\",\n",
    "    \"city_manager\": \"professionals\",\n",
    "    \"sociology professor\": \"professionals\",\n",
    "    \"editor\": \"professionals\",\n",
    "    \"deputy inspector\": \"law_enforcement_agencies\",\n",
    "    \"police_officer\": \"law_enforcement_agencies\",\n",
    "    \"police spokesperson\": \"law_enforcement_agencies\",\n",
    "    \"family_member\": \"other_individuals\",\n",
    "    \"friend\": \"other_individuals\",\n",
    "    \"witness\": \"other_individuals\",\n",
    "    \"individual\": \"other_individuals\",\n",
    "    \"individuals\": \"other_individuals\",\n",
    "    \"general public\": \"other_individuals\",\n",
    "    \"general_public\": \"other_individuals\",\n",
    "    \"social_circle\": \"other_individuals\",\n",
    "    \"community_activist\": \"other_individuals\",\n",
    "    \"community_leader\": \"other_individuals\",\n",
    "    \"organizer\": \"other_individuals\",\n",
    "    \"community organizer\": \"other_individuals\",\n",
    "    \"other individual\": \"other_individuals\",\n",
    "    \"rally organizer\": \"other_individuals\",\n",
    "    \"activist\": \"other_individuals\",\n",
    "    \"supporter\": \"other_individuals\",\n",
    "    \"co-host\": \"celebrities\",\n",
    "    \"artist\": \"celebrities\",\n",
    "    \"former assistant district attorney\": \"professionals\",\n",
    "    \"official\": \"professionals\",\n",
    "    \"non-Asian\": \"other_individuals\",\n",
    "    \"youth coordinator\": \"professionals\",\n",
    "    \"school_board_member\": \"professionals\",\n",
    "    \"Dean\": \"professionals\",\n",
    "    \"community leader\": \"other_individuals\",\n",
    "    \"government body\": \"government_bodies\",\n",
    "\n",
    "    # Organizations\n",
    "    \"law_enforcement_agency\": \"law_enforcement_agencies\",\n",
    "    \"law_enforcement_agencies\": \"law_enforcement_agencies\",\n",
    "    \"government_body\": \"government_bodies\",\n",
    "    \"government_bodies\": \"government_bodies\",\n",
    "    \"ngo_or_advocacy_group\": \"ngo_or_advocacy_groups\",\n",
    "    \"ngo_or_advocacy_groups\": \"ngo_or_advocacy_groups\",\n",
    "    \"business_entity\": \"business_entities\",\n",
    "    \"business_entities\": \"business_entities\",\n",
    "    \"community_group\": \"community_groups\",\n",
    "    \"community_groups\": \"community_groups\",\n",
    "    \"educational_institution\": \"government_bodies\",  # 假設為正式機構\n",
    "\n",
    "    # Fallback\n",
    "    \"other\": \"other_individuals\",\n",
    "    \"other_individual\": \"other_individuals\",\n",
    "    \"other_individuals\": \"other_individuals\",\n",
    "    \"group\": \"unknown\",\n",
    "    # \"Cannot be inferred\": \"unknown\",\n",
    "}\n",
    "\n",
    "# === 3. 替換 entity_type 欄位（直接覆蓋）===\n",
    "df[\"entity_type\"] = df[\"entity_type\"].map(entity_type_mapping).fillna(df[\"entity_type\"])\n",
    "\n",
    "# === 4. 輸出成新檔案 ===\n",
    "df.to_csv(\"step3_all_new.csv\", index=False)\n",
    "print(\"✅ finish\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db2ccddd-d383-49a3-b74a-6dd57ab3f180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 找到 118 筆 fostering 需要重分\n",
      "✅ 已完成重分，輸出到 step3_all_refined.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# ======================\n",
    "# LLM 分類提示\n",
    "# ======================\n",
    "def build_refine_prompt(relevant_sentences: str) -> str:\n",
    "    return f\"\"\"You are a sociology professor re-checking misclassified reactions to anti-Asian hate.\n",
    "\n",
    "Task: Re-classify the reaction based ONLY on the exact `reaction_reason`.\n",
    "\n",
    "Reaction Concept Tree:\n",
    "- Support Asian Americans:People or organizations condemned/do not want Anti-Asian incidents to happen, including all forms of crimes, attacks, violence, assaults, physical, verbal, and online harassment. This kind of support is at a conscientious (cognitive) level, not yet taking concrete actions to stop AAPI hate. \n",
    "  - Attending marches/rallies:People or organizations actively attended marches/rallies for supporting Asian American communities.\n",
    "  - Speaking up on social media:People or organizations spoke up in public, such as via social media, to condemn Anti-Asian hate.\n",
    "  - Calling for being united:Asians/Asian Americans become united to support each other to overcome Anti-Asian hate.\n",
    "  - Fostering conversations about anti-Asian hate:The public fostered conversations regarding the Anti-Asian hate issues. Some organizations/groups (e.g., Asian American communities) also think it’s important to have conversations to address the root cause behind the Anti-Asian hate incidents so this will help us to make change.\n",
    "  - Providing shopkeepers with air horns:Some stores or groups provide shopkeepers with air horns. If any anti-Asian hate crimes happened, they can use the air horns to draw everyone’s attention\n",
    "- Advocacy/take actions for changes:Individuals, groups, or organizations want or advocate for changing the current situation where Asians/Asian Americans suffered from Anti-Asian hate, such as physical, verbal, and online harassment, attacks, violence, assaults, and hate crimes. They want cultural shift, open dialogue and listening sessions regarding incidents, practical change in racial stereotypes\n",
    "and perceptions, more awareness about Anti-Asian hate, and human rights.\n",
    "- Politicians initiated anti-Asian hate and racism:Trump and some republican politicians made a lot of comments on the COVID-19 pandemic. One kind of comment is that he used/dubbed Asian-related objects to combine with disease (virus/flu/covid) or directly calling coronavirus/covid (e.g., country’s or region’s names or Kung flu[modified by Kung Fu], or races) to verbally attack Asians/Asian Americans, such as  “China/Chinese virus” or “Kung flu.” Another type of comment is that he blamed China for causing the pandemic. Such kinds of comments initiated racism, Anti-Asian bigotry/hate, and Anti-Asian hate incidents in the US society. Such comments also led some Americans to blame Asians/Asian Americans for causing the pandemic.\n",
    "- Undermining human rights:People want to dehumanize and to undermine the fundamental rights, dignity and belonging of those they target.\n",
    "- Color blind/minimizing racism:Some Americans do not believe discrimination, racism, or racist bias/bigotry against Asians/Asian Americans exists in the community. Incidents of anti-Asian hate (including physical, verbal, and online harassment, attacks, violence, assaults, and Anti-Asian crimes) were downplayed, ignored, or perceived as not existing by the public, the law enforcement system (e.g., the police), and governors (e.g., Mr. Donald Trump). When Asians/Asian Americans were attacked, Anti-Asian hate or racism was not perceived as the perpetrators’ motives/motivations by the police or the perpetrators said their motivations were not triggered by Anti-Asian bigotry or racism.\n",
    "- Youth as not an excuse:Robert Aaron Long murdered eight people in the incident of the 2021 Atlanta Spa Shootings. In news reports, he was called “the 21-year-old.” Some comments advocated stopping calling him “the 21-year-old” as if his youth is an excuse to murder others because of their race, ethnicities, and sex.\n",
    "- Videotaping/confronting harasser/attacker:Asians/Asian Americans or bystanders videotaped/recorded the incidents of physical or verbal harassment; Anti-Asian attacks, assaults, or violence; and Anti-Asian crimes. Asians/Asian Americans who suffered physical harassment, attacks, assaults, violence attacked back to the harassers or attackers. Bystanders’ behaviors aim to defense those who were attacked. Cell phones and survelliance system can be used for videotaping or recording. Additionally, those who experienced verbal harassment speak out to the harassers to let them know their thoughts were biased, offensive, and unjust and tell them to stop. \n",
    "- Sex (sexual) addiction:Excessive sexual thoughts, desires, urges or behaviors that can’t be controlled and cause distress and harm to your relationships, finances and other aspects of life. It is also called hypersexuality or compulsive sexual behavior. It is what the Atlanta shooter claimed as a motivation that led to his senseless killings of the victims.\n",
    "- Religion as a reason:In the 2021 Atlanta Spa Shootings, Robert Aaron Long was the killer who murdered eight people. He told the police that his motive was religious guilt about his sexuality. He said he had sexual desire so he wanted to eliminate it. That’s why he went to the spa to skill women of Asian descent. Asian advocacy groups mentioned whether the killer’s motive was religious guilt about his sexuality, no one should ignore the broader context of Anti-Asian violence and hate crimes. Asian advocacy groups tend to attribute the killer’s motive stems from racism or xenophobia, misogyny, and gendered racism\n",
    "- Feeling hopeless or support AAPI being not enough:Asians/Asian Americans felt worried, frustrated, anxious, and afraid that they may experience Anti-Asian hate crimes, attacks, assaults, and violence. But they felt that nothing happened to stop them. Support for Asian American communities is not enough.\n",
    "- Not confronting attacker/harasser or not reporting:Asians/Asian Americans did not want to confront attackers/harassers/bullies who physically or verbally harassed or attacked them. They thought it is not worthy of reporting the incidents. They did not want to confront because they were afraid of their safety. They just wanted to leave from the incidents soon.\n",
    "- Useless law enforcement:Police did not take a police report and denied there was an Anti-Asian hate crime for the incidents of physical, verbal, or online harassment, attacks, assaults, violence, and Anti-Asian crimes. Another situation is that police affirmed there was a crime, but the motivation did not come from Anti-Asian hate or bigotry/prejudice or racism. Additionally, Asian Americans thought if police often patrolled the streets, a lot of Anti-Asian hate crimes, attacks, assaults, and violence would not happen. But in reality, policy did not do so. \n",
    "  - Did not take a report on Anti-Asian hate crime:police did not take a report on Anti-Asian hate crime, including physical, verbal, or online harassment, attacks, assaults, and violence.\n",
    "  - Did not often patrol the streets:police affirmed there was a crime, but police did not often patrol the streets so that there were a lot of Anti-Asian hate crimes (e.g., physical, verbal, or online harassment, attacks, assaults, and violence) happened.\n",
    "- Takes actions to stop AAPI hate:After the incidents of Anti-Asian hate crimes, attacks, assaults, and violence, state or city government or individuals take concrete actions that aim to stop AAPI hate.\n",
    "  - Installing hotlines:This is a type of action to stop AAPI hate. Some organizations (e.g., city and state governments) install hotlines for victims or people who witness Anti-Asian incidents to report.\n",
    "  - Launching a hate-crime task force:This is a type of action to stop AAPI hate. Some organizations (e.g., city and state governments) launched an Asian hate crime task force to develop approaches to stopping anti-Asian hate crimes.\n",
    "  - Making an announcement to condemn anti-Asian hate:This is a type of action to stop AAPI hate. Some organizations (e.g., city and state governments)  made an open announcement to condemn anti-Asian hate.\n",
    "  - Increasing patrols:Some organizations (e.g., city and state governments) increased patrolling the streets to ensure the safety of Asian Americans. \n",
    "  - Organizing a town hall:Some organizations (e.g., city and state governments) organized a town hall meeting to discuss how to stop anti-Asian hate racism.\n",
    "  - Hiring security guards:Some stores or groups hired security guards to increase safety for Asian Americans and prevent anti-Asian hate crimes or racism.\n",
    "  - Educating students:Schoolteachers and university faculty took actions to educate students on current social and political issues on Anti-Asian hate. They aim to use education to change the public’s view about Asian Americans/Asians and increase the awareness of respecting Asian Americans/Asians\n",
    "  - Rewarding the public to report the info about the suspects:Individuals, groups, or organizations provide rewards to the public when they report any information regarding the suspects who may commit anti-Asian hate crimes.\n",
    "\n",
    "\n",
    "Strict Rules:\n",
    "- Use ONLY the given reaction_reason, no outside knowledge.\n",
    "- If no observable reaction, return \"Cannot be inferred\".\n",
    "- Always pick the most specific category.\n",
    "\n",
    "reaction_reason:\n",
    "{relevant_sentences}\n",
    "\n",
    "Output JSON:\n",
    "{{\n",
    "  \"reaction\": \"<one label from the tree>\",\n",
    "  \"reaction_reason\": \"{relevant_sentences}\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "def get_llm_response(prompt: str) -> dict:\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",   # 可換成你常用的模型\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.0\n",
    "    )\n",
    "    text = resp.choices[0].message.content.strip()\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except:\n",
    "        return {\"reaction\": \"Cannot be inferred\", \"reaction_reason\": \"\"}\n",
    "\n",
    "# ======================\n",
    "# 主程式：重分 Fostering\n",
    "# ======================\n",
    "# def reclassify_fostering(input_csv=\"step3_all_new.csv\", output_csv=\"step3_all_refined.csv\"):\n",
    "#     df = pd.read_csv(input_csv)\n",
    "\n",
    "#     # 找出 fostering 的資料\n",
    "#     mask = df['reaction'] == \"Fostering conversations about anti-Asian hate\"\n",
    "#     fostering_df = df[mask].copy()\n",
    "\n",
    "#     print(f\"🔎 找到 {len(fostering_df)} 筆 fostering 需要重分\")\n",
    "\n",
    "#     new_labels = []\n",
    "#     for _, row in fostering_df.iterrows():\n",
    "#         prompt = build_refine_prompt(str(row['reaction_reason']))\n",
    "#         result = get_llm_response(prompt)\n",
    "#         new_labels.append(result.get(\"reaction\", \"Cannot be inferred\"))\n",
    "\n",
    "#     # 更新回去\n",
    "#     df.loc[mask, \"reaction\"] = new_labels\n",
    "\n",
    "#     # 存新檔\n",
    "#     df.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
    "#     print(f\"✅ 已完成重分，輸出到 {output_csv}\")\n",
    "\n",
    "# # ======================\n",
    "# # 執行\n",
    "# # ======================\n",
    "# if __name__ == \"__main__\":\n",
    "#     reclassify_fostering()\n",
    "\n",
    "def reclassify_fostering(input_csv=\"step3_all_refined.csv\", output_csv=\"step3_all_new_refined.csv\"):\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # 找出 fostering 的資料\n",
    "    mask = df['reaction'] == \"Support Asian Americans\"\n",
    "    fostering_df = df[mask].copy()\n",
    "\n",
    "    print(f\"🔎 找到 {len(fostering_df)} 筆 fostering 需要重分\")\n",
    "\n",
    "    new_labels = []\n",
    "    for _, row in fostering_df.iterrows():\n",
    "        prompt = build_refine_prompt(str(row['reaction_reason']))\n",
    "        result = get_llm_response(prompt)\n",
    "        new_labels.append(result.get(\"reaction\", \"Cannot be inferred\"))\n",
    "\n",
    "    # 更新回去\n",
    "    df.loc[mask, \"reaction\"] = new_labels\n",
    "\n",
    "    # 存新檔\n",
    "    df.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"✅ 已完成重分，輸出到 {output_csv}\")\n",
    "\n",
    "# ======================\n",
    "# 執行\n",
    "# ======================\n",
    "if __name__ == \"__main__\":\n",
    "    reclassify_fostering()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a569e0-82bd-437f-97cd-a4ea32796158",
   "metadata": {},
   "source": [
    "# 重分 emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c67d52-5974-4ae1-a4ed-0e8ec62ccaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 載入 CSV\n",
    "df = pd.read_csv(\"step4_all_with_date.csv\")\n",
    "\n",
    "# 定義 emotion 對照表（細分類 → 六大情緒，全小寫）\n",
    "emotion_map = {\n",
    "    # love\n",
    "    \"love\": \"love\",\n",
    "    \"support\": \"love\", \"solidarity\": \"love\",\n",
    "    \"empathy\": \"love\", \"compassion\": \"love\",\n",
    "    \"recognition\": \"love\", \"gratitude\": \"love\",\n",
    "    \"appreciation\": \"love\", \"encouragement\": \"love\",\n",
    "    \"affection\": \"love\", \"lust\": \"love\", \"longing\": \"love\",\n",
    "\n",
    "    # joy\n",
    "    \"joy\": \"joy\",\n",
    "    \"confidence\": \"joy\", \"optimism\": \"joy\", \"empowerment\": \"joy\",\n",
    "    \"cheerfulness\": \"joy\", \"zest\": \"joy\", \"contentment\": \"joy\",\n",
    "    \"pride\": \"joy\", \"relief\": \"joy\",\n",
    "\n",
    "    # anger\n",
    "    \"anger\": \"anger\",\n",
    "    \"outrage\": \"anger\", \"defiance\": \"anger\", \"responsibility\": \"anger\",\n",
    "    \"irritation\": \"anger\", \"exasperation\": \"anger\", \"rage\": \"anger\",\n",
    "    \"disgust\": \"anger\", \"envy\": \"anger\", \"determination\": \"anger\",\n",
    "    \"urgency\": \"anger\", \"frustration\": \"anger\",\n",
    "\n",
    "    # sadness\n",
    "    \"sadness\": \"sadness\",\n",
    "    \"worry\": \"sadness\", \"resignation\": \"sadness\", \"regret\": \"sadness\",\n",
    "    \"mixed emotions\": \"sadness\", \"dismay\": \"sadness\",\n",
    "    \"disquiet\": \"sadness\", \"disturbance\": \"sadness\",\n",
    "    \"guilt\": \"sadness\",\n",
    "    \"suffering\": \"sadness\", \"disappointment\": \"sadness\", \"shame\": \"sadness\",\n",
    "    \"neglect\": \"sadness\", \"sympathy\": \"sadness\", \"heartbreak\": \"sadness\",\n",
    "    \"pain\": \"sadness\", \"grief\": \"sadness\", \"grieving\": \"sadness\",\n",
    "    \"hurt\": \"sadness\", \"loneliness\": \"sadness\", \"despondency\": \"sadness\",\n",
    "    \"helplessness\": \"sadness\", \"exhaustion\": \"sadness\",\n",
    "\n",
    "    # fear\n",
    "    \"fear\": \"fear\",\n",
    "    \"terror\": \"fear\", \"doubt\": \"fear\",\n",
    "    \"alarm\": \"fear\", \"anxiety\": \"fear\", \"insecurity\": \"fear\",\n",
    "    \"panic\": \"fear\", \"dread\": \"fear\", \"overwhelming\": \"fear\",\n",
    "    \"overwhelmed\": \"fear\", \"horror\": \"fear\", \"shock\": \"fear\",\n",
    "\n",
    "    # surprise\n",
    "    \"surprise\": \"surprise\",\n",
    "    \"confusion\": \"surprise\", \"lightbulb moment\": \"surprise\",\n",
    "    \"amazement\": \"surprise\", \"wonder\": \"surprise\"\n",
    "}\n",
    "\n",
    "def map_emotions(emotion_str):\n",
    "    \"\"\"把情緒收斂成六大基本情緒，其他歸為 cannot be inferred，全小寫\"\"\"\n",
    "    if pd.isna(emotion_str):\n",
    "        return \"cannot be inferred\"\n",
    "    emotions = [e.strip().lower() for e in emotion_str.split(\"|\")]\n",
    "    mapped = [emotion_map.get(e, \"cannot be inferred\") for e in emotions]\n",
    "    mapped = list(dict.fromkeys(mapped))  # 去重但保留順序\n",
    "    return \" | \".join(mapped)\n",
    "\n",
    "# 建立新的欄位\n",
    "df[\"emotion\"] = df[\"emotion\"].apply(map_emotions)\n",
    "\n",
    "# 輸出結果\n",
    "df.to_csv(\"step4_all_with_date.csv\", index=False)\n",
    "print(\"✅ 已完成：emotion 全部轉成小寫 (love, joy, anger, sadness, fear, surprise, cannot be inferred)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "6f46e2f9-a698-4ca8-9b94-36b6c8508a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 已讀取 step2_batch_1.json，目前總文章數：50\n",
      "✅ 已讀取 step2_batch_10.json，目前總文章數：100\n",
      "✅ 已讀取 step2_batch_11.json，目前總文章數：150\n",
      "✅ 已讀取 step2_batch_12.json，目前總文章數：184\n",
      "✅ 已讀取 step2_batch_2.json，目前總文章數：234\n",
      "✅ 已讀取 step2_batch_3.json，目前總文章數：284\n",
      "✅ 已讀取 step2_batch_4.json，目前總文章數：334\n",
      "✅ 已讀取 step2_batch_5.json，目前總文章數：384\n",
      "✅ 已讀取 step2_batch_6.json，目前總文章數：434\n",
      "✅ 已讀取 step2_batch_7.json，目前總文章數：484\n",
      "✅ 已讀取 step2_batch_8.json，目前總文章數：534\n",
      "✅ 已讀取 step2_batch_9.json，目前總文章數：584\n",
      "💾 已輸出 step2_all.json 和 step2_all.csv，共 6068 筆 entities\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def merge_step2_to_csv(input_dir=\"step2_batches\", prefix=\"step2_batch_\", \n",
    "                       output_json=\"step2_all.json\", output_csv=\"step2_all.csv\"):\n",
    "    batch_files = sorted(glob.glob(os.path.join(input_dir, f\"{prefix}*.json\")))\n",
    "    merged_result = {}\n",
    "\n",
    "    # 合併所有 batch JSON\n",
    "    for file in batch_files:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            merged_result.update(data)\n",
    "        print(f\"✅ 已讀取 {os.path.basename(file)}，目前總文章數：{len(merged_result)}\")\n",
    "\n",
    "    # 存成 step2_all.json\n",
    "    with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(merged_result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # 轉成 CSV\n",
    "    rows = []\n",
    "    idx = 1\n",
    "    for article_id, entities in merged_result.items():\n",
    "        if isinstance(entities, dict):  # 正常狀況\n",
    "            for entity, meta in entities.items():\n",
    "                rows.append({\n",
    "                    \"id\": f\"entity_{idx}\",  # ← 這裡改成 id\n",
    "                    \"article_id\": article_id,\n",
    "                    \"entity\": entity,\n",
    "                    \"entity_type\": meta.get(\"entity_type\", \"\"),\n",
    "                    \"asian_status\": meta.get(\"asian_status\", \"\"),\n",
    "                    \"relevant_sentences\": \"\\n\".join(meta.get(\"relevant_sentences\", [])) \n",
    "                                           if isinstance(meta.get(\"relevant_sentences\", []), list)\n",
    "                                           else str(meta.get(\"relevant_sentences\", \"\"))\n",
    "                })\n",
    "                idx += 1\n",
    "        else:\n",
    "            # 如果 step2 有壞掉的（存成字串），就保留原始\n",
    "            rows.append({\n",
    "                \"id\": f\"entity_{idx}\",  # ← 同樣改成 id\n",
    "                \"article_id\": article_id,\n",
    "                \"entity\": \"\",\n",
    "                \"entity_type\": \"\",\n",
    "                \"asian_status\": \"\",\n",
    "                \"relevant_sentences\": str(entities)\n",
    "            })\n",
    "            idx += 1\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"💾 已輸出 {output_json} 和 {output_csv}，共 {len(df)} 筆 entities\")\n",
    "\n",
    "# 執行\n",
    "if __name__ == \"__main__\":\n",
    "    merge_step2_to_csv(\n",
    "        input_dir=\"step2_batches\",\n",
    "        prefix=\"step2_batch_\",\n",
    "        output_json=\"step2_all.json\",\n",
    "        output_csv=\"step2_all.csv\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
